2023-05-01 14:15:48.288871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-01 14:15:48.404761: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-01 14:15:48.408520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-05-01 14:15:48.408553: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-05-01 14:15:52.731179: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-05-01 14:15:52.731261: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-05-01 14:15:52.731276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using custom data configuration default-6a4048e0959d3390
Found cached dataset csv (/home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  9.66it/s]100%|██████████| 1/1 [00:00<00:00,  9.65it/s]
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-625c8a49e46d5dd8.arrow
***** Running Prediction *****
  Num examples = 594
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 18.96it/s] 26%|██▋       | 5/19 [00:00<00:00, 14.06it/s] 37%|███▋      | 7/19 [00:00<00:00, 13.15it/s] 47%|████▋     | 9/19 [00:00<00:00, 12.75it/s] 58%|█████▊    | 11/19 [00:00<00:00, 12.29it/s] 68%|██████▊   | 13/19 [00:01<00:00, 12.27it/s] 79%|███████▉  | 15/19 [00:01<00:00,  9.83it/s] 89%|████████▉ | 17/19 [00:01<00:00, 10.43it/s]100%|██████████| 19/19 [00:01<00:00, 11.24it/s]100%|██████████| 19/19 [00:01<00:00, 11.80it/s]
loading configuration file ../../../models/paper_rep/framing/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-16f1a20df289d911.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 18.97it/s] 26%|██▋       | 5/19 [00:00<00:00, 14.33it/s] 37%|███▋      | 7/19 [00:00<00:00, 13.30it/s] 47%|████▋     | 9/19 [00:00<00:00, 12.91it/s] 58%|█████▊    | 11/19 [00:00<00:00, 12.62it/s] 68%|██████▊   | 13/19 [00:00<00:00, 12.60it/s] 79%|███████▉  | 15/19 [00:01<00:00, 12.58it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.58it/s]100%|██████████| 19/19 [00:01<00:00, 12.87it/s]100%|██████████| 19/19 [00:01<00:00, 13.08it/s]
loading configuration file ../../../models/paper_rep/framing/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c96175d13133a58e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.16it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.39it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.17it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.62it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.31it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.13it/s] 79%|███████▉  | 15/19 [00:01<00:00, 13.01it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.94it/s]100%|██████████| 19/19 [00:01<00:00, 13.16it/s]100%|██████████| 19/19 [00:01<00:00, 13.57it/s]
loading configuration file ../../../models/paper_rep/framing/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3734f8b77241d8b7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.15it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.40it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.20it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.64it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.33it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.14it/s] 79%|███████▉  | 15/19 [00:01<00:00, 13.02it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.94it/s]100%|██████████| 19/19 [00:01<00:00, 13.17it/s]100%|██████████| 19/19 [00:01<00:00, 13.58it/s]
loading configuration file ../../../models/paper_rep/framing/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-18ad1da83475ff3d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.17it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.41it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.21it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.64it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.32it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.13it/s] 79%|███████▉  | 15/19 [00:01<00:00, 13.01it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.94it/s]100%|██████████| 19/19 [00:01<00:00, 13.16it/s]100%|██████████| 19/19 [00:01<00:00, 13.58it/s]
loading configuration file ../../../models/paper_rep/framing/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ca57cd2c2f3e6dc7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.17it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.40it/s] 37%|███▋      | 7/19 [00:00<00:01, 11.24it/s] 47%|████▋     | 9/19 [00:00<00:00, 11.76it/s] 58%|█████▊    | 11/19 [00:00<00:00, 12.09it/s] 68%|██████▊   | 13/19 [00:01<00:00, 12.31it/s] 79%|███████▉  | 15/19 [00:01<00:00, 12.45it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.55it/s]100%|██████████| 19/19 [00:01<00:00, 12.89it/s]100%|██████████| 19/19 [00:01<00:00, 12.76it/s]
loading configuration file ../../../models/paper_rep/framing/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ca097e86947179db.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.17it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.40it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.20it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.64it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.32it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.13it/s] 79%|███████▉  | 15/19 [00:01<00:00, 13.01it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.93it/s]100%|██████████| 19/19 [00:01<00:00, 13.17it/s]100%|██████████| 19/19 [00:01<00:00, 13.58it/s]
loading configuration file ../../../models/paper_rep/framing/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cddcfd580717fb2c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 18.99it/s] 26%|██▋       | 5/19 [00:00<00:01, 13.65it/s] 37%|███▋      | 7/19 [00:00<00:00, 13.15it/s] 47%|████▋     | 9/19 [00:00<00:00, 12.71it/s] 58%|█████▊    | 11/19 [00:00<00:00, 10.12it/s] 68%|██████▊   | 13/19 [00:01<00:00, 10.66it/s] 79%|███████▉  | 15/19 [00:01<00:00, 11.19it/s] 89%|████████▉ | 17/19 [00:01<00:00, 11.59it/s]100%|██████████| 19/19 [00:01<00:00, 12.14it/s]100%|██████████| 19/19 [00:01<00:00, 12.01it/s]
loading configuration file ../../../models/paper_rep/framing/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-12d3d19e3b0e6278.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.14it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.39it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.19it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.63it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.32it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.13it/s] 79%|███████▉  | 15/19 [00:01<00:00, 13.02it/s] 89%|████████▉ | 17/19 [00:01<00:00, 12.94it/s]100%|██████████| 19/19 [00:01<00:00, 13.17it/s]100%|██████████| 19/19 [00:01<00:00, 13.58it/s]
loading configuration file ../../../models/paper_rep/framing/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/paper_rep/framing/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/paper_rep/framing/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2ae2a0c047200cf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 32
  0%|          | 0/19 [00:00<?, ?it/s] 16%|█▌        | 3/19 [00:00<00:00, 19.16it/s] 26%|██▋       | 5/19 [00:00<00:00, 15.41it/s] 37%|███▋      | 7/19 [00:00<00:00, 14.20it/s] 47%|████▋     | 9/19 [00:00<00:00, 13.64it/s] 58%|█████▊    | 11/19 [00:00<00:00, 13.32it/s] 68%|██████▊   | 13/19 [00:00<00:00, 13.13it/s] 79%|███████▉  | 15/19 [00:01<00:00, 11.03it/s] 89%|████████▉ | 17/19 [00:01<00:00, 11.52it/s]100%|██████████| 19/19 [00:01<00:00, 12.12it/s]100%|██████████| 19/19 [00:01<00:00, 12.79it/s]
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.670     0.700     0.685       414
           2      0.476     0.429     0.451       210
           3      0.562     0.474     0.514        76
           4      0.272     0.258     0.265       155
           5      0.686     0.658     0.672       957
           6      0.398     0.444     0.420       473
           7      0.742     0.757     0.750       803
           8      0.635     0.650     0.642       286
           9      0.636     0.636     0.636       239
          10      0.521     0.493     0.506       410
          11      0.756     0.698     0.726       556
          12      0.632     0.671     0.651       243
          13      0.776     0.803     0.789       969
          14      0.735     0.758     0.746       132

    accuracy                          0.653      5933
   macro avg      0.566     0.562     0.564      5933
weighted avg      0.653     0.653     0.652      5933


============================= JOB FEEDBACK =============================

NodeName=uc2n901
Job ID: 22164212
Cluster: uc2
User/Group: ma_ytong/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 64
CPU Utilized: 00:00:44
CPU Efficiency: 0.68% of 01:47:44 core-walltime
Job Wall-clock time: 00:01:41
Memory Utilized: 5.58 GB
Memory Efficiency: 28.59% of 19.53 GB
