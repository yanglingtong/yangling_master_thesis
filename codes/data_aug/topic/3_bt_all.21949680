2023-03-26 23:11:49.328245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-26 23:11:49.442830: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-26 23:11:49.446483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-03-26 23:11:49.446517: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-03-26 23:11:53.163905: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-03-26 23:11:53.163992: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-03-26 23:11:53.164008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using custom data configuration default-e4be6b9a0d3f20d4
Found cached dataset csv (/home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 585.88it/s]
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.35it/s]  4%|▍         | 4/97 [00:00<00:07, 11.63it/s]  6%|▌         | 6/97 [00:00<00:08, 10.42it/s]  8%|▊         | 8/97 [00:00<00:08,  9.93it/s] 10%|█         | 10/97 [00:00<00:08,  9.68it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.54it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.48it/s] 14%|█▍        | 14/97 [00:01<00:11,  7.53it/s] 15%|█▌        | 15/97 [00:01<00:10,  7.87it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.18it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.45it/s] 19%|█▊        | 18/97 [00:01<00:09,  8.65it/s] 20%|█▉        | 19/97 [00:02<00:08,  8.81it/s] 21%|██        | 20/97 [00:02<00:08,  8.94it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.03it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.10it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.14it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.18it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.23it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.24it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.25it/s] 31%|███       | 30/97 [00:03<00:07,  9.25it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.25it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.25it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.26it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.26it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.26it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.27it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.27it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.27it/s] 40%|████      | 39/97 [00:04<00:06,  9.26it/s] 41%|████      | 40/97 [00:04<00:06,  9.26it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.26it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.26it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.26it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.26it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.24it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.24it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.25it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.25it/s] 51%|█████     | 49/97 [00:05<00:05,  9.26it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.26it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.25it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.26it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.26it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.25it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.25it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.25it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.25it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.25it/s] 61%|██████    | 59/97 [00:06<00:04,  9.25it/s] 62%|██████▏   | 60/97 [00:06<00:03,  9.25it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.26it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.26it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.26it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.27it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.26it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.26it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.26it/s] 70%|███████   | 68/97 [00:07<00:03,  9.26it/s] 71%|███████   | 69/97 [00:07<00:03,  9.26it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.27it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.27it/s] 74%|███████▍  | 72/97 [00:07<00:03,  7.49it/s] 75%|███████▌  | 73/97 [00:07<00:03,  7.94it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.30it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.57it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.76it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.91it/s] 80%|████████  | 78/97 [00:08<00:02,  9.01it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.09it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.14it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.23it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.23it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.24it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.24it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.25it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.26it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.26it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.26it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.26it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.26it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.25it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.25it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.25it/s]100%|██████████| 97/97 [00:10<00:00,  9.26it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.53it/s]  4%|▍         | 4/97 [00:00<00:07, 11.68it/s]  6%|▌         | 6/97 [00:00<00:08, 10.44it/s]  8%|▊         | 8/97 [00:00<00:08,  9.94it/s] 10%|█         | 10/97 [00:00<00:08,  9.69it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.55it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.49it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.44it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.40it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.36it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.34it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.32it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.30it/s] 21%|██        | 20/97 [00:02<00:08,  9.29it/s] 22%|██▏       | 21/97 [00:02<00:10,  7.45it/s] 23%|██▎       | 22/97 [00:02<00:09,  7.90it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.25it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.53it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.73it/s] 27%|██▋       | 26/97 [00:02<00:07,  8.88it/s] 28%|██▊       | 27/97 [00:02<00:07,  8.99it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.07it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.13it/s] 31%|███       | 30/97 [00:03<00:07,  9.17it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.23it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.24it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.25it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.25it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.26it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.26it/s] 40%|████      | 39/97 [00:04<00:06,  9.26it/s] 41%|████      | 40/97 [00:04<00:06,  9.26it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.27it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.27it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.26it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.27it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.27it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.27it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.27it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.27it/s] 51%|█████     | 49/97 [00:05<00:05,  9.27it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.27it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.27it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.26it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.27it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.26it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.27it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.26it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.25it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.24it/s] 61%|██████    | 59/97 [00:06<00:05,  7.28it/s] 62%|██████▏   | 60/97 [00:06<00:04,  7.77it/s] 63%|██████▎   | 61/97 [00:06<00:04,  8.16it/s] 64%|██████▍   | 62/97 [00:06<00:04,  8.46it/s] 65%|██████▍   | 63/97 [00:06<00:03,  8.69it/s] 66%|██████▌   | 64/97 [00:06<00:03,  8.85it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.97it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.06it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.12it/s] 70%|███████   | 68/97 [00:07<00:03,  9.17it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.23it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.24it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.25it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.25it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.26it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.26it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.26it/s] 80%|████████  | 78/97 [00:08<00:02,  9.26it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.26it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.24it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.25it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.25it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.26it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.26it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.26it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.26it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.26it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.26it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.26it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.26it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.26it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.25it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.26it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.25it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.26it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.26it/s]100%|██████████| 97/97 [00:10<00:00,  9.26it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.52it/s]  4%|▍         | 4/97 [00:00<00:07, 11.67it/s]  6%|▌         | 6/97 [00:00<00:08, 10.43it/s]  8%|▊         | 8/97 [00:00<00:08,  9.93it/s] 10%|█         | 10/97 [00:00<00:08,  9.67it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.53it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.48it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.43it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.39it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.35it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.33it/s] 19%|█▊        | 18/97 [00:01<00:10,  7.25it/s] 20%|█▉        | 19/97 [00:02<00:10,  7.72it/s] 21%|██        | 20/97 [00:02<00:09,  8.11it/s] 22%|██▏       | 21/97 [00:02<00:09,  8.41it/s] 23%|██▎       | 22/97 [00:02<00:08,  8.64it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.81it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.94it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.03it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.10it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.15it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.17it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.23it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.24it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.25it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.25it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.25it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.26it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.25it/s] 40%|████      | 39/97 [00:04<00:06,  9.25it/s] 41%|████      | 40/97 [00:04<00:06,  9.25it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.25it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.25it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.25it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.25it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.26it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.25it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.24it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.24it/s] 51%|█████     | 49/97 [00:05<00:05,  9.24it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.24it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.24it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.24it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.24it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.24it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.23it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.24it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.23it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.24it/s] 61%|██████    | 59/97 [00:06<00:04,  9.23it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.23it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.24it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.24it/s] 65%|██████▍   | 63/97 [00:06<00:04,  7.41it/s] 66%|██████▌   | 64/97 [00:07<00:04,  7.87it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.24it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.52it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.72it/s] 70%|███████   | 68/97 [00:07<00:03,  8.87it/s] 71%|███████   | 69/97 [00:07<00:03,  8.98it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.05it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.10it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.14it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.18it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.22it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.22it/s] 80%|████████  | 78/97 [00:08<00:02,  9.22it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.23it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.23it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.23it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.21it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.22it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.22it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.21it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.23it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.22it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.22it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.23it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.22it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.23it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.23it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.22it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.23it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.23it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.23it/s]100%|██████████| 97/97 [00:10<00:00,  9.24it/s]100%|██████████| 97/97 [00:10<00:00,  9.16it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.51it/s]  4%|▍         | 4/97 [00:00<00:07, 11.66it/s]  6%|▌         | 6/97 [00:00<00:10,  8.66it/s]  8%|▊         | 8/97 [00:00<00:10,  8.88it/s]  9%|▉         | 9/97 [00:00<00:09,  8.96it/s] 10%|█         | 10/97 [00:01<00:09,  9.03it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.09it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.13it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.16it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.19it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.21it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.23it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.24it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.25it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.25it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.25it/s] 24%|██▎       | 23/97 [00:02<00:07,  9.25it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.26it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.27it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.26it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.26it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.26it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.25it/s] 31%|███       | 30/97 [00:03<00:07,  9.26it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.25it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.25it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.26it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.26it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.26it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.26it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.26it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.25it/s] 40%|████      | 39/97 [00:04<00:06,  9.25it/s] 41%|████      | 40/97 [00:04<00:06,  9.25it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.26it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.26it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.26it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.26it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.26it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.26it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.26it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.26it/s] 51%|█████     | 49/97 [00:05<00:05,  9.25it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.25it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.25it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.25it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.25it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.25it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.25it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.25it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.26it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.26it/s] 61%|██████    | 59/97 [00:06<00:04,  9.25it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.25it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.26it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.25it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.26it/s] 66%|██████▌   | 64/97 [00:06<00:04,  7.48it/s] 67%|██████▋   | 65/97 [00:07<00:04,  7.93it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.29it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.56it/s] 70%|███████   | 68/97 [00:07<00:03,  8.76it/s] 71%|███████   | 69/97 [00:07<00:03,  8.91it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.01it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.09it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.13it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.17it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.22it/s] 80%|████████  | 78/97 [00:08<00:02,  9.24it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.24it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.24it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.23it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.24it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.24it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.24it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.23it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.23it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.23it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.23it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.24it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.24it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.25it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.24it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.24it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.25it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.25it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.26it/s]100%|██████████| 97/97 [00:10<00:00,  9.26it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.51it/s]  4%|▍         | 4/97 [00:00<00:07, 11.63it/s]  6%|▌         | 6/97 [00:00<00:08, 10.41it/s]  8%|▊         | 8/97 [00:00<00:08,  9.91it/s] 10%|█         | 10/97 [00:00<00:09,  9.66it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.52it/s] 13%|█▎        | 13/97 [00:01<00:10,  8.17it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.38it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.56it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.72it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.86it/s] 19%|█▊        | 18/97 [00:01<00:08,  8.96it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.04it/s] 21%|██        | 20/97 [00:02<00:08,  9.11it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.14it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.17it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.19it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.23it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.24it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.24it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.25it/s] 31%|███       | 30/97 [00:03<00:07,  9.24it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.24it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.23it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.24it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.24it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.24it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.24it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.25it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.25it/s] 40%|████      | 39/97 [00:04<00:06,  9.25it/s] 41%|████      | 40/97 [00:04<00:06,  9.24it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.24it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.25it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.24it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.24it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.25it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.25it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.24it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.25it/s] 51%|█████     | 49/97 [00:05<00:05,  9.26it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.24it/s] 53%|█████▎    | 51/97 [00:05<00:06,  7.46it/s] 54%|█████▎    | 52/97 [00:05<00:05,  7.92it/s] 55%|█████▍    | 53/97 [00:05<00:05,  8.27it/s] 56%|█████▌    | 54/97 [00:05<00:05,  8.54it/s] 57%|█████▋    | 55/97 [00:06<00:04,  8.74it/s] 58%|█████▊    | 56/97 [00:06<00:04,  8.88it/s] 59%|█████▉    | 57/97 [00:06<00:04,  8.99it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.07it/s] 61%|██████    | 59/97 [00:06<00:04,  9.12it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.16it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.23it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.24it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.25it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.25it/s] 70%|███████   | 68/97 [00:07<00:03,  9.25it/s] 71%|███████   | 69/97 [00:07<00:03,  9.25it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.24it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.24it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.24it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.23it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.24it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.24it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.24it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.24it/s] 80%|████████  | 78/97 [00:08<00:02,  9.24it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.23it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.23it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.23it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.24it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.23it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.22it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.23it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.22it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.22it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.22it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.22it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.22it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.23it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.22it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.23it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.24it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.24it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.24it/s]100%|██████████| 97/97 [00:10<00:00,  9.25it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:07, 11.66it/s]  6%|▌         | 6/97 [00:00<00:08, 10.42it/s]  8%|▊         | 8/97 [00:00<00:08,  9.93it/s] 10%|█         | 10/97 [00:01<00:10,  8.40it/s] 11%|█▏        | 11/97 [00:01<00:10,  8.55it/s] 12%|█▏        | 12/97 [00:01<00:09,  8.71it/s] 13%|█▎        | 13/97 [00:01<00:09,  8.83it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.94it/s] 15%|█▌        | 15/97 [00:01<00:09,  9.02it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.06it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.12it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.16it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.19it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.25it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.24it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.25it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.24it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.24it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.25it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.24it/s] 31%|███       | 30/97 [00:03<00:07,  9.25it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.25it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.25it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.26it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.26it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.25it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.25it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.26it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.25it/s] 40%|████      | 39/97 [00:04<00:06,  9.26it/s] 41%|████      | 40/97 [00:04<00:06,  9.25it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.26it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.27it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.26it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.27it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.26it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.26it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.25it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.25it/s] 51%|█████     | 49/97 [00:05<00:05,  9.25it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.26it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.26it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.25it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.24it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.22it/s] 57%|█████▋    | 55/97 [00:06<00:06,  6.96it/s] 58%|█████▊    | 56/97 [00:06<00:05,  7.52it/s] 59%|█████▉    | 57/97 [00:06<00:05,  7.96it/s] 60%|█████▉    | 58/97 [00:06<00:04,  8.31it/s] 61%|██████    | 59/97 [00:06<00:04,  8.57it/s] 62%|██████▏   | 60/97 [00:06<00:04,  8.76it/s] 63%|██████▎   | 61/97 [00:06<00:04,  8.90it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.01it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.07it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.12it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.17it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.23it/s] 71%|███████   | 69/97 [00:07<00:03,  9.24it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.24it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.25it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.26it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.25it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.25it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.25it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.24it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.24it/s] 80%|████████  | 78/97 [00:08<00:02,  9.24it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.24it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.23it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.24it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.23it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.24it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.24it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.23it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.24it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.24it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.22it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.22it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.20it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.21it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.22it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.23it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.23it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.23it/s] 99%|█████████▉| 96/97 [00:10<00:00,  7.44it/s]100%|██████████| 97/97 [00:10<00:00,  7.90it/s]100%|██████████| 97/97 [00:10<00:00,  9.09it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.48it/s]  4%|▍         | 4/97 [00:00<00:07, 11.64it/s]  6%|▌         | 6/97 [00:00<00:08, 10.42it/s]  8%|▊         | 8/97 [00:00<00:08,  9.91it/s] 10%|█         | 10/97 [00:00<00:09,  9.66it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.51it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.45it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.41it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.37it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.34it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.31it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.29it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.27it/s] 21%|██        | 20/97 [00:02<00:08,  9.26it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.26it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.25it/s] 24%|██▎       | 23/97 [00:02<00:07,  9.25it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.25it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.25it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.24it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.23it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.24it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.24it/s] 31%|███       | 30/97 [00:03<00:07,  9.23it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.24it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.24it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.24it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.24it/s] 37%|███▋      | 36/97 [00:03<00:08,  7.48it/s] 38%|███▊      | 37/97 [00:03<00:07,  7.92it/s] 39%|███▉      | 38/97 [00:04<00:07,  8.27it/s] 40%|████      | 39/97 [00:04<00:06,  8.54it/s] 41%|████      | 40/97 [00:04<00:06,  8.74it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.88it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.98it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.06it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.12it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.16it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.18it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.23it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.23it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.23it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.22it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.22it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.22it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.22it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.22it/s] 61%|██████    | 59/97 [00:06<00:04,  9.22it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.23it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.23it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.24it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.25it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.24it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.24it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.25it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.25it/s] 70%|███████   | 68/97 [00:07<00:03,  9.25it/s] 71%|███████   | 69/97 [00:07<00:03,  9.24it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.25it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.22it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.23it/s] 76%|███████▋  | 74/97 [00:07<00:02,  9.23it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.23it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.23it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.23it/s] 80%|████████  | 78/97 [00:08<00:02,  9.24it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.23it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.23it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.23it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.22it/s] 86%|████████▌ | 83/97 [00:08<00:01,  9.22it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.22it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.23it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.24it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.23it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.23it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.22it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.22it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.22it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.23it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.23it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.23it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.22it/s] 99%|█████████▉| 96/97 [00:10<00:00,  7.41it/s]100%|██████████| 97/97 [00:10<00:00,  7.88it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.54it/s]  4%|▍         | 4/97 [00:00<00:07, 11.66it/s]  6%|▌         | 6/97 [00:00<00:08, 10.42it/s]  8%|▊         | 8/97 [00:00<00:08,  9.92it/s] 10%|█         | 10/97 [00:00<00:09,  9.67it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.51it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.46it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.41it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.37it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.34it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.32it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.29it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.27it/s] 21%|██        | 20/97 [00:02<00:08,  9.25it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.25it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.24it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.24it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.23it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.24it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.24it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.23it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.23it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.24it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.24it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.24it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.24it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.23it/s] 40%|████      | 39/97 [00:04<00:06,  9.23it/s] 41%|████      | 40/97 [00:04<00:06,  9.24it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.23it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.23it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.23it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.22it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.22it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.22it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.23it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.23it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.23it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.23it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.23it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.24it/s] 59%|█████▉    | 57/97 [00:06<00:05,  7.48it/s] 60%|█████▉    | 58/97 [00:06<00:04,  7.92it/s] 61%|██████    | 59/97 [00:06<00:04,  8.27it/s] 62%|██████▏   | 60/97 [00:06<00:04,  8.54it/s] 63%|██████▎   | 61/97 [00:06<00:04,  8.74it/s] 64%|██████▍   | 62/97 [00:06<00:03,  8.88it/s] 65%|██████▍   | 63/97 [00:06<00:03,  8.99it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.06it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.11it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.15it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.18it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:07<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.22it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.22it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.22it/s] 80%|████████  | 78/97 [00:08<00:02,  9.23it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.22it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.22it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.22it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.22it/s] 86%|████████▌ | 83/97 [00:08<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.21it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.21it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.21it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.21it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.20it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.22it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.21it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.21it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.21it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.21it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.22it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.22it/s]100%|██████████| 97/97 [00:10<00:00,  9.23it/s]100%|██████████| 97/97 [00:10<00:00,  9.25it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.45it/s]  4%|▍         | 4/97 [00:00<00:08, 11.61it/s]  6%|▌         | 6/97 [00:00<00:10,  8.66it/s]  8%|▊         | 8/97 [00:00<00:10,  8.88it/s]  9%|▉         | 9/97 [00:00<00:09,  8.94it/s] 10%|█         | 10/97 [00:01<00:09,  9.02it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.07it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.11it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.15it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.18it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.20it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.22it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.22it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.22it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.24it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.25it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.23it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.22it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.23it/s] 31%|███       | 30/97 [00:03<00:07,  9.23it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.24it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.24it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.23it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.24it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.25it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.24it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.24it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.23it/s] 40%|████      | 39/97 [00:04<00:06,  9.23it/s] 41%|████      | 40/97 [00:04<00:06,  9.23it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.23it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.24it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.24it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.25it/s] 46%|████▋     | 45/97 [00:04<00:06,  7.46it/s] 47%|████▋     | 46/97 [00:05<00:06,  7.91it/s] 48%|████▊     | 47/97 [00:05<00:06,  8.27it/s] 49%|████▉     | 48/97 [00:05<00:05,  8.53it/s] 51%|█████     | 49/97 [00:05<00:05,  8.72it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.87it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.98it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.06it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.11it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.14it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.17it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.22it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.22it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.22it/s] 70%|███████   | 68/97 [00:07<00:03,  9.22it/s] 71%|███████   | 69/97 [00:07<00:03,  9.23it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.24it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.24it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.24it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.22it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.22it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.22it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.22it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.22it/s] 80%|████████  | 78/97 [00:08<00:02,  9.22it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.21it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.22it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.21it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.21it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.21it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.21it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.21it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.21it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.21it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.20it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.21it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.21it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.21it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.22it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.22it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.22it/s]100%|██████████| 97/97 [00:10<00:00,  9.22it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.05_0.95/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.05_0.95/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.05_0.95/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.48it/s]  4%|▍         | 4/97 [00:00<00:10,  8.93it/s]  6%|▌         | 6/97 [00:00<00:10,  9.06it/s]  8%|▊         | 8/97 [00:00<00:09,  9.13it/s]  9%|▉         | 9/97 [00:00<00:09,  9.15it/s] 10%|█         | 10/97 [00:01<00:09,  9.16it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.19it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.19it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.19it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.20it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.21it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.21it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.21it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.22it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.24it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.24it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.24it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.23it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.22it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.23it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.24it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.23it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.23it/s] 40%|████      | 39/97 [00:04<00:06,  9.23it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.23it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.23it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.23it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.22it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.23it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.22it/s] 51%|█████     | 49/97 [00:05<00:06,  7.48it/s] 52%|█████▏    | 50/97 [00:05<00:05,  7.92it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.27it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.53it/s] 55%|█████▍    | 53/97 [00:05<00:05,  8.72it/s] 56%|█████▌    | 54/97 [00:05<00:04,  8.87it/s] 57%|█████▋    | 55/97 [00:06<00:04,  8.97it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.04it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.10it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.14it/s] 61%|██████    | 59/97 [00:06<00:04,  9.15it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.17it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.22it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.23it/s] 70%|███████   | 68/97 [00:07<00:03,  9.22it/s] 71%|███████   | 69/97 [00:07<00:03,  9.22it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.22it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.22it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.22it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.22it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.22it/s] 80%|████████  | 78/97 [00:08<00:02,  9.22it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.23it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.22it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.22it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.21it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.20it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.20it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.20it/s] 93%|█████████▎| 90/97 [00:09<00:00,  7.44it/s] 94%|█████████▍| 91/97 [00:10<00:00,  7.89it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.24it/s] 96%|█████████▌| 93/97 [00:10<00:00,  8.50it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.70it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.85it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.95it/s]100%|██████████| 97/97 [00:10<00:00,  9.03it/s]100%|██████████| 97/97 [00:10<00:00,  9.09it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
The result is below bt_0.05_0.95
              precision    recall  f1-score   support

           0      0.356     0.151     0.212       172
           1      0.836     0.822     0.829       964
           2      0.746     0.670     0.706       914
           3      0.871     0.886     0.878      1799
           4      0.778     0.792     0.785       168
           5      0.721     0.772     0.745       749
           6      0.871     0.918     0.894       912
           7      0.772     0.746     0.759       354
           8      0.817     0.866     0.841       299
          10      0.765     0.840     0.801       594
          12      0.827     0.812     0.820      2088
          13      0.760     0.777     0.768       273
          14      0.689     0.644     0.666       410
          15      0.715     0.741     0.728      1249
          16      0.827     0.891     0.857      4479
          17      0.759     0.727     0.743       719
          18      0.650     0.681     0.665       254
          19      0.912     0.864     0.888      6354
          20      0.917     0.896     0.906      3958
          21      0.575     0.554     0.564       269
          24      0.837     0.835     0.836       715
          26      0.804     0.829     0.816       573
          27      0.667     0.775     0.717       129
          28      0.701     0.727     0.713       769
          29      0.895     0.949     0.921      1273
          30      0.848     0.832     0.840       268
          31      0.773     0.757     0.765       329

    accuracy                          0.837     31034
   macro avg      0.766     0.769     0.765     31034
weighted avg      0.837     0.837     0.836     31034

  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.44it/s]  4%|▍         | 4/97 [00:00<00:08, 11.62it/s]  6%|▌         | 6/97 [00:00<00:08, 10.39it/s]  8%|▊         | 8/97 [00:00<00:09,  9.89it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.44it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.39it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.35it/s] 16%|█▋        | 16/97 [00:01<00:10,  7.40it/s] 18%|█▊        | 17/97 [00:01<00:10,  7.81it/s] 19%|█▊        | 18/97 [00:01<00:09,  8.15it/s] 20%|█▉        | 19/97 [00:02<00:09,  8.42it/s] 21%|██        | 20/97 [00:02<00:08,  8.64it/s] 22%|██▏       | 21/97 [00:02<00:08,  8.80it/s] 23%|██▎       | 22/97 [00:02<00:08,  8.92it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.01it/s] 25%|██▍       | 24/97 [00:02<00:08,  9.07it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.11it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.15it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.17it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.18it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.22it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.23it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.23it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.24it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.23it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.23it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.22it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.21it/s] 61%|██████    | 59/97 [00:06<00:04,  9.21it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:04,  6.91it/s] 71%|███████   | 69/97 [00:07<00:03,  7.46it/s] 72%|███████▏  | 70/97 [00:07<00:03,  7.92it/s] 73%|███████▎  | 71/97 [00:07<00:03,  8.27it/s] 74%|███████▍  | 72/97 [00:07<00:02,  8.54it/s] 75%|███████▌  | 73/97 [00:08<00:02,  8.73it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.86it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.97it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.04it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.09it/s] 80%|████████  | 78/97 [00:08<00:02,  9.12it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.15it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.16it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.20it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.21it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.20it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.21it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.11it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.89it/s] 10%|█         | 10/97 [00:00<00:09,  9.64it/s] 12%|█▏        | 12/97 [00:01<00:10,  8.10it/s] 13%|█▎        | 13/97 [00:01<00:10,  8.30it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.48it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.64it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.78it/s] 18%|█▊        | 17/97 [00:01<00:08,  8.89it/s] 19%|█▊        | 18/97 [00:01<00:08,  8.98it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.05it/s] 21%|██        | 20/97 [00:02<00:08,  9.10it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.14it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.17it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.18it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.20it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.19it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.22it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.22it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.22it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.21it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.21it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.21it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  7.19it/s] 80%|████████  | 78/97 [00:08<00:02,  7.69it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.09it/s] 82%|████████▏ | 80/97 [00:08<00:02,  8.39it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.62it/s] 85%|████████▍ | 82/97 [00:09<00:01,  8.78it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.90it/s] 87%|████████▋ | 84/97 [00:09<00:01,  8.99it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.04it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.08it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.12it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.14it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.15it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.16it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.16it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.12it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.37it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.35it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.31it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.20it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.20it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.19it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:09,  7.19it/s] 34%|███▍      | 33/97 [00:03<00:08,  7.69it/s] 35%|███▌      | 34/97 [00:03<00:07,  8.09it/s] 36%|███▌      | 35/97 [00:03<00:07,  8.39it/s] 37%|███▋      | 36/97 [00:03<00:07,  8.61it/s] 38%|███▊      | 37/97 [00:04<00:06,  8.78it/s] 39%|███▉      | 38/97 [00:04<00:06,  8.90it/s] 40%|████      | 39/97 [00:04<00:06,  8.99it/s] 41%|████      | 40/97 [00:04<00:06,  9.05it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.09it/s] 43%|████▎     | 42/97 [00:04<00:06,  9.12it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.14it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.15it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.17it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.18it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.18it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.18it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.18it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.18it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.18it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.18it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.18it/s] 79%|███████▉  | 77/97 [00:08<00:02,  7.21it/s] 80%|████████  | 78/97 [00:08<00:02,  7.70it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.09it/s] 82%|████████▏ | 80/97 [00:08<00:02,  8.39it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.62it/s] 85%|████████▍ | 82/97 [00:09<00:01,  8.77it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.89it/s] 87%|████████▋ | 84/97 [00:09<00:01,  8.98it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.03it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.07it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.10it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.12it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.14it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.16it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.16it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.16it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.17it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.17it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.11it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.61it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.60it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.31it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.28it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.23it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.22it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:07,  7.20it/s] 43%|████▎     | 42/97 [00:04<00:07,  7.69it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.10it/s] 45%|████▌     | 44/97 [00:04<00:06,  8.41it/s] 46%|████▋     | 45/97 [00:04<00:06,  8.63it/s] 47%|████▋     | 46/97 [00:04<00:05,  8.80it/s] 48%|████▊     | 47/97 [00:05<00:05,  8.92it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.01it/s] 51%|█████     | 49/97 [00:05<00:05,  9.07it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.11it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.13it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.16it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.17it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.22it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.21it/s] 80%|████████  | 78/97 [00:08<00:02,  9.21it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  7.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  7.68it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.09it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.39it/s]100%|██████████| 97/97 [00:10<00:00,  8.62it/s]100%|██████████| 97/97 [00:10<00:00,  9.12it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.44it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.35it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.24it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.23it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.23it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:04<00:07,  7.25it/s] 48%|████▊     | 47/97 [00:05<00:06,  7.73it/s] 49%|████▉     | 48/97 [00:05<00:06,  8.12it/s] 51%|█████     | 49/97 [00:05<00:05,  8.43it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.65it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.81it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.92it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.01it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.06it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.10it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.13it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.16it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.16it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.17it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.16it/s] 71%|███████   | 69/97 [00:07<00:03,  9.18it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.18it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:01,  7.25it/s] 93%|█████████▎| 90/97 [00:09<00:00,  7.73it/s] 94%|█████████▍| 91/97 [00:09<00:00,  8.12it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.40it/s] 96%|█████████▌| 93/97 [00:10<00:00,  8.63it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.79it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.91it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.99it/s]100%|██████████| 97/97 [00:10<00:00,  9.05it/s]100%|██████████| 97/97 [00:10<00:00,  9.12it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.36it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.46it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.32it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.29it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:08,  7.13it/s] 38%|███▊      | 37/97 [00:04<00:07,  7.64it/s] 39%|███▉      | 38/97 [00:04<00:07,  8.05it/s] 40%|████      | 39/97 [00:04<00:06,  8.37it/s] 41%|████      | 40/97 [00:04<00:06,  8.61it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.79it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.92it/s] 44%|████▍     | 43/97 [00:04<00:06,  9.00it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.06it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.10it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.13it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.15it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.18it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.21it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.21it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.21it/s] 61%|██████    | 59/97 [00:06<00:04,  9.21it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.21it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.21it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:11,  8.62it/s]  3%|▎         | 3/97 [00:00<00:10,  8.84it/s]  4%|▍         | 4/97 [00:00<00:10,  8.98it/s]  5%|▌         | 5/97 [00:00<00:10,  9.05it/s]  6%|▌         | 6/97 [00:00<00:10,  9.09it/s]  7%|▋         | 7/97 [00:00<00:09,  9.14it/s]  8%|▊         | 8/97 [00:00<00:09,  9.16it/s]  9%|▉         | 9/97 [00:00<00:09,  9.17it/s] 10%|█         | 10/97 [00:01<00:09,  9.19it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.19it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.19it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.19it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.19it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.19it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.20it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.21it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.21it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.20it/s] 21%|██        | 20/97 [00:02<00:08,  9.20it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.22it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.22it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.22it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.23it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.22it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.23it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.23it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.22it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.22it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.22it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.22it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.22it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.22it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:05,  7.25it/s] 57%|█████▋    | 55/97 [00:06<00:05,  7.70it/s] 58%|█████▊    | 56/97 [00:06<00:05,  8.09it/s] 59%|█████▉    | 57/97 [00:06<00:04,  8.39it/s] 60%|█████▉    | 58/97 [00:06<00:04,  8.62it/s] 61%|██████    | 59/97 [00:06<00:04,  8.78it/s] 62%|██████▏   | 60/97 [00:06<00:04,  8.90it/s] 63%|██████▎   | 61/97 [00:06<00:04,  8.98it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.05it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.09it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.13it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.15it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.17it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.18it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:08<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.21it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.14it/s]100%|██████████| 97/97 [00:10<00:00,  9.16it/s]100%|██████████| 97/97 [00:10<00:00,  9.10it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.33it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.40it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.31it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.28it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.15it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.18it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.18it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:07,  7.28it/s] 47%|████▋     | 46/97 [00:04<00:06,  7.76it/s] 48%|████▊     | 47/97 [00:05<00:06,  8.14it/s] 49%|████▉     | 48/97 [00:05<00:05,  8.43it/s] 51%|█████     | 49/97 [00:05<00:05,  8.65it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.82it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.93it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.02it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.07it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.11it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.14it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.15it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.17it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.20it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.22it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.22it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.22it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.20it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.21it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.21it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.22it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:01<00:10,  8.22it/s] 11%|█▏        | 11/97 [00:01<00:10,  8.40it/s] 12%|█▏        | 12/97 [00:01<00:09,  8.58it/s] 13%|█▎        | 13/97 [00:01<00:09,  8.73it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.85it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.93it/s] 16%|█▋        | 16/97 [00:01<00:09,  9.00it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.06it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.10it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.13it/s] 21%|██        | 20/97 [00:02<00:08,  9.14it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.16it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.19it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.19it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.19it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.23it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.22it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.22it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.17it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.17it/s] 63%|██████▎   | 61/97 [00:06<00:04,  7.23it/s] 64%|██████▍   | 62/97 [00:06<00:04,  7.73it/s] 65%|██████▍   | 63/97 [00:06<00:04,  8.12it/s] 66%|██████▌   | 64/97 [00:07<00:03,  8.42it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.65it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.81it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.92it/s] 70%|███████   | 68/97 [00:07<00:03,  9.01it/s] 71%|███████   | 69/97 [00:07<00:03,  9.08it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.13it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.16it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.17it/s] 75%|███████▌  | 73/97 [00:08<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.16it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.17it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.12it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.1_0.9/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.1_0.9/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.1_0.9/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.41it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:10,  8.43it/s]  7%|▋         | 7/97 [00:00<00:10,  8.60it/s]  8%|▊         | 8/97 [00:00<00:10,  8.75it/s]  9%|▉         | 9/97 [00:00<00:09,  8.87it/s] 10%|█         | 10/97 [00:01<00:09,  8.95it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.04it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.09it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.12it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.15it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.17it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.18it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.19it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.21it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.20it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.23it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.23it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.23it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.22it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:03,  7.25it/s] 77%|███████▋  | 75/97 [00:08<00:02,  7.72it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.11it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.41it/s] 80%|████████  | 78/97 [00:08<00:02,  8.65it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.80it/s] 82%|████████▏ | 80/97 [00:08<00:01,  8.92it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.99it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.05it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.10it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.12it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.14it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.15it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.16it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.16it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.17it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.13it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
The result is below bt_0.1_0.9
              precision    recall  f1-score   support

           0      0.484     0.267     0.345       172
           1      0.853     0.832     0.842       964
           2      0.732     0.707     0.719       914
           3      0.882     0.882     0.882      1799
           4      0.801     0.768     0.784       168
           5      0.731     0.760     0.745       749
           6      0.866     0.902     0.884       912
           7      0.772     0.746     0.759       354
           8      0.839     0.903     0.870       299
          10      0.730     0.842     0.782       594
          12      0.812     0.829     0.820      2088
          13      0.752     0.744     0.748       273
          14      0.678     0.646     0.662       410
          15      0.744     0.728     0.736      1249
          16      0.845     0.866     0.856      4479
          17      0.784     0.723     0.753       719
          18      0.727     0.650     0.686       254
          19      0.896     0.890     0.893      6354
          20      0.914     0.907     0.911      3958
          21      0.552     0.591     0.571       269
          24      0.843     0.846     0.844       715
          26      0.848     0.848     0.848       573
          27      0.716     0.744     0.730       129
          28      0.728     0.733     0.731       769
          29      0.922     0.929     0.926      1273
          30      0.876     0.813     0.843       268
          31      0.754     0.726     0.740       329

    accuracy                          0.841     31034
   macro avg      0.781     0.771     0.774     31034
weighted avg      0.841     0.841     0.840     31034

  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.38it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:10,  8.09it/s] 13%|█▎        | 13/97 [00:01<00:10,  8.28it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.47it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.63it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.76it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.88it/s] 19%|█▊        | 18/97 [00:01<00:08,  8.97it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.03it/s] 21%|██        | 20/97 [00:02<00:08,  9.08it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.11it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.14it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.15it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.16it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.16it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.18it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.18it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.18it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.18it/s] 31%|███       | 30/97 [00:03<00:07,  9.18it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.18it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.15it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.16it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.17it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.18it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.18it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.19it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.19it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.18it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.18it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.17it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.20it/s] 87%|████████▋ | 84/97 [00:09<00:01,  6.70it/s] 88%|████████▊ | 85/97 [00:09<00:01,  7.28it/s] 89%|████████▊ | 86/97 [00:09<00:01,  7.76it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.14it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.43it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.63it/s] 93%|█████████▎| 90/97 [00:09<00:00,  8.80it/s] 94%|█████████▍| 91/97 [00:10<00:00,  8.91it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.00it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.06it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.10it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.12it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.15it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.50it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.25it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.24it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.21it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:07,  7.04it/s] 44%|████▍     | 43/97 [00:04<00:07,  7.56it/s] 45%|████▌     | 44/97 [00:04<00:06,  7.99it/s] 46%|████▋     | 45/97 [00:04<00:06,  8.32it/s] 47%|████▋     | 46/97 [00:04<00:05,  8.56it/s] 48%|████▊     | 47/97 [00:05<00:05,  8.74it/s] 49%|████▉     | 48/97 [00:05<00:05,  8.87it/s] 51%|█████     | 49/97 [00:05<00:05,  8.96it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.03it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.08it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.12it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.14it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.17it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.18it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.20it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.21it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.20it/s] 92%|█████████▏| 89/97 [00:09<00:01,  7.10it/s] 93%|█████████▎| 90/97 [00:09<00:00,  7.61it/s] 94%|█████████▍| 91/97 [00:09<00:00,  8.03it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.34it/s] 96%|█████████▌| 93/97 [00:10<00:00,  8.58it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.75it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.89it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.98it/s]100%|██████████| 97/97 [00:10<00:00,  9.04it/s]100%|██████████| 97/97 [00:10<00:00,  9.11it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.39it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.34it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.44it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.37it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.33it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.29it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.25it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.20it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.19it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.19it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.19it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.18it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.16it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.17it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.17it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.17it/s] 40%|████      | 39/97 [00:04<00:06,  9.18it/s] 41%|████      | 40/97 [00:04<00:08,  7.10it/s] 42%|████▏     | 41/97 [00:04<00:07,  7.61it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.02it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.34it/s] 45%|████▌     | 44/97 [00:04<00:06,  8.58it/s] 46%|████▋     | 45/97 [00:04<00:05,  8.76it/s] 47%|████▋     | 46/97 [00:05<00:05,  8.88it/s] 48%|████▊     | 47/97 [00:05<00:05,  8.97it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.03it/s] 51%|█████     | 49/97 [00:05<00:05,  9.07it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.10it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.13it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.14it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.15it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.16it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.16it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.17it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.17it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.17it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.18it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.17it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.18it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.18it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.18it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.18it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.18it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.18it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.18it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.17it/s] 80%|████████  | 78/97 [00:08<00:02,  9.17it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.17it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.16it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.16it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.17it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.17it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.17it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.17it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.17it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.17it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.17it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.17it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.37it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.46it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:11,  7.40it/s] 15%|█▌        | 15/97 [00:01<00:10,  7.81it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.16it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.43it/s] 19%|█▊        | 18/97 [00:01<00:09,  8.63it/s] 20%|█▉        | 19/97 [00:02<00:08,  8.79it/s] 21%|██        | 20/97 [00:02<00:08,  8.90it/s] 22%|██▏       | 21/97 [00:02<00:08,  8.99it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.05it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.10it/s] 25%|██▍       | 24/97 [00:02<00:08,  9.12it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.15it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.16it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.16it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.17it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.17it/s] 31%|███       | 30/97 [00:03<00:07,  9.18it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.17it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.18it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.19it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:03,  7.11it/s] 75%|███████▌  | 73/97 [00:08<00:03,  7.62it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.04it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.35it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.59it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.75it/s] 80%|████████  | 78/97 [00:08<00:02,  8.88it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.96it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.03it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.07it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.11it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.13it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.14it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.15it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.15it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.17it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.17it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.17it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.10it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.35it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.60it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:10,  7.15it/s] 23%|██▎       | 22/97 [00:02<00:09,  7.65it/s] 24%|██▎       | 23/97 [00:02<00:09,  8.05it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.37it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.61it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.78it/s] 28%|██▊       | 27/97 [00:02<00:07,  8.91it/s] 29%|██▉       | 28/97 [00:03<00:07,  8.99it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.05it/s] 31%|███       | 30/97 [00:03<00:07,  9.10it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.12it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.14it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.15it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.17it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.17it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.18it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.18it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.19it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.19it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.19it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.20it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.21it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.18it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.17it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  7.11it/s] 95%|█████████▍| 92/97 [00:10<00:00,  7.61it/s] 96%|█████████▌| 93/97 [00:10<00:00,  8.03it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.34it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.57it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.75it/s]100%|██████████| 97/97 [00:10<00:00,  8.88it/s]100%|██████████| 97/97 [00:10<00:00,  9.10it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.29it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.24it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.24it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.22it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.22it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.21it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.21it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:04,  6.92it/s] 68%|██████▊   | 66/97 [00:07<00:04,  7.46it/s] 69%|██████▉   | 67/97 [00:07<00:03,  7.92it/s] 70%|███████   | 68/97 [00:07<00:03,  8.27it/s] 71%|███████   | 69/97 [00:07<00:03,  8.52it/s] 72%|███████▏  | 70/97 [00:07<00:03,  8.71it/s] 73%|███████▎  | 71/97 [00:07<00:02,  8.85it/s] 74%|███████▍  | 72/97 [00:07<00:02,  8.96it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.04it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.08it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.12it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.13it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.15it/s] 80%|████████  | 78/97 [00:08<00:02,  9.16it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.16it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.39it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.47it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.42it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.29it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:10,  6.82it/s] 25%|██▍       | 24/97 [00:02<00:09,  7.38it/s] 26%|██▌       | 25/97 [00:02<00:09,  7.84it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.19it/s] 28%|██▊       | 27/97 [00:02<00:08,  8.46it/s] 29%|██▉       | 28/97 [00:03<00:07,  8.67it/s] 30%|██▉       | 29/97 [00:03<00:07,  8.82it/s] 31%|███       | 30/97 [00:03<00:07,  8.92it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.00it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.06it/s] 34%|███▍      | 33/97 [00:03<00:07,  9.11it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.14it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.16it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.17it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.18it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.22it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:03,  6.98it/s] 75%|███████▌  | 73/97 [00:08<00:03,  7.52it/s] 76%|███████▋  | 74/97 [00:08<00:02,  7.95it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.29it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.54it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.73it/s] 80%|████████  | 78/97 [00:08<00:02,  8.86it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.96it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.01it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.07it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.10it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.12it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.14it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.16it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.17it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.17it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.21it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:04<00:07,  7.07it/s] 48%|████▊     | 47/97 [00:05<00:06,  7.60it/s] 49%|████▉     | 48/97 [00:05<00:06,  8.01it/s] 51%|█████     | 49/97 [00:05<00:05,  8.34it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.58it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.76it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.89it/s] 55%|█████▍    | 53/97 [00:05<00:04,  8.98it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.04it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.08it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.11it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.14it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.16it/s] 61%|██████    | 59/97 [00:06<00:04,  9.18it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:09<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:10,  9.28it/s]  3%|▎         | 3/97 [00:00<00:10,  9.22it/s]  4%|▍         | 4/97 [00:00<00:10,  9.21it/s]  5%|▌         | 5/97 [00:00<00:10,  9.15it/s]  6%|▌         | 6/97 [00:00<00:09,  9.16it/s]  7%|▋         | 7/97 [00:00<00:09,  9.17it/s]  8%|▊         | 8/97 [00:00<00:09,  9.18it/s]  9%|▉         | 9/97 [00:00<00:09,  9.18it/s] 10%|█         | 10/97 [00:01<00:09,  9.19it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.19it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.20it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.21it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.22it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.21it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.20it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.20it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.21it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.20it/s] 21%|██        | 20/97 [00:02<00:08,  9.20it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.23it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:06,  7.12it/s] 54%|█████▎    | 52/97 [00:05<00:05,  7.64it/s] 55%|█████▍    | 53/97 [00:05<00:05,  8.04it/s] 56%|█████▌    | 54/97 [00:05<00:05,  8.36it/s] 57%|█████▋    | 55/97 [00:06<00:04,  8.59it/s] 58%|█████▊    | 56/97 [00:06<00:04,  8.76it/s] 59%|█████▉    | 57/97 [00:06<00:04,  8.88it/s] 60%|█████▉    | 58/97 [00:06<00:04,  8.97it/s] 61%|██████    | 59/97 [00:06<00:04,  9.03it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.07it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.11it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.13it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.15it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.18it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.18it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:08<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.11it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.15_0.85/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.15_0.85/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.15_0.85/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.41it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:10,  7.09it/s] 25%|██▍       | 24/97 [00:02<00:09,  7.60it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.01it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.34it/s] 28%|██▊       | 27/97 [00:02<00:08,  8.59it/s] 29%|██▉       | 28/97 [00:03<00:07,  8.76it/s] 30%|██▉       | 29/97 [00:03<00:07,  8.89it/s] 31%|███       | 30/97 [00:03<00:07,  8.98it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.05it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.10it/s] 34%|███▍      | 33/97 [00:03<00:07,  9.13it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.15it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.18it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:02,  7.08it/s] 85%|████████▍ | 82/97 [00:09<00:01,  7.59it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.01it/s] 87%|████████▋ | 84/97 [00:09<00:01,  8.33it/s] 88%|████████▊ | 85/97 [00:09<00:01,  8.57it/s] 89%|████████▊ | 86/97 [00:09<00:01,  8.74it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.87it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.96it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.03it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.07it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.11it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.13it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.15it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.16it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.17it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.11it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
The result is below bt_0.15_0.85
              precision    recall  f1-score   support

           0      0.333     0.244     0.282       172
           1      0.843     0.835     0.839       964
           2      0.744     0.695     0.718       914
           3      0.865     0.883     0.874      1799
           4      0.765     0.815     0.790       168
           5      0.717     0.754     0.735       749
           6      0.874     0.896     0.885       912
           7      0.774     0.746     0.760       354
           8      0.833     0.870     0.851       299
          10      0.780     0.823     0.801       594
          12      0.809     0.813     0.811      2088
          13      0.741     0.766     0.753       273
          14      0.697     0.656     0.676       410
          15      0.759     0.689     0.722      1249
          16      0.844     0.865     0.854      4479
          17      0.726     0.754     0.739       719
          18      0.732     0.646     0.686       254
          19      0.899     0.883     0.891      6354
          20      0.916     0.909     0.913      3958
          21      0.575     0.572     0.574       269
          24      0.829     0.846     0.837       715
          26      0.826     0.853     0.839       573
          27      0.662     0.760     0.708       129
          28      0.714     0.730     0.722       769
          29      0.913     0.940     0.926      1273
          30      0.827     0.854     0.840       268
          31      0.723     0.739     0.731       329

    accuracy                          0.838     31034
   macro avg      0.767     0.772     0.769     31034
weighted avg      0.837     0.838     0.837     31034

  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.34it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:02<00:11,  7.04it/s] 21%|██        | 20/97 [00:02<00:10,  7.55it/s] 22%|██▏       | 21/97 [00:02<00:09,  7.98it/s] 23%|██▎       | 22/97 [00:02<00:09,  8.31it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.56it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.74it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.87it/s] 27%|██▋       | 26/97 [00:02<00:07,  8.97it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.03it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.08it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.11it/s] 31%|███       | 30/97 [00:03<00:07,  9.14it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.16it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.17it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.18it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.18it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.18it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.17it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.18it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.18it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:09<00:02,  6.79it/s] 86%|████████▌ | 83/97 [00:09<00:01,  7.37it/s] 87%|████████▋ | 84/97 [00:09<00:01,  7.83it/s] 88%|████████▊ | 85/97 [00:09<00:01,  8.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  8.46it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.67it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.82it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.92it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.00it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.06it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.10it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.12it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.13it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.15it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.38it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.20it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:09,  6.63it/s] 37%|███▋      | 36/97 [00:03<00:08,  7.23it/s] 38%|███▊      | 37/97 [00:04<00:07,  7.72it/s] 39%|███▉      | 38/97 [00:04<00:07,  8.11it/s] 40%|████      | 39/97 [00:04<00:06,  8.41it/s] 41%|████      | 40/97 [00:04<00:06,  8.63it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.80it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.92it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.99it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.05it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.09it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.12it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.14it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.16it/s] 51%|█████     | 49/97 [00:05<00:05,  9.17it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.17it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.18it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.18it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.36it/s]  4%|▍         | 4/97 [00:00<00:08, 11.56it/s]  6%|▌         | 6/97 [00:00<00:08, 10.34it/s]  8%|▊         | 8/97 [00:00<00:09,  9.84it/s] 10%|█         | 10/97 [00:00<00:09,  9.59it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.50it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.43it/s] 13%|█▎        | 13/97 [00:01<00:11,  7.37it/s] 14%|█▍        | 14/97 [00:01<00:10,  7.76it/s] 15%|█▌        | 15/97 [00:01<00:10,  8.10it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.37it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.58it/s] 19%|█▊        | 18/97 [00:01<00:09,  8.75it/s] 20%|█▉        | 19/97 [00:02<00:08,  8.87it/s] 21%|██        | 20/97 [00:02<00:08,  8.96it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.03it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.08it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.11it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.13it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.15it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.16it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.17it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.17it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.17it/s] 31%|███       | 30/97 [00:03<00:07,  9.17it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.18it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.18it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.17it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.18it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.18it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.18it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.18it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.18it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.19it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.19it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.18it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.18it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.18it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.18it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.18it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.18it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.18it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.17it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.18it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.18it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.18it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.18it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.18it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.18it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.18it/s] 76%|███████▋  | 74/97 [00:08<00:03,  6.92it/s] 77%|███████▋  | 75/97 [00:08<00:02,  7.47it/s] 78%|███████▊  | 76/97 [00:08<00:02,  7.90it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.25it/s] 80%|████████  | 78/97 [00:08<00:02,  8.50it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.69it/s] 82%|████████▏ | 80/97 [00:08<00:01,  8.83it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.93it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.00it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.04it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.08it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.10it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.13it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.14it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.15it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.16it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.16it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.16it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.17it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.16it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.15it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.15it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.16it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.07it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.39it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.35it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.33it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.20it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.20it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.20it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:09,  6.99it/s] 31%|███       | 30/97 [00:03<00:08,  7.53it/s] 32%|███▏      | 31/97 [00:03<00:08,  7.95it/s] 33%|███▎      | 32/97 [00:03<00:07,  8.29it/s] 34%|███▍      | 33/97 [00:03<00:07,  8.54it/s] 35%|███▌      | 34/97 [00:03<00:07,  8.72it/s] 36%|███▌      | 35/97 [00:03<00:07,  8.85it/s] 37%|███▋      | 36/97 [00:03<00:06,  8.96it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.03it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.08it/s] 40%|████      | 39/97 [00:04<00:06,  9.12it/s] 41%|████      | 40/97 [00:04<00:06,  9.14it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.16it/s] 43%|████▎     | 42/97 [00:04<00:06,  9.17it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.18it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.17it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.17it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.17it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.17it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.17it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.17it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.17it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.56it/s]  6%|▌         | 6/97 [00:00<00:11,  8.07it/s]  7%|▋         | 7/97 [00:00<00:10,  8.32it/s]  8%|▊         | 8/97 [00:00<00:10,  8.52it/s]  9%|▉         | 9/97 [00:00<00:10,  8.69it/s] 10%|█         | 10/97 [00:01<00:09,  8.83it/s] 11%|█▏        | 11/97 [00:01<00:09,  8.93it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.00it/s] 13%|█▎        | 13/97 [00:01<00:09,  9.05it/s] 14%|█▍        | 14/97 [00:01<00:09,  9.09it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.13it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.14it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.15it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.16it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.16it/s] 21%|██        | 20/97 [00:02<00:08,  9.18it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.18it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.19it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.19it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.19it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.19it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.19it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.19it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.19it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.19it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.19it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:04,  6.99it/s] 71%|███████   | 69/97 [00:07<00:03,  7.52it/s] 72%|███████▏  | 70/97 [00:07<00:03,  7.95it/s] 73%|███████▎  | 71/97 [00:07<00:03,  8.28it/s] 74%|███████▍  | 72/97 [00:07<00:02,  8.54it/s] 75%|███████▌  | 73/97 [00:08<00:02,  8.72it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.85it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.94it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.00it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.05it/s] 80%|████████  | 78/97 [00:08<00:02,  9.09it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.12it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.14it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.15it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.16it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.16it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.17it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.17it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.17it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.39it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.34it/s]  8%|▊         | 8/97 [00:00<00:09,  9.85it/s] 10%|█         | 10/97 [00:00<00:09,  9.60it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.44it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.28it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:11,  7.00it/s] 22%|██▏       | 21/97 [00:02<00:10,  7.53it/s] 23%|██▎       | 22/97 [00:02<00:09,  7.96it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.30it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.55it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.73it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.85it/s] 28%|██▊       | 27/97 [00:02<00:07,  8.94it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.01it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.06it/s] 31%|███       | 30/97 [00:03<00:07,  9.10it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.12it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.14it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.17it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.18it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.18it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.19it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.19it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.17it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.18it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.18it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.18it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.18it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.18it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.18it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.18it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.18it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.17it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.17it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.17it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  6.58it/s]100%|██████████| 97/97 [00:10<00:00,  9.06it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.41it/s]  4%|▍         | 4/97 [00:00<00:08, 11.56it/s]  6%|▌         | 6/97 [00:00<00:08, 10.34it/s]  8%|▊         | 8/97 [00:00<00:09,  9.85it/s] 10%|█         | 10/97 [00:00<00:09,  9.60it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.51it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.44it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.33it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.29it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.24it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.22it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.21it/s] 21%|██        | 20/97 [00:02<00:08,  9.20it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.19it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.20it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.19it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.18it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.19it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.19it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.19it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.19it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.18it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:06<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.18it/s] 76%|███████▋  | 74/97 [00:07<00:02,  9.18it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.18it/s] 78%|███████▊  | 76/97 [00:08<00:03,  6.83it/s] 79%|███████▉  | 77/97 [00:08<00:02,  7.39it/s] 80%|████████  | 78/97 [00:08<00:02,  7.85it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.21it/s] 82%|████████▏ | 80/97 [00:08<00:02,  8.48it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.67it/s] 85%|████████▍ | 82/97 [00:08<00:01,  8.82it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.93it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.00it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.06it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.09it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.12it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.14it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.15it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.16it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.17it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.41it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.46it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:08,  6.94it/s] 40%|████      | 39/97 [00:04<00:07,  7.49it/s] 41%|████      | 40/97 [00:04<00:07,  7.93it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.26it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.52it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.72it/s] 45%|████▌     | 44/97 [00:04<00:05,  8.86it/s] 46%|████▋     | 45/97 [00:04<00:05,  8.95it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.03it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.08it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.11it/s] 51%|█████     | 49/97 [00:05<00:05,  9.14it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.16it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.17it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.18it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.18it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.21it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.19it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:11<00:05,  1.71it/s] 92%|█████████▏| 89/97 [00:11<00:03,  2.22it/s] 93%|█████████▎| 90/97 [00:11<00:02,  2.78it/s] 94%|█████████▍| 91/97 [00:11<00:02,  2.50it/s] 95%|█████████▍| 92/97 [00:12<00:01,  3.17it/s] 96%|█████████▌| 93/97 [00:12<00:01,  3.93it/s] 97%|█████████▋| 94/97 [00:12<00:00,  4.50it/s] 98%|█████████▊| 95/97 [00:12<00:00,  5.32it/s] 99%|█████████▉| 96/97 [00:12<00:00,  6.09it/s]100%|██████████| 97/97 [00:12<00:00,  6.78it/s]100%|██████████| 97/97 [00:12<00:00,  7.67it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.45it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.39it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.21it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:07,  6.95it/s] 47%|████▋     | 46/97 [00:05<00:06,  7.49it/s] 48%|████▊     | 47/97 [00:05<00:06,  7.92it/s] 49%|████▉     | 48/97 [00:05<00:05,  8.27it/s] 51%|█████     | 49/97 [00:05<00:05,  8.53it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.72it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.86it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.97it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.03it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.08it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.11it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.13it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.15it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.16it/s] 61%|██████    | 59/97 [00:06<00:04,  9.16it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.17it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.17it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.18it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.2_0.8/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.2_0.8/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.2_0.8/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.42it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.29it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:10,  6.98it/s] 23%|██▎       | 22/97 [00:02<00:09,  7.50it/s] 24%|██▎       | 23/97 [00:02<00:09,  7.94it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.27it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.53it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.71it/s] 28%|██▊       | 27/97 [00:02<00:07,  8.86it/s] 29%|██▉       | 28/97 [00:03<00:07,  8.96it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.02it/s] 31%|███       | 30/97 [00:03<00:07,  9.08it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.11it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.13it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.16it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.18it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.20it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.18it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  6.92it/s] 88%|████████▊ | 85/97 [00:09<00:01,  7.47it/s] 89%|████████▊ | 86/97 [00:09<00:01,  7.91it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.25it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.51it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.69it/s] 93%|█████████▎| 90/97 [00:09<00:00,  8.84it/s] 94%|█████████▍| 91/97 [00:10<00:00,  8.94it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.01it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.06it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.10it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.11it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.14it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
The result is below bt_0.2_0.8
              precision    recall  f1-score   support

           0      0.377     0.233     0.288       172
           1      0.817     0.853     0.835       964
           2      0.729     0.704     0.716       914
           3      0.886     0.879     0.883      1799
           4      0.815     0.786     0.800       168
           5      0.759     0.744     0.751       749
           6      0.886     0.890     0.888       912
           7      0.783     0.746     0.764       354
           8      0.833     0.883     0.857       299
          10      0.761     0.808     0.784       594
          12      0.813     0.822     0.817      2088
          13      0.775     0.769     0.772       273
          14      0.676     0.656     0.666       410
          15      0.741     0.706     0.723      1249
          16      0.830     0.868     0.849      4479
          17      0.758     0.754     0.756       719
          18      0.704     0.701     0.702       254
          19      0.894     0.881     0.887      6354
          20      0.915     0.907     0.911      3958
          21      0.592     0.550     0.570       269
          24      0.818     0.838     0.828       715
          26      0.841     0.843     0.842       573
          27      0.686     0.729     0.707       129
          28      0.699     0.737     0.718       769
          29      0.929     0.933     0.931      1273
          30      0.868     0.810     0.838       268
          31      0.752     0.726     0.739       329

    accuracy                          0.838     31034
   macro avg      0.775     0.769     0.771     31034
weighted avg      0.837     0.838     0.837     31034

  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.35it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.35it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:11,  6.87it/s] 20%|█▉        | 19/97 [00:02<00:10,  7.41it/s] 21%|██        | 20/97 [00:02<00:09,  7.86it/s] 22%|██▏       | 21/97 [00:02<00:09,  8.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  8.48it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.68it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.82it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.94it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.03it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.07it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.11it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.13it/s] 31%|███       | 30/97 [00:03<00:07,  9.14it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.16it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.16it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.17it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.20it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.19it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.18it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.18it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.18it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.19it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:08<00:03,  6.51it/s] 76%|███████▋  | 74/97 [00:08<00:03,  7.12it/s] 77%|███████▋  | 75/97 [00:08<00:02,  7.64it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.04it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.36it/s] 80%|████████  | 78/97 [00:08<00:02,  8.59it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.76it/s] 82%|████████▏ | 80/97 [00:08<00:01,  8.88it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.97it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.04it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.08it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.12it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.14it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.15it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.16it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.16it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.17it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.17it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.17it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]100%|██████████| 97/97 [00:10<00:00,  9.05it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.42it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.24it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.23it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.21it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.19it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.19it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:05,  6.61it/s] 63%|██████▎   | 61/97 [00:06<00:04,  7.21it/s] 64%|██████▍   | 62/97 [00:06<00:04,  7.71it/s] 65%|██████▍   | 63/97 [00:06<00:04,  8.10it/s] 66%|██████▌   | 64/97 [00:06<00:03,  8.41it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.63it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.79it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.91it/s] 70%|███████   | 68/97 [00:07<00:03,  8.99it/s] 71%|███████   | 69/97 [00:07<00:03,  9.05it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.10it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.13it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.14it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.16it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.17it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.18it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.18it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.20it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.38it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.52it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.44it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.39it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.28it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.23it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.22it/s] 21%|██        | 20/97 [00:02<00:08,  9.20it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.20it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.19it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:10,  6.79it/s] 31%|███       | 30/97 [00:03<00:09,  7.36it/s] 32%|███▏      | 31/97 [00:03<00:08,  7.83it/s] 33%|███▎      | 32/97 [00:03<00:07,  8.19it/s] 34%|███▍      | 33/97 [00:03<00:07,  8.47it/s] 35%|███▌      | 34/97 [00:03<00:07,  8.67it/s] 36%|███▌      | 35/97 [00:03<00:07,  8.82it/s] 37%|███▋      | 36/97 [00:03<00:06,  8.92it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.01it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.06it/s] 40%|████      | 39/97 [00:04<00:06,  9.11it/s] 41%|████      | 40/97 [00:04<00:06,  9.13it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.16it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.17it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.18it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.18it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.18it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.18it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.18it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.18it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.19it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.19it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.18it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.19it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.19it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.18it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.17it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.17it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.17it/s] 87%|████████▋ | 84/97 [00:09<00:01,  6.79it/s] 88%|████████▊ | 85/97 [00:09<00:01,  7.36it/s] 89%|████████▊ | 86/97 [00:09<00:01,  7.82it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.18it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.45it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.66it/s] 93%|█████████▎| 90/97 [00:09<00:00,  8.81it/s] 94%|█████████▍| 91/97 [00:10<00:00,  8.91it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.98it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.04it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.08it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.12it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.14it/s]100%|██████████| 97/97 [00:10<00:00,  9.16it/s]100%|██████████| 97/97 [00:10<00:00,  9.06it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.29it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.27it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.24it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.21it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.23it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.22it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:06<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:04,  6.80it/s] 70%|███████   | 68/97 [00:07<00:03,  7.36it/s] 71%|███████   | 69/97 [00:07<00:03,  7.83it/s] 72%|███████▏  | 70/97 [00:07<00:03,  8.20it/s] 73%|███████▎  | 71/97 [00:07<00:03,  8.47it/s] 74%|███████▍  | 72/97 [00:07<00:02,  8.68it/s] 75%|███████▌  | 73/97 [00:07<00:02,  8.83it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.94it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.01it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.07it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.10it/s] 80%|████████  | 78/97 [00:08<00:02,  9.14it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.15it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.17it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.17it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.18it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.23it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.22it/s] 31%|███       | 30/97 [00:03<00:07,  9.22it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:08,  6.79it/s] 38%|███▊      | 37/97 [00:04<00:08,  7.37it/s] 39%|███▉      | 38/97 [00:04<00:07,  7.84it/s] 40%|████      | 39/97 [00:04<00:07,  8.21it/s] 41%|████      | 40/97 [00:04<00:06,  8.48it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.68it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.83it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.93it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.01it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.07it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.10it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.13it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.18it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.18it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.18it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.19it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.19it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.19it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.21it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:10<00:00,  6.83it/s] 95%|█████████▍| 92/97 [00:10<00:00,  7.39it/s] 96%|█████████▌| 93/97 [00:10<00:00,  7.85it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.22it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.49it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.69it/s]100%|██████████| 97/97 [00:10<00:00,  8.83it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.61it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.44it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.39it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.35it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.32it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.29it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.27it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.24it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.19it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.21it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.22it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.22it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:07,  6.69it/s] 53%|█████▎    | 51/97 [00:05<00:06,  7.28it/s] 54%|█████▎    | 52/97 [00:05<00:05,  7.77it/s] 55%|█████▍    | 53/97 [00:05<00:05,  8.15it/s] 56%|█████▌    | 54/97 [00:05<00:05,  8.44it/s] 57%|█████▋    | 55/97 [00:05<00:04,  8.65it/s] 58%|█████▊    | 56/97 [00:06<00:04,  8.81it/s] 59%|█████▉    | 57/97 [00:06<00:04,  8.93it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.00it/s] 61%|██████    | 59/97 [00:06<00:04,  9.06it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.10it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.13it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.15it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.16it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.17it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.18it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.21it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.20it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.37it/s]  4%|▍         | 4/97 [00:00<00:08, 11.57it/s]  6%|▌         | 6/97 [00:00<00:08, 10.34it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.53it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.45it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.33it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.31it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.28it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.22it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.22it/s] 40%|████      | 39/97 [00:04<00:08,  6.46it/s] 41%|████      | 40/97 [00:04<00:08,  7.08it/s] 42%|████▏     | 41/97 [00:04<00:07,  7.61it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.03it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.35it/s] 45%|████▌     | 44/97 [00:04<00:06,  8.59it/s] 46%|████▋     | 45/97 [00:04<00:05,  8.78it/s] 47%|████▋     | 46/97 [00:05<00:05,  8.91it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.00it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.05it/s] 51%|█████     | 49/97 [00:05<00:05,  9.09it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.12it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.15it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.17it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.18it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.18it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.19it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.19it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.18it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.16it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:07, 13.36it/s]  4%|▍         | 4/97 [00:00<00:10,  8.68it/s]  5%|▌         | 5/97 [00:00<00:11,  8.24it/s]  6%|▌         | 6/97 [00:00<00:15,  5.97it/s]  7%|▋         | 7/97 [00:00<00:14,  6.31it/s]  8%|▊         | 8/97 [00:01<00:13,  6.57it/s]  9%|▉         | 9/97 [00:01<00:13,  6.76it/s] 10%|█         | 10/97 [00:01<00:12,  6.92it/s] 11%|█▏        | 11/97 [00:01<00:12,  7.00it/s] 12%|█▏        | 12/97 [00:01<00:13,  6.16it/s] 13%|█▎        | 13/97 [00:01<00:12,  6.65it/s] 14%|█▍        | 14/97 [00:02<00:12,  6.80it/s] 15%|█▌        | 15/97 [00:02<00:11,  7.14it/s] 16%|█▋        | 16/97 [00:02<00:11,  7.10it/s] 18%|█▊        | 17/97 [00:02<00:11,  7.12it/s] 19%|█▊        | 18/97 [00:02<00:11,  7.17it/s] 20%|█▉        | 19/97 [00:02<00:10,  7.19it/s] 21%|██        | 20/97 [00:02<00:10,  7.39it/s] 22%|██▏       | 21/97 [00:02<00:10,  7.52it/s] 23%|██▎       | 22/97 [00:03<00:09,  7.60it/s] 24%|██▎       | 23/97 [00:03<00:09,  8.02it/s] 25%|██▍       | 24/97 [00:03<00:08,  8.35it/s] 26%|██▌       | 25/97 [00:03<00:08,  8.59it/s] 27%|██▋       | 26/97 [00:03<00:08,  8.76it/s] 28%|██▊       | 27/97 [00:03<00:07,  8.89it/s] 29%|██▉       | 28/97 [00:03<00:07,  8.99it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.05it/s] 31%|███       | 30/97 [00:03<00:07,  9.09it/s] 32%|███▏      | 31/97 [00:04<00:07,  9.12it/s] 33%|███▎      | 32/97 [00:04<00:07,  9.03it/s] 34%|███▍      | 33/97 [00:04<00:07,  8.98it/s] 35%|███▌      | 34/97 [00:04<00:07,  8.94it/s] 36%|███▌      | 35/97 [00:04<00:06,  8.91it/s] 37%|███▋      | 36/97 [00:04<00:06,  8.90it/s] 38%|███▊      | 37/97 [00:04<00:07,  8.43it/s] 39%|███▉      | 38/97 [00:04<00:07,  7.55it/s] 40%|████      | 39/97 [00:05<00:07,  7.65it/s] 41%|████      | 40/97 [00:05<00:07,  7.69it/s] 42%|████▏     | 41/97 [00:05<00:07,  7.85it/s] 43%|████▎     | 42/97 [00:05<00:06,  8.04it/s] 44%|████▍     | 43/97 [00:05<00:06,  8.03it/s] 45%|████▌     | 44/97 [00:05<00:06,  8.23it/s] 46%|████▋     | 45/97 [00:05<00:06,  8.33it/s] 47%|████▋     | 46/97 [00:05<00:06,  8.39it/s] 48%|████▊     | 47/97 [00:05<00:05,  8.42it/s] 49%|████▉     | 48/97 [00:06<00:06,  8.06it/s] 51%|█████     | 49/97 [00:06<00:06,  7.82it/s] 52%|█████▏    | 50/97 [00:06<00:06,  7.66it/s] 53%|█████▎    | 51/97 [00:06<00:05,  7.96it/s] 54%|█████▎    | 52/97 [00:06<00:05,  7.81it/s] 55%|█████▍    | 53/97 [00:06<00:05,  7.56it/s] 56%|█████▌    | 54/97 [00:06<00:05,  7.99it/s] 57%|█████▋    | 55/97 [00:07<00:05,  8.31it/s] 58%|█████▊    | 56/97 [00:07<00:04,  8.56it/s] 59%|█████▉    | 57/97 [00:07<00:04,  8.74it/s] 60%|█████▉    | 58/97 [00:07<00:04,  8.88it/s] 61%|██████    | 59/97 [00:07<00:04,  8.97it/s] 62%|██████▏   | 60/97 [00:07<00:04,  9.04it/s] 63%|██████▎   | 61/97 [00:07<00:03,  9.05it/s] 64%|██████▍   | 62/97 [00:07<00:03,  9.09it/s] 65%|██████▍   | 63/97 [00:08<00:05,  6.80it/s] 66%|██████▌   | 64/97 [00:08<00:04,  7.36it/s] 67%|██████▋   | 65/97 [00:08<00:04,  7.79it/s] 68%|██████▊   | 66/97 [00:08<00:03,  8.10it/s] 69%|██████▉   | 67/97 [00:08<00:03,  8.30it/s] 70%|███████   | 68/97 [00:08<00:03,  8.43it/s] 71%|███████   | 69/97 [00:08<00:03,  8.58it/s] 72%|███████▏  | 70/97 [00:08<00:03,  8.33it/s] 73%|███████▎  | 71/97 [00:08<00:03,  7.94it/s] 74%|███████▍  | 72/97 [00:09<00:03,  7.76it/s] 75%|███████▌  | 73/97 [00:09<00:02,  8.04it/s] 76%|███████▋  | 74/97 [00:09<00:02,  8.31it/s] 77%|███████▋  | 75/97 [00:09<00:02,  8.49it/s] 78%|███████▊  | 76/97 [00:09<00:02,  8.59it/s] 79%|███████▉  | 77/97 [00:09<00:02,  8.69it/s] 80%|████████  | 78/97 [00:09<00:02,  8.74it/s] 81%|████████▏ | 79/97 [00:09<00:02,  8.83it/s] 82%|████████▏ | 80/97 [00:09<00:01,  8.74it/s] 84%|████████▎ | 81/97 [00:10<00:01,  8.75it/s] 85%|████████▍ | 82/97 [00:10<00:01,  8.81it/s] 86%|████████▌ | 83/97 [00:10<00:01,  8.82it/s] 87%|████████▋ | 84/97 [00:10<00:01,  8.66it/s] 88%|████████▊ | 85/97 [00:10<00:01,  8.81it/s] 89%|████████▊ | 86/97 [00:10<00:01,  8.92it/s] 90%|████████▉ | 87/97 [00:10<00:01,  9.00it/s] 91%|█████████ | 88/97 [00:10<00:00,  9.05it/s] 92%|█████████▏| 89/97 [00:10<00:00,  9.09it/s] 93%|█████████▎| 90/97 [00:11<00:00,  9.12it/s] 94%|█████████▍| 91/97 [00:11<00:00,  9.14it/s] 95%|█████████▍| 92/97 [00:11<00:00,  9.15it/s] 96%|█████████▌| 93/97 [00:11<00:00,  9.16it/s] 97%|█████████▋| 94/97 [00:11<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:11<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:11<00:00,  9.19it/s]100%|██████████| 97/97 [00:11<00:00,  9.20it/s]100%|██████████| 97/97 [00:11<00:00,  8.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.47it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.80it/s] 10%|█         | 10/97 [00:00<00:09,  9.57it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.50it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.43it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.38it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.35it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.26it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.21it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.23it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.19it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.23it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.23it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.23it/s] 48%|████▊     | 47/97 [00:05<00:07,  6.83it/s] 49%|████▉     | 48/97 [00:05<00:06,  7.39it/s] 51%|█████     | 49/97 [00:05<00:06,  7.86it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.22it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.49it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.70it/s] 55%|█████▍    | 53/97 [00:05<00:04,  8.85it/s] 56%|█████▌    | 54/97 [00:05<00:04,  8.95it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.02it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.08it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.11it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.14it/s] 61%|██████    | 59/97 [00:06<00:04,  9.15it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.17it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.18it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.22it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.20it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.20it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.20it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.25_0.75/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.25_0.75/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.25_0.75/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:11,  7.18it/s] 18%|█▊        | 17/97 [00:01<00:10,  7.62it/s] 19%|█▊        | 18/97 [00:01<00:09,  8.00it/s] 20%|█▉        | 19/97 [00:02<00:09,  8.30it/s] 21%|██        | 20/97 [00:02<00:09,  8.54it/s] 22%|██▏       | 21/97 [00:02<00:08,  8.73it/s] 23%|██▎       | 22/97 [00:02<00:08,  8.87it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.97it/s] 25%|██▍       | 24/97 [00:02<00:08,  9.04it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.09it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.12it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.15it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.16it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.17it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.22it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.19it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:03,  6.84it/s] 74%|███████▍  | 72/97 [00:07<00:03,  7.40it/s] 75%|███████▌  | 73/97 [00:08<00:03,  7.86it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.22it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.49it/s] 78%|███████▊  | 76/97 [00:08<00:02,  8.69it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.83it/s] 80%|████████  | 78/97 [00:08<00:02,  8.94it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.02it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.07it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.12it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.14it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.16it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.17it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.16it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.17it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.17it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]100%|██████████| 97/97 [00:10<00:00,  9.08it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63bab90519e9aaa7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
The result is below bt_0.25_0.75
              precision    recall  f1-score   support

           0      0.360     0.209     0.265       172
           1      0.826     0.854     0.840       964
           2      0.747     0.677     0.710       914
           3      0.879     0.897     0.888      1799
           4      0.794     0.780     0.787       168
           5      0.769     0.754     0.761       749
           6      0.865     0.900     0.882       912
           7      0.783     0.763     0.773       354
           8      0.865     0.876     0.870       299
          10      0.780     0.781     0.780       594
          12      0.828     0.804     0.816      2088
          13      0.744     0.736     0.740       273
          14      0.698     0.615     0.654       410
          15      0.707     0.733     0.720      1249
          16      0.832     0.871     0.851      4479
          17      0.756     0.743     0.749       719
          18      0.732     0.665     0.697       254
          19      0.892     0.882     0.887      6354
          20      0.914     0.903     0.908      3958
          21      0.550     0.569     0.559       269
          24      0.833     0.859     0.846       715
          26      0.837     0.827     0.832       573
          27      0.699     0.721     0.710       129
          28      0.701     0.733     0.717       769
          29      0.915     0.921     0.918      1273
          30      0.830     0.836     0.833       268
          31      0.740     0.751     0.745       329

    accuracy                          0.837     31034
   macro avg      0.773     0.765     0.768     31034
weighted avg      0.836     0.837     0.836     31034

  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.42it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:11,  6.76it/s] 22%|██▏       | 21/97 [00:02<00:10,  7.33it/s] 23%|██▎       | 22/97 [00:02<00:09,  7.79it/s] 24%|██▎       | 23/97 [00:02<00:09,  8.15it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.43it/s] 26%|██▌       | 25/97 [00:02<00:08,  8.65it/s] 27%|██▋       | 26/97 [00:02<00:08,  8.81it/s] 28%|██▊       | 27/97 [00:02<00:07,  8.93it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.01it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.06it/s] 31%|███       | 30/97 [00:03<00:07,  9.11it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.14it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.16it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.17it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.18it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.19it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.21it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.23it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.23it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.22it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.19it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.18it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.19it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.20it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.20it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.20it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.20it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.20it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-25cdd65934f6c7a4.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.42it/s]  4%|▍         | 4/97 [00:00<00:08, 11.60it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:11,  7.38it/s] 13%|█▎        | 13/97 [00:01<00:10,  7.68it/s] 14%|█▍        | 14/97 [00:01<00:10,  7.96it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.23it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.45it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.65it/s] 19%|█▊        | 18/97 [00:02<00:08,  8.80it/s] 20%|█▉        | 19/97 [00:02<00:08,  8.91it/s] 21%|██        | 20/97 [00:02<00:08,  8.99it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.06it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.11it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.14it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.16it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.19it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.19it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.21it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.20it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.22it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.22it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.20it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.19it/s] 51%|█████     | 49/97 [00:05<00:05,  9.19it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.19it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.21it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.19it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.20it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.21it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  6.71it/s] 88%|████████▊ | 85/97 [00:09<00:01,  7.29it/s] 89%|████████▊ | 86/97 [00:09<00:01,  7.78it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.15it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.44it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.65it/s] 93%|█████████▎| 90/97 [00:09<00:00,  8.80it/s] 94%|█████████▍| 91/97 [00:10<00:00,  8.92it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.99it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.05it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.10it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.13it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.16it/s]100%|██████████| 97/97 [00:10<00:00,  9.17it/s]100%|██████████| 97/97 [00:10<00:00,  9.03it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b612b7916b5badf5.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.40it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.86it/s] 10%|█         | 10/97 [00:00<00:09,  9.61it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.49it/s] 12%|█▏        | 12/97 [00:01<00:09,  9.42it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.36it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.33it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.30it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.25it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.24it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.23it/s] 21%|██        | 20/97 [00:02<00:08,  9.22it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.21it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.21it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.20it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.20it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.20it/s] 40%|████      | 39/97 [00:04<00:06,  9.21it/s] 41%|████      | 40/97 [00:04<00:06,  9.20it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.20it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:07,  6.71it/s] 47%|████▋     | 46/97 [00:05<00:06,  7.30it/s] 48%|████▊     | 47/97 [00:05<00:06,  7.78it/s] 49%|████▉     | 48/97 [00:05<00:06,  8.15it/s] 51%|█████     | 49/97 [00:05<00:05,  8.43it/s] 52%|█████▏    | 50/97 [00:05<00:05,  8.64it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.80it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.91it/s] 55%|█████▍    | 53/97 [00:05<00:05,  8.78it/s] 56%|█████▌    | 54/97 [00:05<00:04,  8.87it/s] 57%|█████▋    | 55/97 [00:06<00:04,  8.97it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.03it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.08it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.11it/s] 61%|██████    | 59/97 [00:06<00:04,  9.13it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.15it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.16it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.17it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.16it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.17it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.20it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.19it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.18it/s] 80%|████████  | 78/97 [00:08<00:02,  9.18it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.18it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.18it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.18it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.18it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.18it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.18it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.18it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.17it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.18it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.18it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.18it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.16it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-950fac11955e7576.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3104
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.43it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.30it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.23it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.22it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.22it/s] 39%|███▉      | 38/97 [00:04<00:08,  6.71it/s] 40%|████      | 39/97 [00:04<00:07,  7.30it/s] 41%|████      | 40/97 [00:04<00:07,  7.79it/s] 42%|████▏     | 41/97 [00:04<00:06,  8.17it/s] 43%|████▎     | 42/97 [00:04<00:06,  8.45it/s] 44%|████▍     | 43/97 [00:04<00:06,  8.67it/s] 45%|████▌     | 44/97 [00:04<00:06,  8.82it/s] 46%|████▋     | 45/97 [00:04<00:05,  8.93it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.01it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.07it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.12it/s] 51%|█████     | 49/97 [00:05<00:05,  9.14it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.16it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.17it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.18it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.14it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.17it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.18it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.18it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.18it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.18it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.19it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.19it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.19it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.21it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.21it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.21it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.21it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.18it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.20it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-136d888bc824088a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.38it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 11%|█▏        | 11/97 [00:01<00:11,  7.55it/s] 12%|█▏        | 12/97 [00:01<00:10,  7.86it/s] 13%|█▎        | 13/97 [00:01<00:10,  8.15it/s] 14%|█▍        | 14/97 [00:01<00:09,  8.40it/s] 15%|█▌        | 15/97 [00:01<00:09,  8.60it/s] 16%|█▋        | 16/97 [00:01<00:09,  8.76it/s] 18%|█▊        | 17/97 [00:01<00:09,  8.87it/s] 19%|█▊        | 18/97 [00:01<00:08,  8.97it/s] 20%|█▉        | 19/97 [00:02<00:08,  9.03it/s] 21%|██        | 20/97 [00:02<00:08,  9.09it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.12it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.14it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.16it/s] 25%|██▍       | 24/97 [00:02<00:10,  7.05it/s] 26%|██▌       | 25/97 [00:03<00:28,  2.55it/s] 27%|██▋       | 26/97 [00:03<00:21,  3.25it/s] 28%|██▊       | 27/97 [00:03<00:17,  4.04it/s] 29%|██▉       | 28/97 [00:04<00:14,  4.85it/s] 30%|██▉       | 29/97 [00:04<00:12,  5.65it/s] 31%|███       | 30/97 [00:04<00:10,  6.39it/s] 32%|███▏      | 31/97 [00:04<00:09,  7.04it/s] 33%|███▎      | 32/97 [00:04<00:08,  7.57it/s] 34%|███▍      | 33/97 [00:04<00:08,  8.00it/s] 35%|███▌      | 34/97 [00:04<00:07,  8.33it/s] 36%|███▌      | 35/97 [00:04<00:07,  8.58it/s] 37%|███▋      | 36/97 [00:04<00:06,  8.76it/s] 38%|███▊      | 37/97 [00:05<00:06,  8.89it/s] 39%|███▉      | 38/97 [00:05<00:06,  8.99it/s] 40%|████      | 39/97 [00:05<00:06,  9.05it/s] 41%|████      | 40/97 [00:05<00:06,  9.10it/s] 42%|████▏     | 41/97 [00:05<00:06,  9.13it/s] 43%|████▎     | 42/97 [00:05<00:06,  9.16it/s] 44%|████▍     | 43/97 [00:05<00:05,  9.17it/s] 45%|████▌     | 44/97 [00:05<00:05,  9.18it/s] 46%|████▋     | 45/97 [00:05<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:06<00:05,  9.20it/s] 48%|████▊     | 47/97 [00:06<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:06<00:05,  9.21it/s] 51%|█████     | 49/97 [00:06<00:05,  9.22it/s] 52%|█████▏    | 50/97 [00:06<00:05,  9.22it/s] 53%|█████▎    | 51/97 [00:06<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:06<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:06<00:04,  9.22it/s] 56%|█████▌    | 54/97 [00:06<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.21it/s] 58%|█████▊    | 56/97 [00:07<00:04,  9.21it/s] 59%|█████▉    | 57/97 [00:07<00:04,  9.21it/s] 60%|█████▉    | 58/97 [00:07<00:04,  9.21it/s] 61%|██████    | 59/97 [00:07<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:07<00:04,  9.21it/s] 63%|██████▎   | 61/97 [00:07<00:03,  9.21it/s] 64%|██████▍   | 62/97 [00:07<00:03,  9.22it/s] 65%|██████▍   | 63/97 [00:07<00:03,  9.21it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:08<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:08<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:08<00:03,  9.20it/s] 70%|███████   | 68/97 [00:08<00:03,  9.21it/s] 71%|███████   | 69/97 [00:08<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:09<00:06,  4.41it/s] 73%|███████▎  | 71/97 [00:09<00:04,  5.23it/s] 74%|███████▍  | 72/97 [00:09<00:04,  6.01it/s] 75%|███████▌  | 73/97 [00:09<00:03,  6.70it/s] 76%|███████▋  | 74/97 [00:09<00:03,  7.29it/s] 77%|███████▋  | 75/97 [00:09<00:02,  7.78it/s] 78%|███████▊  | 76/97 [00:09<00:02,  8.16it/s] 79%|███████▉  | 77/97 [00:09<00:02,  8.44it/s] 80%|████████  | 78/97 [00:09<00:02,  8.66it/s] 81%|████████▏ | 79/97 [00:09<00:02,  8.81it/s] 82%|████████▏ | 80/97 [00:10<00:01,  8.92it/s] 84%|████████▎ | 81/97 [00:10<00:01,  9.00it/s] 85%|████████▍ | 82/97 [00:10<00:01,  9.05it/s] 86%|████████▌ | 83/97 [00:10<00:01,  9.08it/s] 87%|████████▋ | 84/97 [00:10<00:01,  9.11it/s] 88%|████████▊ | 85/97 [00:10<00:01,  9.14it/s] 89%|████████▊ | 86/97 [00:10<00:01,  9.15it/s] 90%|████████▉ | 87/97 [00:10<00:01,  9.16it/s] 91%|█████████ | 88/97 [00:10<00:00,  9.18it/s] 92%|█████████▏| 89/97 [00:11<00:00,  9.18it/s] 93%|█████████▎| 90/97 [00:11<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:11<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:11<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:11<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:11<00:00,  9.19it/s] 98%|█████████▊| 95/97 [00:11<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:11<00:00,  9.20it/s]100%|██████████| 97/97 [00:11<00:00,  9.20it/s]100%|██████████| 97/97 [00:11<00:00,  8.12it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-309414e95c09455d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.47it/s]  4%|▍         | 4/97 [00:00<00:08, 11.61it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.35it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.32it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.27it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.25it/s] 21%|██        | 20/97 [00:02<00:08,  9.24it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.24it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.24it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.24it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.23it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.22it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.22it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.21it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.22it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.22it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.20it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.23it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.22it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.22it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.22it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.21it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.22it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.21it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:05<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.20it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:05,  6.62it/s] 63%|██████▎   | 61/97 [00:06<00:04,  7.22it/s] 64%|██████▍   | 62/97 [00:06<00:04,  7.72it/s] 65%|██████▍   | 63/97 [00:06<00:04,  8.11it/s] 66%|██████▌   | 64/97 [00:06<00:03,  8.41it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.63it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.78it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.91it/s] 70%|███████   | 68/97 [00:07<00:03,  9.00it/s] 71%|███████   | 69/97 [00:07<00:03,  9.07it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.11it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.13it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.16it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.17it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.18it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.18it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.18it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.21it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.20it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.21it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.20it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.20it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.20it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.21it/s]100%|██████████| 97/97 [00:10<00:00,  9.22it/s]100%|██████████| 97/97 [00:10<00:00,  9.18it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-388e339bcf92dc22.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.41it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.36it/s]  8%|▊         | 8/97 [00:00<00:09,  9.87it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.42it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.37it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.33it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.28it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.25it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.20it/s] 31%|███       | 30/97 [00:03<00:07,  9.19it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.19it/s] 33%|███▎      | 32/97 [00:03<00:10,  6.31it/s] 34%|███▍      | 33/97 [00:03<00:09,  6.97it/s] 35%|███▌      | 34/97 [00:03<00:08,  7.51it/s] 36%|███▌      | 35/97 [00:03<00:07,  7.95it/s] 37%|███▋      | 36/97 [00:03<00:07,  8.29it/s] 38%|███▊      | 37/97 [00:04<00:07,  8.55it/s] 39%|███▉      | 38/97 [00:04<00:06,  8.75it/s] 40%|████      | 39/97 [00:04<00:06,  8.89it/s] 41%|████      | 40/97 [00:04<00:06,  8.99it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.05it/s] 43%|████▎     | 42/97 [00:04<00:06,  9.10it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.13it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.16it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.19it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.19it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.20it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:05,  9.19it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.21it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.21it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.21it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.21it/s] 61%|██████    | 59/97 [00:06<00:04,  9.20it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.20it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.21it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.21it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.21it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.22it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.22it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.22it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.21it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.21it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.19it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.20it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.20it/s] 82%|████████▏ | 80/97 [00:08<00:01,  9.20it/s] 84%|████████▎ | 81/97 [00:08<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:08<00:01,  9.19it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.19it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.19it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.20it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.19it/s] 94%|█████████▍| 91/97 [00:09<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  6.60it/s] 97%|█████████▋| 94/97 [00:10<00:00,  7.20it/s] 98%|█████████▊| 95/97 [00:10<00:00,  7.70it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.09it/s]100%|██████████| 97/97 [00:10<00:00,  8.39it/s]100%|██████████| 97/97 [00:10<00:00,  9.03it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2b1911c832f1ec8.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.38it/s]  4%|▍         | 4/97 [00:00<00:08, 11.58it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.49it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.38it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.29it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.23it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.22it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.21it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.21it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.21it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.21it/s] 31%|███       | 30/97 [00:03<00:07,  9.21it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.22it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.23it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.22it/s] 40%|████      | 39/97 [00:04<00:06,  9.22it/s] 41%|████      | 40/97 [00:04<00:06,  9.22it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.21it/s] 47%|████▋     | 46/97 [00:04<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:04<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.20it/s] 51%|█████     | 49/97 [00:05<00:05,  9.20it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.20it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.20it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.20it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.20it/s] 57%|█████▋    | 55/97 [00:05<00:06,  6.71it/s] 58%|█████▊    | 56/97 [00:06<00:05,  7.30it/s] 59%|█████▉    | 57/97 [00:06<00:05,  7.77it/s] 60%|█████▉    | 58/97 [00:06<00:04,  8.16it/s] 61%|██████    | 59/97 [00:06<00:04,  8.45it/s] 62%|██████▏   | 60/97 [00:06<00:04,  8.66it/s] 63%|██████▎   | 61/97 [00:06<00:04,  8.82it/s] 64%|██████▍   | 62/97 [00:06<00:03,  8.93it/s] 65%|██████▍   | 63/97 [00:06<00:04,  7.91it/s] 66%|██████▌   | 64/97 [00:07<00:03,  8.26it/s] 67%|██████▋   | 65/97 [00:07<00:03,  8.52it/s] 68%|██████▊   | 66/97 [00:07<00:03,  8.71it/s] 69%|██████▉   | 67/97 [00:07<00:03,  8.86it/s] 70%|███████   | 68/97 [00:07<00:03,  8.95it/s] 71%|███████   | 69/97 [00:07<00:03,  8.77it/s] 72%|███████▏  | 70/97 [00:07<00:03,  8.89it/s] 73%|███████▎  | 71/97 [00:07<00:02,  8.83it/s] 74%|███████▍  | 72/97 [00:07<00:02,  8.94it/s] 75%|███████▌  | 73/97 [00:08<00:02,  8.86it/s] 76%|███████▋  | 74/97 [00:08<00:02,  8.95it/s] 77%|███████▋  | 75/97 [00:08<00:02,  8.87it/s] 78%|███████▊  | 76/97 [00:08<00:02,  7.72it/s] 79%|███████▉  | 77/97 [00:08<00:02,  8.11it/s] 80%|████████  | 78/97 [00:08<00:02,  8.26it/s] 81%|████████▏ | 79/97 [00:08<00:02,  8.52it/s] 82%|████████▏ | 80/97 [00:08<00:01,  8.56it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.75it/s] 85%|████████▍ | 82/97 [00:09<00:01,  8.87it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.72it/s] 87%|████████▋ | 84/97 [00:09<00:01,  8.86it/s] 88%|████████▊ | 85/97 [00:09<00:01,  7.92it/s] 89%|████████▊ | 86/97 [00:09<00:01,  8.26it/s] 90%|████████▉ | 87/97 [00:09<00:01,  8.50it/s] 91%|█████████ | 88/97 [00:09<00:01,  8.49it/s] 92%|█████████▏| 89/97 [00:09<00:00,  8.64it/s] 93%|█████████▎| 90/97 [00:10<00:00,  8.73it/s] 94%|█████████▍| 91/97 [00:10<00:00,  8.80it/s] 95%|█████████▍| 92/97 [00:10<00:00,  8.70it/s] 96%|█████████▌| 93/97 [00:10<00:00,  8.81it/s] 97%|█████████▋| 94/97 [00:10<00:00,  8.34it/s] 98%|█████████▊| 95/97 [00:10<00:00,  8.59it/s] 99%|█████████▉| 96/97 [00:10<00:00,  8.76it/s]100%|██████████| 97/97 [00:10<00:00,  8.90it/s]100%|██████████| 97/97 [00:10<00:00,  8.95it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0b7459de4cf9898c.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.48it/s]  4%|▍         | 4/97 [00:00<00:08, 11.61it/s]  6%|▌         | 6/97 [00:00<00:08, 10.38it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.63it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.48it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.43it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.39it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.34it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.31it/s] 18%|█▊        | 17/97 [00:01<00:08,  9.27it/s] 19%|█▊        | 18/97 [00:01<00:08,  9.26it/s] 20%|█▉        | 19/97 [00:01<00:08,  9.24it/s] 21%|██        | 20/97 [00:02<00:08,  9.23it/s] 22%|██▏       | 21/97 [00:02<00:08,  9.22it/s] 23%|██▎       | 22/97 [00:02<00:08,  9.22it/s] 24%|██▎       | 23/97 [00:02<00:08,  9.23it/s] 25%|██▍       | 24/97 [00:02<00:07,  9.22it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.21it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.20it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.20it/s] 29%|██▉       | 28/97 [00:02<00:07,  9.20it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.19it/s] 31%|███       | 30/97 [00:03<00:07,  9.20it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.20it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.20it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.20it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.21it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.21it/s] 38%|███▊      | 37/97 [00:03<00:06,  9.20it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.19it/s] 40%|████      | 39/97 [00:04<00:06,  9.19it/s] 41%|████      | 40/97 [00:04<00:06,  9.19it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.20it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.22it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.23it/s] 46%|████▋     | 45/97 [00:05<00:12,  4.18it/s] 47%|████▋     | 46/97 [00:05<00:10,  5.00it/s] 48%|████▊     | 47/97 [00:05<00:08,  5.78it/s] 49%|████▉     | 48/97 [00:05<00:07,  6.51it/s] 51%|█████     | 49/97 [00:05<00:06,  7.13it/s] 52%|█████▏    | 50/97 [00:05<00:06,  7.64it/s] 53%|█████▎    | 51/97 [00:05<00:05,  8.06it/s] 54%|█████▎    | 52/97 [00:05<00:05,  8.38it/s] 55%|█████▍    | 53/97 [00:06<00:05,  8.61it/s] 56%|█████▌    | 54/97 [00:06<00:04,  8.78it/s] 57%|█████▋    | 55/97 [00:06<00:04,  8.90it/s] 58%|█████▊    | 56/97 [00:06<00:04,  8.98it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.04it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.09it/s] 61%|██████    | 59/97 [00:06<00:04,  9.12it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.14it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.16it/s] 64%|██████▍   | 62/97 [00:07<00:03,  9.17it/s] 65%|██████▍   | 63/97 [00:07<00:03,  9.17it/s] 66%|██████▌   | 64/97 [00:07<00:03,  9.19it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.19it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.19it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.20it/s] 70%|███████   | 68/97 [00:07<00:03,  9.19it/s] 71%|███████   | 69/97 [00:07<00:03,  9.20it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:08<00:02,  9.21it/s] 74%|███████▍  | 72/97 [00:08<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:08<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.20it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.19it/s] 80%|████████  | 78/97 [00:08<00:02,  9.19it/s] 81%|████████▏ | 79/97 [00:08<00:01,  9.19it/s] 82%|████████▏ | 80/97 [00:09<00:01,  9.19it/s] 84%|████████▎ | 81/97 [00:09<00:01,  9.19it/s] 85%|████████▍ | 82/97 [00:09<00:01,  9.18it/s] 86%|████████▌ | 83/97 [00:09<00:01,  9.19it/s] 87%|████████▋ | 84/97 [00:09<00:01,  9.20it/s] 88%|████████▊ | 85/97 [00:09<00:01,  9.20it/s] 89%|████████▊ | 86/97 [00:09<00:01,  9.19it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.19it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.19it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.19it/s] 93%|█████████▎| 90/97 [00:10<00:00,  9.20it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.19it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.19it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.19it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.18it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.19it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  8.86it/s]
loading configuration file ../../../models/data_aug/topic/bt_0.3_0.7/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/topic/bt_0.3_0.7/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/topic/bt_0.3_0.7/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-e4be6b9a0d3f20d4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-feaf7296164a151b.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 3103
  Batch size = 32
  0%|          | 0/97 [00:00<?, ?it/s]  2%|▏         | 2/97 [00:00<00:05, 18.34it/s]  4%|▍         | 4/97 [00:00<00:08, 11.59it/s]  6%|▌         | 6/97 [00:00<00:08, 10.37it/s]  8%|▊         | 8/97 [00:00<00:09,  9.88it/s] 10%|█         | 10/97 [00:00<00:09,  9.62it/s] 11%|█▏        | 11/97 [00:01<00:09,  9.54it/s] 12%|█▏        | 12/97 [00:01<00:08,  9.46it/s] 13%|█▎        | 13/97 [00:01<00:08,  9.40it/s] 14%|█▍        | 14/97 [00:01<00:08,  9.34it/s] 15%|█▌        | 15/97 [00:01<00:08,  9.31it/s] 16%|█▋        | 16/97 [00:01<00:08,  9.27it/s] 18%|█▊        | 17/97 [00:01<00:11,  6.82it/s] 19%|█▊        | 18/97 [00:01<00:10,  7.36it/s] 20%|█▉        | 19/97 [00:02<00:09,  7.82it/s] 21%|██        | 20/97 [00:02<00:09,  8.18it/s] 22%|██▏       | 21/97 [00:02<00:08,  8.46it/s] 23%|██▎       | 22/97 [00:02<00:08,  8.67it/s] 24%|██▎       | 23/97 [00:02<00:08,  8.83it/s] 25%|██▍       | 24/97 [00:02<00:08,  8.93it/s] 26%|██▌       | 25/97 [00:02<00:07,  9.02it/s] 27%|██▋       | 26/97 [00:02<00:07,  9.08it/s] 28%|██▊       | 27/97 [00:02<00:07,  9.12it/s] 29%|██▉       | 28/97 [00:03<00:07,  9.15it/s] 30%|██▉       | 29/97 [00:03<00:07,  9.16it/s] 31%|███       | 30/97 [00:03<00:07,  9.18it/s] 32%|███▏      | 31/97 [00:03<00:07,  9.18it/s] 33%|███▎      | 32/97 [00:03<00:07,  9.18it/s] 34%|███▍      | 33/97 [00:03<00:06,  9.19it/s] 35%|███▌      | 34/97 [00:03<00:06,  9.19it/s] 36%|███▌      | 35/97 [00:03<00:06,  9.21it/s] 37%|███▋      | 36/97 [00:03<00:06,  9.20it/s] 38%|███▊      | 37/97 [00:04<00:06,  9.21it/s] 39%|███▉      | 38/97 [00:04<00:06,  9.21it/s] 40%|████      | 39/97 [00:04<00:06,  9.20it/s] 41%|████      | 40/97 [00:04<00:06,  9.21it/s] 42%|████▏     | 41/97 [00:04<00:06,  9.22it/s] 43%|████▎     | 42/97 [00:04<00:05,  9.21it/s] 44%|████▍     | 43/97 [00:04<00:05,  9.21it/s] 45%|████▌     | 44/97 [00:04<00:05,  9.21it/s] 46%|████▋     | 45/97 [00:04<00:05,  9.22it/s] 47%|████▋     | 46/97 [00:05<00:05,  9.21it/s] 48%|████▊     | 47/97 [00:05<00:05,  9.21it/s] 49%|████▉     | 48/97 [00:05<00:05,  9.21it/s] 51%|█████     | 49/97 [00:05<00:05,  9.22it/s] 52%|█████▏    | 50/97 [00:05<00:05,  9.21it/s] 53%|█████▎    | 51/97 [00:05<00:04,  9.21it/s] 54%|█████▎    | 52/97 [00:05<00:04,  9.22it/s] 55%|█████▍    | 53/97 [00:05<00:04,  9.20it/s] 56%|█████▌    | 54/97 [00:05<00:04,  9.21it/s] 57%|█████▋    | 55/97 [00:06<00:04,  9.20it/s] 58%|█████▊    | 56/97 [00:06<00:04,  9.20it/s] 59%|█████▉    | 57/97 [00:06<00:04,  9.21it/s] 60%|█████▉    | 58/97 [00:06<00:04,  9.20it/s] 61%|██████    | 59/97 [00:06<00:04,  9.19it/s] 62%|██████▏   | 60/97 [00:06<00:04,  9.20it/s] 63%|██████▎   | 61/97 [00:06<00:03,  9.20it/s] 64%|██████▍   | 62/97 [00:06<00:03,  9.20it/s] 65%|██████▍   | 63/97 [00:06<00:03,  9.20it/s] 66%|██████▌   | 64/97 [00:06<00:03,  9.20it/s] 67%|██████▋   | 65/97 [00:07<00:03,  9.20it/s] 68%|██████▊   | 66/97 [00:07<00:03,  9.20it/s] 69%|██████▉   | 67/97 [00:07<00:03,  9.21it/s] 70%|███████   | 68/97 [00:07<00:03,  9.21it/s] 71%|███████   | 69/97 [00:07<00:03,  9.21it/s] 72%|███████▏  | 70/97 [00:07<00:02,  9.20it/s] 73%|███████▎  | 71/97 [00:07<00:02,  9.20it/s] 74%|███████▍  | 72/97 [00:07<00:02,  9.20it/s] 75%|███████▌  | 73/97 [00:07<00:02,  9.20it/s] 76%|███████▋  | 74/97 [00:08<00:02,  9.21it/s] 77%|███████▋  | 75/97 [00:08<00:02,  9.20it/s] 78%|███████▊  | 76/97 [00:08<00:02,  9.20it/s] 79%|███████▉  | 77/97 [00:08<00:02,  9.20it/s] 80%|████████  | 78/97 [00:08<00:02,  6.72it/s] 81%|████████▏ | 79/97 [00:08<00:02,  7.30it/s] 82%|████████▏ | 80/97 [00:08<00:02,  7.78it/s] 84%|████████▎ | 81/97 [00:08<00:01,  8.16it/s] 85%|████████▍ | 82/97 [00:09<00:01,  8.44it/s] 86%|████████▌ | 83/97 [00:09<00:01,  8.65it/s] 87%|████████▋ | 84/97 [00:09<00:01,  8.80it/s] 88%|████████▊ | 85/97 [00:09<00:01,  8.91it/s] 89%|████████▊ | 86/97 [00:09<00:01,  8.99it/s] 90%|████████▉ | 87/97 [00:09<00:01,  9.04it/s] 91%|█████████ | 88/97 [00:09<00:00,  9.09it/s] 92%|█████████▏| 89/97 [00:09<00:00,  9.11it/s] 93%|█████████▎| 90/97 [00:09<00:00,  9.13it/s] 94%|█████████▍| 91/97 [00:10<00:00,  9.15it/s] 95%|█████████▍| 92/97 [00:10<00:00,  9.16it/s] 96%|█████████▌| 93/97 [00:10<00:00,  9.17it/s] 97%|█████████▋| 94/97 [00:10<00:00,  9.17it/s] 98%|█████████▊| 95/97 [00:10<00:00,  9.18it/s] 99%|█████████▉| 96/97 [00:10<00:00,  9.19it/s]100%|██████████| 97/97 [00:10<00:00,  9.21it/s]100%|██████████| 97/97 [00:10<00:00,  9.06it/s]
The result is below bt_0.3_0.7
              precision    recall  f1-score   support

           0      0.344     0.262     0.297       172
           1      0.843     0.829     0.836       964
           2      0.681     0.734     0.707       914
           3      0.883     0.879     0.881      1799
           4      0.697     0.833     0.759       168
           5      0.725     0.757     0.741       749
           6      0.877     0.899     0.888       912
           7      0.735     0.782     0.758       354
           8      0.866     0.843     0.854       299
          10      0.780     0.798     0.789       594
          12      0.818     0.791     0.804      2088
          13      0.746     0.777     0.761       273
          14      0.674     0.620     0.645       410
          15      0.741     0.705     0.723      1249
          16      0.839     0.863     0.851      4479
          17      0.765     0.743     0.754       719
          18      0.705     0.657     0.680       254
          19      0.891     0.889     0.890      6354
          20      0.918     0.902     0.910      3958
          21      0.558     0.572     0.565       269
          24      0.834     0.828     0.831       715
          26      0.816     0.822     0.819       573
          27      0.645     0.775     0.704       129
          28      0.724     0.709     0.716       769
          29      0.918     0.940     0.929      1273
          30      0.851     0.854     0.853       268
          31      0.778     0.702     0.738       329

    accuracy                          0.836     31034
   macro avg      0.765     0.769     0.766     31034
weighted avg      0.835     0.836     0.835     31034


============================= JOB FEEDBACK =============================

NodeName=uc2n901
Job ID: 21949680
Cluster: uc2
User/Group: ma_ytong/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 64
CPU Utilized: 00:14:14
CPU Efficiency: 1.64% of 14:29:20 core-walltime
Job Wall-clock time: 00:13:35
Memory Utilized: 4.66 GB
Memory Efficiency: 2.38% of 195.31 GB
