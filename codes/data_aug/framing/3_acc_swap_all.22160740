2023-04-30 10:19:09.624215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-30 10:19:13.419341: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-30 10:19:13.962876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-04-30 10:19:13.962979: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-04-30 10:19:27.541571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-04-30 10:19:27.555278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-04-30 10:19:27.555306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using custom data configuration default-6a4048e0959d3390
Found cached dataset csv (/home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-625c8a49e46d5dd8.arrow
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.72it/s] 13%|█▎        | 10/75 [00:00<00:01, 36.09it/s] 19%|█▊        | 14/75 [00:00<00:01, 33.82it/s] 24%|██▍       | 18/75 [00:00<00:01, 31.65it/s] 29%|██▉       | 22/75 [00:00<00:01, 31.38it/s] 35%|███▍      | 26/75 [00:00<00:01, 31.59it/s] 40%|████      | 30/75 [00:00<00:01, 32.05it/s] 45%|████▌     | 34/75 [00:01<00:01, 29.79it/s] 51%|█████     | 38/75 [00:01<00:01, 30.06it/s] 56%|█████▌    | 42/75 [00:01<00:01, 28.68it/s] 60%|██████    | 45/75 [00:01<00:01, 28.78it/s] 65%|██████▌   | 49/75 [00:01<00:00, 28.59it/s] 69%|██████▉   | 52/75 [00:01<00:00, 28.66it/s] 75%|███████▍  | 56/75 [00:01<00:00, 29.44it/s] 79%|███████▊  | 59/75 [00:01<00:00, 27.95it/s] 83%|████████▎ | 62/75 [00:02<00:00, 28.22it/s] 88%|████████▊ | 66/75 [00:02<00:00, 29.69it/s] 93%|█████████▎| 70/75 [00:02<00:00, 29.22it/s] 99%|█████████▊| 74/75 [00:02<00:00, 30.33it/s]100%|██████████| 75/75 [00:02<00:00, 30.19it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-16f1a20df289d911.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.45it/s] 13%|█▎        | 10/75 [00:00<00:01, 34.47it/s] 19%|█▊        | 14/75 [00:00<00:01, 32.33it/s] 24%|██▍       | 18/75 [00:00<00:01, 32.28it/s] 29%|██▉       | 22/75 [00:00<00:01, 31.82it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.13it/s] 40%|████      | 30/75 [00:00<00:01, 32.45it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.43it/s] 51%|█████     | 38/75 [00:01<00:01, 32.45it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.54it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.65it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.00it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.13it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.26it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.39it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.59it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.49it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.60it/s]100%|██████████| 75/75 [00:02<00:00, 32.83it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c96175d13133a58e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.53it/s] 13%|█▎        | 10/75 [00:00<00:01, 33.98it/s] 19%|█▊        | 14/75 [00:00<00:01, 33.57it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.21it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.01it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.90it/s] 40%|████      | 30/75 [00:00<00:01, 32.92it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.82it/s] 51%|█████     | 38/75 [00:01<00:01, 32.79it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.75it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.23it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.89it/s] 72%|███████▏  | 54/75 [00:01<00:00, 31.93it/s] 77%|███████▋  | 58/75 [00:01<00:00, 31.66it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.80it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.17it/s] 93%|█████████▎| 70/75 [00:02<00:00, 31.77it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.14it/s]100%|██████████| 75/75 [00:02<00:00, 32.88it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3734f8b77241d8b7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.54it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.95it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.64it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.63it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.25it/s] 35%|███▍      | 26/75 [00:00<00:01, 31.85it/s] 40%|████      | 30/75 [00:00<00:01, 32.23it/s] 45%|████▌     | 34/75 [00:01<00:01, 31.85it/s] 51%|█████     | 38/75 [00:01<00:01, 32.00it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.23it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.35it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.47it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.40it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.43it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.02it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.57it/s] 93%|█████████▎| 70/75 [00:02<00:00, 31.81it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.07it/s]100%|██████████| 75/75 [00:02<00:00, 32.82it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-18ad1da83475ff3d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.55it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.81it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.53it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.81it/s] 29%|██▉       | 22/75 [00:00<00:01, 26.58it/s] 35%|███▍      | 26/75 [00:00<00:01, 28.20it/s] 40%|████      | 30/75 [00:01<00:01, 29.56it/s] 45%|████▌     | 34/75 [00:01<00:01, 29.94it/s] 51%|█████     | 38/75 [00:01<00:01, 29.49it/s] 56%|█████▌    | 42/75 [00:01<00:01, 29.92it/s] 61%|██████▏   | 46/75 [00:01<00:00, 30.59it/s] 67%|██████▋   | 50/75 [00:01<00:00, 29.59it/s] 72%|███████▏  | 54/75 [00:01<00:00, 30.32it/s] 77%|███████▋  | 58/75 [00:01<00:00, 30.98it/s] 83%|████████▎ | 62/75 [00:02<00:00, 31.45it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.87it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.12it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.22it/s]100%|██████████| 75/75 [00:02<00:00, 30.94it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ca57cd2c2f3e6dc7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.26it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.79it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.39it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.58it/s] 29%|██▉       | 22/75 [00:00<00:01, 32.99it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.76it/s] 40%|████      | 30/75 [00:00<00:01, 32.79it/s] 45%|████▌     | 34/75 [00:01<00:01, 31.54it/s] 51%|█████     | 38/75 [00:01<00:01, 31.03it/s] 56%|█████▌    | 42/75 [00:01<00:01, 30.55it/s] 61%|██████▏   | 46/75 [00:01<00:00, 30.36it/s] 67%|██████▋   | 50/75 [00:01<00:00, 30.54it/s] 72%|███████▏  | 54/75 [00:01<00:00, 30.44it/s] 77%|███████▋  | 58/75 [00:01<00:00, 31.03it/s] 83%|████████▎ | 62/75 [00:01<00:00, 30.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 30.91it/s] 93%|█████████▎| 70/75 [00:02<00:00, 30.96it/s] 99%|█████████▊| 74/75 [00:02<00:00, 31.41it/s]100%|██████████| 75/75 [00:02<00:00, 32.06it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:01<00:00,  1.19s/ba]100%|██████████| 1/1 [00:01<00:00,  1.19s/ba]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.31it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.87it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.53it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.83it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.44it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.17it/s] 40%|████      | 30/75 [00:00<00:01, 33.02it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.87it/s] 51%|█████     | 38/75 [00:01<00:01, 32.83it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.85it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.80it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.80it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.80it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.78it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.49it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.57it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.65it/s]100%|██████████| 75/75 [00:02<00:00, 33.48it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.32it/s] 13%|█▎        | 10/75 [00:00<00:01, 34.03it/s] 19%|█▊        | 14/75 [00:00<00:01, 33.40it/s] 24%|██▍       | 18/75 [00:00<00:01, 32.93it/s] 29%|██▉       | 22/75 [00:00<00:01, 32.71it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.65it/s] 40%|████      | 30/75 [00:00<00:01, 32.69it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.59it/s] 51%|█████     | 38/75 [00:01<00:01, 32.29it/s] 56%|█████▌    | 42/75 [00:01<00:01, 31.60it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.81it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.98it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.09it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.14it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.32it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.33it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.41it/s]100%|██████████| 75/75 [00:02<00:00, 32.89it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.21it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.67it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.37it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.53it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.20it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.93it/s] 40%|████      | 30/75 [00:00<00:01, 32.94it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.66it/s] 51%|█████     | 38/75 [00:01<00:01, 32.59it/s] 56%|█████▌    | 42/75 [00:01<00:01, 31.92it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.09it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.20it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.26it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.29it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.37it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.44it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.47it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.55it/s]100%|██████████| 75/75 [00:02<00:00, 33.19it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.1_0.9/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.1_0.9/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.1_0.9/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.33it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.60it/s] 19%|█▊        | 14/75 [00:00<00:01, 32.72it/s] 24%|██▍       | 18/75 [00:00<00:01, 31.06it/s] 29%|██▉       | 22/75 [00:00<00:01, 30.26it/s] 35%|███▍      | 26/75 [00:00<00:01, 30.83it/s] 40%|████      | 30/75 [00:00<00:01, 31.42it/s] 45%|████▌     | 34/75 [00:01<00:01, 31.35it/s] 51%|█████     | 38/75 [00:01<00:01, 30.48it/s] 56%|█████▌    | 42/75 [00:01<00:01, 30.59it/s] 61%|██████▏   | 46/75 [00:01<00:00, 30.24it/s] 67%|██████▋   | 50/75 [00:01<00:00, 30.84it/s] 72%|███████▏  | 54/75 [00:01<00:00, 29.71it/s] 77%|███████▋  | 58/75 [00:01<00:00, 30.55it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.57it/s] 93%|█████████▎| 70/75 [00:02<00:00, 31.84it/s] 99%|█████████▊| 74/75 [00:02<00:00, 31.99it/s]100%|██████████| 75/75 [00:02<00:00, 31.74it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below swap_0.1_0.9
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.665     0.667     0.666       414
           2      0.461     0.448     0.454       210
           3      0.569     0.434     0.493        76
           4      0.336     0.258     0.292       155
           5      0.673     0.669     0.671       957
           6      0.396     0.372     0.383       473
           7      0.745     0.757     0.751       803
           8      0.617     0.608     0.613       286
           9      0.630     0.678     0.653       239
          10      0.465     0.480     0.472       410
          11      0.736     0.707     0.721       556
          12      0.614     0.642     0.628       243
          13      0.752     0.811     0.781       969
          14      0.736     0.720     0.728       132

    accuracy                          0.646      5933
   macro avg      0.560     0.550     0.554      5933
weighted avg      0.640     0.646     0.642      5933

  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.43it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.43it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.21it/s] 24%|██▍       | 18/75 [00:00<00:01, 31.57it/s] 29%|██▉       | 22/75 [00:00<00:01, 31.85it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.09it/s] 40%|████      | 30/75 [00:00<00:01, 32.30it/s] 45%|████▌     | 34/75 [00:01<00:01, 31.62it/s] 51%|█████     | 38/75 [00:01<00:01, 31.92it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.07it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.16it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.42it/s] 72%|███████▏  | 54/75 [00:01<00:00, 30.97it/s] 77%|███████▋  | 58/75 [00:01<00:00, 31.45it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.71it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.97it/s] 93%|█████████▎| 70/75 [00:02<00:00, 31.17it/s] 99%|█████████▊| 74/75 [00:02<00:00, 31.66it/s]100%|██████████| 75/75 [00:02<00:00, 32.36it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.43it/s] 13%|█▎        | 10/75 [00:00<00:01, 33.73it/s] 19%|█▊        | 14/75 [00:00<00:01, 33.16it/s] 24%|██▍       | 18/75 [00:00<00:01, 32.80it/s] 29%|██▉       | 22/75 [00:00<00:01, 32.76it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.66it/s] 40%|████      | 30/75 [00:00<00:01, 32.64it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.46it/s] 51%|█████     | 38/75 [00:01<00:01, 32.50it/s] 56%|█████▌    | 42/75 [00:01<00:01, 31.80it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.18it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.18it/s] 72%|███████▏  | 54/75 [00:01<00:00, 30.31it/s] 77%|███████▋  | 58/75 [00:01<00:00, 30.46it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.07it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 31.75it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.05it/s]100%|██████████| 75/75 [00:02<00:00, 32.36it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.29it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.49it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.15it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.43it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.15it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.01it/s] 40%|████      | 30/75 [00:00<00:01, 32.98it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.13it/s] 51%|█████     | 38/75 [00:01<00:01, 32.26it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.28it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.32it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.29it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.24it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.33it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.95it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.19it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.12it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.27it/s]100%|██████████| 75/75 [00:02<00:00, 33.01it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.41it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.78it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.31it/s] 24%|██▍       | 18/75 [00:00<00:01, 32.46it/s] 29%|██▉       | 22/75 [00:00<00:01, 31.16it/s] 35%|███▍      | 26/75 [00:00<00:01, 30.82it/s] 40%|████      | 30/75 [00:00<00:01, 31.43it/s] 45%|████▌     | 34/75 [00:01<00:01, 30.77it/s] 51%|█████     | 38/75 [00:01<00:01, 29.92it/s] 56%|█████▌    | 42/75 [00:01<00:01, 30.69it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.23it/s] 67%|██████▋   | 50/75 [00:01<00:00, 29.29it/s] 71%|███████   | 53/75 [00:01<00:00, 29.24it/s] 76%|███████▌  | 57/75 [00:01<00:00, 30.30it/s] 81%|████████▏ | 61/75 [00:01<00:00, 30.51it/s] 87%|████████▋ | 65/75 [00:02<00:00, 30.43it/s] 92%|█████████▏| 69/75 [00:02<00:00, 30.99it/s] 97%|█████████▋| 73/75 [00:02<00:00, 31.46it/s]100%|██████████| 75/75 [00:02<00:00, 31.58it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.14it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.48it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.24it/s] 24%|██▍       | 18/75 [00:00<00:01, 30.62it/s] 29%|██▉       | 22/75 [00:00<00:01, 30.79it/s] 35%|███▍      | 26/75 [00:00<00:01, 30.52it/s] 40%|████      | 30/75 [00:00<00:01, 31.19it/s] 45%|████▌     | 34/75 [00:01<00:01, 31.27it/s] 51%|█████     | 38/75 [00:01<00:01, 31.39it/s] 56%|█████▌    | 42/75 [00:01<00:01, 31.40it/s] 61%|██████▏   | 46/75 [00:01<00:00, 30.99it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.04it/s] 72%|███████▏  | 54/75 [00:01<00:00, 30.82it/s] 77%|███████▋  | 58/75 [00:01<00:00, 30.87it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.23it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.71it/s] 93%|█████████▎| 70/75 [00:02<00:00, 30.29it/s] 99%|█████████▊| 74/75 [00:02<00:00, 30.90it/s]100%|██████████| 75/75 [00:02<00:00, 31.73it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.19it/s] 13%|█▎        | 10/75 [00:00<00:01, 34.14it/s] 19%|█▊        | 14/75 [00:00<00:01, 31.70it/s] 24%|██▍       | 18/75 [00:00<00:01, 31.00it/s] 29%|██▉       | 22/75 [00:00<00:01, 31.14it/s] 35%|███▍      | 26/75 [00:00<00:01, 31.52it/s] 40%|████      | 30/75 [00:00<00:01, 31.83it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.04it/s] 51%|█████     | 38/75 [00:01<00:01, 32.00it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.18it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.73it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.90it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.00it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.07it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.16it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.31it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.38it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.36it/s]100%|██████████| 75/75 [00:02<00:00, 32.52it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.29it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.69it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.40it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.52it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.06it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.84it/s] 40%|████      | 30/75 [00:00<00:01, 32.76it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.12it/s] 51%|█████     | 38/75 [00:01<00:01, 32.10it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.25it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.28it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.30it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.19it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.25it/s] 83%|████████▎ | 62/75 [00:01<00:00, 31.49it/s] 88%|████████▊ | 66/75 [00:02<00:00, 31.88it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.05it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.14it/s]100%|██████████| 75/75 [00:02<00:00, 32.96it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.14it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.76it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.41it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.73it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.31it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.13it/s] 40%|████      | 30/75 [00:00<00:01, 32.92it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.79it/s] 51%|█████     | 38/75 [00:01<00:01, 32.72it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.66it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.63it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.65it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.66it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.66it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.63it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.62it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.60it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.15it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.63it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.39it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.72it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.36it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.11it/s] 40%|████      | 30/75 [00:00<00:01, 32.98it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.94it/s] 51%|█████     | 38/75 [00:01<00:01, 32.85it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.75it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.71it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.71it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.66it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.71it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.75it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.77it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.72it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.69it/s]100%|██████████| 75/75 [00:02<00:00, 33.47it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.3_0.7/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.3_0.7/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.3_0.7/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.03it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.57it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.32it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.75it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.44it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.21it/s] 40%|████      | 30/75 [00:00<00:01, 33.01it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.87it/s] 51%|█████     | 38/75 [00:01<00:01, 32.84it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.70it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.69it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.61it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.60it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.60it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.54it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below swap_0.3_0.7
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.694     0.647     0.670       414
           2      0.538     0.433     0.480       210
           3      0.567     0.500     0.531        76
           4      0.361     0.252     0.297       155
           5      0.687     0.631     0.658       957
           6      0.380     0.450     0.412       473
           7      0.748     0.752     0.750       803
           8      0.611     0.608     0.609       286
           9      0.654     0.649     0.651       239
          10      0.504     0.566     0.533       410
          11      0.712     0.703     0.708       556
          12      0.645     0.642     0.643       243
          13      0.746     0.814     0.779       969
          14      0.730     0.697     0.713       132

    accuracy                          0.648      5933
   macro avg      0.572     0.556     0.562      5933
weighted avg      0.648     0.648     0.647      5933

  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.29it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.69it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.49it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.81it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.39it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.11it/s] 40%|████      | 30/75 [00:00<00:01, 32.99it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.90it/s] 51%|█████     | 38/75 [00:01<00:01, 32.79it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.73it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.69it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.69it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.70it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.62it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.63it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.62it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.61it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.61it/s]100%|██████████| 75/75 [00:02<00:00, 33.39it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.98it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.65it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.33it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.73it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.37it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.19it/s] 40%|████      | 30/75 [00:00<00:01, 32.99it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.84it/s] 51%|█████     | 38/75 [00:01<00:01, 32.75it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.71it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.67it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.61it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.65it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.61it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.64it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.63it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.62it/s]100%|██████████| 75/75 [00:02<00:00, 33.37it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.17it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.66it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.41it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.72it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.45it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.17it/s] 40%|████      | 30/75 [00:00<00:01, 33.02it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.94it/s] 51%|█████     | 38/75 [00:01<00:01, 32.82it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.78it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.75it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.66it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.61it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.62it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.64it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.63it/s]100%|██████████| 75/75 [00:02<00:00, 33.40it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.27it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.76it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.42it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.76it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.31it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.08it/s] 40%|████      | 30/75 [00:00<00:01, 32.93it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.86it/s] 51%|█████     | 38/75 [00:01<00:01, 32.79it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.75it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.69it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.69it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.62it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.59it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.55it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.59it/s]100%|██████████| 75/75 [00:02<00:00, 33.40it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.43it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.81it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.46it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.84it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.46it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.24it/s] 40%|████      | 30/75 [00:00<00:01, 33.08it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.97it/s] 51%|█████     | 38/75 [00:01<00:01, 32.88it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.82it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.72it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.71it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.67it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.65it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.60it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.59it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.65it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.62it/s]100%|██████████| 75/75 [00:02<00:00, 33.48it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.25it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.77it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.50it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.77it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.36it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.12it/s] 40%|████      | 30/75 [00:00<00:01, 33.00it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.92it/s] 51%|█████     | 38/75 [00:01<00:01, 32.77it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.74it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.70it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.68it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.66it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.64it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.68it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.63it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.69it/s]100%|██████████| 75/75 [00:02<00:00, 33.46it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.19it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.81it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.56it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.80it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.51it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.24it/s] 40%|████      | 30/75 [00:00<00:01, 33.03it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.81it/s] 51%|█████     | 38/75 [00:01<00:01, 32.73it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.66it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.63it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.61it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.61it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.60it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.59it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.64it/s]100%|██████████| 75/75 [00:02<00:00, 33.44it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.24it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.77it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.54it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.87it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.40it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.14it/s] 40%|████      | 30/75 [00:00<00:01, 32.92it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.76it/s] 51%|█████     | 38/75 [00:01<00:01, 32.70it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.65it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.64it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.65it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.67it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.60it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.61it/s]100%|██████████| 75/75 [00:02<00:00, 33.42it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.17it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.75it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.45it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.82it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.43it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.14it/s] 40%|████      | 30/75 [00:00<00:01, 32.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.84it/s] 51%|█████     | 38/75 [00:01<00:01, 32.70it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.71it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.61it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.63it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.60it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.58it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.5_0.5/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.5_0.5/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.5_0.5/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.96it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.63it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.33it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.67it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.40it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.18it/s] 40%|████      | 30/75 [00:00<00:01, 32.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.81it/s] 51%|█████     | 38/75 [00:01<00:01, 32.74it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.68it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.65it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.57it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.59it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.52it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.54it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.53it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.51it/s]100%|██████████| 75/75 [00:02<00:00, 33.37it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below swap_0.5_0.5
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.650     0.676     0.663       414
           2      0.495     0.429     0.459       210
           3      0.600     0.474     0.529        76
           4      0.377     0.187     0.250       155
           5      0.647     0.644     0.645       957
           6      0.370     0.395     0.382       473
           7      0.754     0.740     0.747       803
           8      0.636     0.598     0.616       286
           9      0.677     0.569     0.618       239
          10      0.461     0.510     0.484       410
          11      0.705     0.714     0.710       556
          12      0.647     0.642     0.645       243
          13      0.738     0.821     0.778       969
          14      0.750     0.750     0.750       132

    accuracy                          0.640      5933
   macro avg      0.567     0.543     0.552      5933
weighted avg      0.636     0.640     0.636      5933

  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.04it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.62it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.42it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.78it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.42it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.14it/s] 40%|████      | 30/75 [00:00<00:01, 32.99it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.87it/s] 51%|█████     | 38/75 [00:01<00:01, 32.81it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.74it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.65it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.58it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.61it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.57it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.42it/s]100%|██████████| 75/75 [00:02<00:00, 33.33it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.01it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.57it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.31it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.69it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.34it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.07it/s] 40%|████      | 30/75 [00:00<00:01, 32.93it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.81it/s] 51%|█████     | 38/75 [00:01<00:01, 32.74it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.67it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.67it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.61it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.57it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.60it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.50it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.54it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.55it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.57it/s]100%|██████████| 75/75 [00:02<00:00, 33.32it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.26it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.71it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.39it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.60it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.23it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.98it/s] 40%|████      | 30/75 [00:00<00:01, 32.93it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.82it/s] 51%|█████     | 38/75 [00:01<00:01, 32.73it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.63it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.65it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.65it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.64it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.65it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.61it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.55it/s]100%|██████████| 75/75 [00:02<00:00, 33.34it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.25it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.66it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.34it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.70it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.35it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.07it/s] 40%|████      | 30/75 [00:00<00:01, 32.92it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.79it/s] 51%|█████     | 38/75 [00:01<00:01, 32.74it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.62it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.57it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.59it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.62it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.54it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.58it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.59it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.54it/s]100%|██████████| 75/75 [00:02<00:00, 33.37it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.04it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.61it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.33it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.71it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.37it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.11it/s] 40%|████      | 30/75 [00:00<00:01, 32.96it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.82it/s] 51%|█████     | 38/75 [00:01<00:01, 32.76it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.75it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.69it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.72it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.71it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.68it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.60it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.58it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.59it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.55it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.99it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.63it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.40it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.72it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.36it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.17it/s] 40%|████      | 30/75 [00:00<00:01, 32.99it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.88it/s] 51%|█████     | 38/75 [00:01<00:01, 32.82it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.74it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.68it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.65it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.67it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.65it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.64it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.65it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.58it/s]100%|██████████| 75/75 [00:02<00:00, 33.42it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.08it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.63it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.34it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.71it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.39it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.13it/s] 40%|████      | 30/75 [00:00<00:01, 33.00it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.85it/s] 51%|█████     | 38/75 [00:01<00:01, 32.77it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.63it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.63it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.62it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.60it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.65it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.61it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.58it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.60it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.12it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.67it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.37it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.74it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.35it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.06it/s] 40%|████      | 30/75 [00:00<00:01, 32.93it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.85it/s] 51%|█████     | 38/75 [00:01<00:01, 32.72it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.67it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.69it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.60it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.56it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.57it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.57it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.57it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.53it/s]100%|██████████| 75/75 [00:02<00:00, 33.37it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.10it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.72it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.49it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.70it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.27it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.04it/s] 40%|████      | 30/75 [00:00<00:01, 32.93it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.87it/s] 51%|█████     | 38/75 [00:01<00:01, 32.79it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.78it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.71it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.71it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.71it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.64it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.56it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.53it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.55it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.60it/s]100%|██████████| 75/75 [00:02<00:00, 33.41it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.7_0.3/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.7_0.3/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.7_0.3/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.10it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.52it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.17it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.55it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.19it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.96it/s] 40%|████      | 30/75 [00:00<00:01, 32.81it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.75it/s] 51%|█████     | 38/75 [00:01<00:01, 32.69it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.53it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.52it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.52it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.49it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.52it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.54it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.51it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.44it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.45it/s]100%|██████████| 75/75 [00:02<00:00, 33.27it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below swap_0.7_0.3
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.663     0.633     0.648       414
           2      0.522     0.457     0.487       210
           3      0.540     0.447     0.489        76
           4      0.343     0.148     0.207       155
           5      0.652     0.649     0.651       957
           6      0.371     0.414     0.391       473
           7      0.731     0.748     0.740       803
           8      0.654     0.580     0.615       286
           9      0.619     0.611     0.615       239
          10      0.451     0.471     0.461       410
          11      0.707     0.721     0.714       556
          12      0.682     0.626     0.652       243
          13      0.736     0.830     0.780       969
          14      0.793     0.727     0.759       132

    accuracy                          0.639      5933
   macro avg      0.564     0.538     0.547      5933
weighted avg      0.634     0.639     0.635      5933

  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.30it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.70it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.50it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.90it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.45it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.21it/s] 40%|████      | 30/75 [00:00<00:01, 32.98it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.86it/s] 51%|█████     | 38/75 [00:01<00:01, 32.81it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.70it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.66it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.56it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.52it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.44it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.46it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.53it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.54it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.52it/s]100%|██████████| 75/75 [00:02<00:00, 33.34it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.87it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.48it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.29it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.66it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.18it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.96it/s] 40%|████      | 30/75 [00:00<00:01, 32.80it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.66it/s] 51%|█████     | 38/75 [00:01<00:01, 32.63it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.55it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.60it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.61it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.52it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.56it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.54it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.43it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.39it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.45it/s]100%|██████████| 75/75 [00:02<00:00, 33.23it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.86it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.53it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.31it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.62it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.28it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.03it/s] 40%|████      | 30/75 [00:00<00:01, 32.83it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.68it/s] 51%|█████     | 38/75 [00:01<00:01, 32.65it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.66it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.57it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.57it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.53it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.50it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.55it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.58it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.54it/s]100%|██████████| 75/75 [00:02<00:00, 33.28it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.75it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.41it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.25it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.60it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.22it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.99it/s] 40%|████      | 30/75 [00:00<00:01, 32.82it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.72it/s] 51%|█████     | 38/75 [00:01<00:01, 32.68it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.65it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.65it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.63it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.49it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.48it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.41it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.45it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.47it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.35it/s]100%|██████████| 75/75 [00:02<00:00, 33.26it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.90it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.61it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.35it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.67it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.33it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.03it/s] 40%|████      | 30/75 [00:00<00:01, 32.82it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.69it/s] 51%|█████     | 38/75 [00:01<00:01, 32.67it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.66it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.62it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.59it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.63it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.51it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.54it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.51it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.47it/s]100%|██████████| 75/75 [00:02<00:00, 33.33it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.78it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.52it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.22it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.61it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.25it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.95it/s] 40%|████      | 30/75 [00:00<00:01, 32.69it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.67it/s] 51%|█████     | 38/75 [00:01<00:01, 32.68it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.67it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.67it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.60it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.58it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.55it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.51it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.55it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.58it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.59it/s]100%|██████████| 75/75 [00:02<00:00, 33.32it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.97it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.48it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.34it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.52it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.06it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.86it/s] 40%|████      | 30/75 [00:00<00:01, 32.76it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.67it/s] 51%|█████     | 38/75 [00:01<00:01, 32.60it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.59it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.62it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.61it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.59it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.60it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.53it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.55it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.52it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.41it/s]100%|██████████| 75/75 [00:02<00:00, 33.27it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.00it/s] 13%|█▎        | 10/75 [00:00<00:01, 33.76it/s] 19%|█▊        | 14/75 [00:00<00:01, 33.15it/s] 24%|██▍       | 18/75 [00:00<00:01, 32.81it/s] 29%|██▉       | 22/75 [00:00<00:01, 32.56it/s] 35%|███▍      | 26/75 [00:00<00:01, 32.43it/s] 40%|████      | 30/75 [00:00<00:01, 32.45it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.38it/s] 51%|█████     | 38/75 [00:01<00:01, 32.23it/s] 56%|█████▌    | 42/75 [00:01<00:01, 31.40it/s] 61%|██████▏   | 46/75 [00:01<00:00, 31.66it/s] 67%|██████▋   | 50/75 [00:01<00:00, 31.75it/s] 72%|███████▏  | 54/75 [00:01<00:00, 31.86it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.01it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.13it/s] 88%|████████▊ | 66/75 [00:02<00:00, 32.19it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.22it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.21it/s]100%|██████████| 75/75 [00:02<00:00, 32.50it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 40.96it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.57it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.19it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.59it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.33it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.07it/s] 40%|████      | 30/75 [00:00<00:01, 32.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.82it/s] 51%|█████     | 38/75 [00:01<00:01, 32.66it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.62it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.56it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.57it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.61it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.58it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.50it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.46it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.54it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.52it/s]100%|██████████| 75/75 [00:02<00:00, 33.33it/s]
loading configuration file ../../../models/data_aug/framing/swap_0.9_0.1/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/swap_0.9_0.1/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/swap_0.9_0.1/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  7%|▋         | 5/75 [00:00<00:01, 41.17it/s] 13%|█▎        | 10/75 [00:00<00:01, 35.70it/s] 19%|█▊        | 14/75 [00:00<00:01, 34.39it/s] 24%|██▍       | 18/75 [00:00<00:01, 33.61it/s] 29%|██▉       | 22/75 [00:00<00:01, 33.28it/s] 35%|███▍      | 26/75 [00:00<00:01, 33.00it/s] 40%|████      | 30/75 [00:00<00:01, 32.89it/s] 45%|████▌     | 34/75 [00:01<00:01, 32.73it/s] 51%|█████     | 38/75 [00:01<00:01, 32.64it/s] 56%|█████▌    | 42/75 [00:01<00:01, 32.62it/s] 61%|██████▏   | 46/75 [00:01<00:00, 32.61it/s] 67%|██████▋   | 50/75 [00:01<00:00, 32.55it/s] 72%|███████▏  | 54/75 [00:01<00:00, 32.46it/s] 77%|███████▋  | 58/75 [00:01<00:00, 32.51it/s] 83%|████████▎ | 62/75 [00:01<00:00, 32.48it/s] 88%|████████▊ | 66/75 [00:01<00:00, 32.41it/s] 93%|█████████▎| 70/75 [00:02<00:00, 32.45it/s] 99%|█████████▊| 74/75 [00:02<00:00, 32.51it/s]100%|██████████| 75/75 [00:02<00:00, 33.31it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
The result is below swap_0.9_0.1
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.669     0.684     0.676       414
           2      0.566     0.386     0.459       210
           3      0.600     0.474     0.529        76
           4      0.330     0.232     0.273       155
           5      0.625     0.691     0.656       957
           6      0.376     0.404     0.389       473
           7      0.718     0.733     0.726       803
           8      0.594     0.594     0.594       286
           9      0.632     0.632     0.632       239
          10      0.508     0.468     0.487       410
          11      0.734     0.703     0.718       556
          12      0.648     0.630     0.639       243
          13      0.744     0.788     0.766       969
          14      0.788     0.674     0.727       132

    accuracy                          0.638      5933
   macro avg      0.569     0.540     0.551      5933
weighted avg      0.635     0.638     0.635      5933


============================= JOB FEEDBACK =============================

NodeName=uc2n901
Job ID: 22160740
Cluster: uc2
User/Group: ma_ytong/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:03:00
CPU Efficiency: 3.17% of 01:34:40 core-walltime
Job Wall-clock time: 00:05:55
Memory Utilized: 3.61 GB
Memory Efficiency: 9.23% of 39.06 GB
