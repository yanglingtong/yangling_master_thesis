2023-04-10 15:10:00.075199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-10 15:10:05.566043: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-10 15:10:06.077437: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-04-10 15:10:06.077485: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-04-10 15:10:19.476660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-04-10 15:10:19.484348: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-04-10 15:10:19.484381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using custom data configuration default-6a4048e0959d3390
Downloading and preparing dataset csv/default to /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7898.88it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 188.75it/s]
Generating train split: 0 examples [00:00, ? examples/s]/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/datasets/download/streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'
  return pd.read_csv(xopen(filepath_or_buffer, "rb", use_auth_token=use_auth_token), **kwargs)
Generating train split: 5933 examples [00:00, 11449.41 examples/s]                                                                  Dataset csv downloaded and prepared to /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 22.92it/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:01<00:00,  1.17s/ba]100%|██████████| 1/1 [00:01<00:00,  1.17s/ba]
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  4%|▍         | 3/75 [00:00<00:02, 29.44it/s]  8%|▊         | 6/75 [00:00<00:02, 26.55it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.44it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.75it/s] 20%|██        | 15/75 [00:00<00:02, 27.00it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.13it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.28it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.34it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.37it/s] 40%|████      | 30/75 [00:01<00:01, 27.40it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.40it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.35it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.38it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.41it/s] 60%|██████    | 45/75 [00:01<00:01, 27.38it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.41it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.46it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.48it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.45it/s] 80%|████████  | 60/75 [00:02<00:00, 27.45it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.42it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.40it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.40it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.41it/s]100%|██████████| 75/75 [00:02<00:00, 27.14it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.26ba/s]100%|██████████| 1/1 [00:00<00:00,  2.26ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 32.80it/s] 11%|█         | 8/75 [00:00<00:02, 28.64it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.09it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.84it/s] 23%|██▎       | 17/75 [00:00<00:02, 27.71it/s] 27%|██▋       | 20/75 [00:00<00:01, 27.64it/s] 31%|███       | 23/75 [00:00<00:01, 27.57it/s] 35%|███▍      | 26/75 [00:00<00:01, 27.54it/s] 39%|███▊      | 29/75 [00:01<00:01, 27.52it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.48it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.44it/s] 51%|█████     | 38/75 [00:01<00:01, 27.41it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.44it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.47it/s] 63%|██████▎   | 47/75 [00:01<00:01, 27.47it/s] 67%|██████▋   | 50/75 [00:01<00:00, 27.47it/s] 71%|███████   | 53/75 [00:01<00:00, 27.48it/s] 75%|███████▍  | 56/75 [00:02<00:00, 27.49it/s] 79%|███████▊  | 59/75 [00:02<00:00, 27.48it/s] 83%|████████▎ | 62/75 [00:02<00:00, 27.51it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.55it/s] 91%|█████████ | 68/75 [00:02<00:00, 27.54it/s] 95%|█████████▍| 71/75 [00:02<00:00, 27.52it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.51it/s]100%|██████████| 75/75 [00:02<00:00, 27.89it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.28ba/s]100%|██████████| 1/1 [00:00<00:00,  2.27ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.76it/s] 11%|█         | 8/75 [00:00<00:02, 30.76it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.16it/s] 20%|██        | 15/75 [00:00<00:02, 28.57it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.16it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.94it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.79it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.69it/s] 40%|████      | 30/75 [00:01<00:01, 27.59it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.54it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.51it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.49it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.47it/s] 60%|██████    | 45/75 [00:01<00:01, 27.44it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.43it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.43it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.44it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.45it/s] 80%|████████  | 60/75 [00:02<00:00, 27.43it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.41it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.41it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.39it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.38it/s]100%|██████████| 75/75 [00:02<00:00, 28.05it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.82it/s] 11%|█         | 8/75 [00:00<00:02, 30.67it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.12it/s] 20%|██        | 15/75 [00:00<00:02, 28.56it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.21it/s] 28%|██▊       | 21/75 [00:00<00:01, 28.02it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.86it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.77it/s] 40%|████      | 30/75 [00:01<00:01, 27.66it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.62it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.54it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.52it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.52it/s] 60%|██████    | 45/75 [00:01<00:01, 27.51it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.48it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.33it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.28it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.27it/s] 80%|████████  | 60/75 [00:02<00:00, 27.29it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.28it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.26it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.24it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.21it/s]100%|██████████| 75/75 [00:02<00:00, 27.97it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.14ba/s]100%|██████████| 1/1 [00:00<00:00,  2.14ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 35.42it/s] 11%|█         | 8/75 [00:00<00:02, 29.82it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.67it/s] 20%|██        | 15/75 [00:00<00:02, 28.17it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.53it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.48it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.41it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.38it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.35it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.27it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.23it/s] 60%|██████    | 45/75 [00:01<00:01, 27.25it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.28it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.34it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.35it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.38it/s] 80%|████████  | 60/75 [00:02<00:00, 27.39it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.28it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.22it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.31it/s]100%|██████████| 75/75 [00:02<00:00, 27.85it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.06ba/s]100%|██████████| 1/1 [00:00<00:00,  2.06ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.71it/s] 11%|█         | 8/75 [00:00<00:02, 30.61it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.12it/s] 20%|██        | 15/75 [00:00<00:02, 28.55it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.17it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.94it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.79it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.68it/s] 40%|████      | 30/75 [00:01<00:01, 27.65it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.59it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.56it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.51it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.50it/s] 60%|██████    | 45/75 [00:01<00:01, 27.49it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.50it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.51it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.49it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.51it/s] 80%|████████  | 60/75 [00:02<00:00, 27.48it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.49it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.48it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.48it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 28.13it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.56it/s] 11%|█         | 8/75 [00:00<00:02, 30.51it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.04it/s] 20%|██        | 15/75 [00:00<00:02, 28.50it/s] 24%|██▍       | 18/75 [00:00<00:03, 16.71it/s] 28%|██▊       | 21/75 [00:00<00:02, 19.03it/s] 32%|███▏      | 24/75 [00:01<00:02, 21.03it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.69it/s] 40%|████      | 30/75 [00:01<00:01, 23.94it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.90it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.60it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.13it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.52it/s] 60%|██████    | 45/75 [00:01<00:01, 26.77it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.96it/s] 68%|██████▊   | 51/75 [00:02<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:02<00:00, 27.19it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.22it/s] 80%|████████  | 60/75 [00:02<00:00, 27.28it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.32it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.33it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.34it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.35it/s]100%|██████████| 75/75 [00:02<00:00, 25.87it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.21ba/s]100%|██████████| 1/1 [00:00<00:00,  2.21ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 30.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.82it/s] 20%|██        | 15/75 [00:00<00:02, 28.30it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.94it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.71it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.57it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.47it/s] 40%|████      | 30/75 [00:01<00:01, 27.39it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.36it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.32it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.29it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.28it/s] 60%|██████    | 45/75 [00:01<00:01, 27.27it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.26it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.27it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.28it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.26it/s] 80%|████████  | 60/75 [00:02<00:00, 27.25it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.24it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.24it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.23it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.21it/s]100%|██████████| 75/75 [00:02<00:00, 27.89it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.22ba/s]100%|██████████| 1/1 [00:00<00:00,  2.22ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.76it/s] 11%|█         | 8/75 [00:00<00:02, 30.71it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.13it/s] 20%|██        | 15/75 [00:00<00:02, 28.54it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.17it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.90it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.76it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.63it/s] 40%|████      | 30/75 [00:01<00:01, 27.58it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.52it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.49it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.47it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.39it/s] 60%|██████    | 45/75 [00:01<00:01, 27.39it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.39it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.41it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.39it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.40it/s] 80%|████████  | 60/75 [00:02<00:00, 27.40it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.37it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.39it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.38it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 28.07it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.1_0.9/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.1_0.9/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.1_0.9/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]100%|██████████| 1/1 [00:00<00:00,  2.25ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.84it/s] 11%|█         | 8/75 [00:00<00:02, 30.70it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.17it/s] 20%|██        | 15/75 [00:00<00:02, 28.59it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.21it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.97it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.82it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.71it/s] 40%|████      | 30/75 [00:01<00:01, 27.66it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.58it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.55it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.55it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.54it/s] 60%|██████    | 45/75 [00:01<00:01, 27.51it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.50it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.48it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.48it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.47it/s] 80%|████████  | 60/75 [00:02<00:00, 27.42it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.43it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.44it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.45it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 28.13it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
The result is below bt_0.1_0.9
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.651     0.715     0.681       414
           2      0.497     0.433     0.463       210
           3      0.514     0.474     0.493        76
           4      0.273     0.226     0.247       155
           5      0.677     0.687     0.682       957
           6      0.386     0.362     0.373       473
           7      0.734     0.776     0.754       803
           8      0.625     0.605     0.615       286
           9      0.707     0.607     0.653       239
          10      0.475     0.502     0.488       410
          11      0.733     0.692     0.712       556
          12      0.637     0.658     0.648       243
          13      0.766     0.802     0.783       969
          14      0.724     0.697     0.710       132

    accuracy                          0.648      5933
   macro avg      0.560     0.549     0.554      5933
weighted avg      0.643     0.648     0.645      5933

  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00,  2.32ba/s]100%|██████████| 1/1 [00:00<00:00,  2.32ba/s]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 32.87it/s] 11%|█         | 8/75 [00:00<00:02, 28.67it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.09it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.82it/s] 23%|██▎       | 17/75 [00:00<00:02, 27.69it/s] 27%|██▋       | 20/75 [00:00<00:01, 27.60it/s] 31%|███       | 23/75 [00:00<00:01, 27.52it/s] 35%|███▍      | 26/75 [00:00<00:01, 27.48it/s] 39%|███▊      | 29/75 [00:01<00:01, 27.46it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.45it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.39it/s] 51%|█████     | 38/75 [00:01<00:01, 27.39it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.38it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.39it/s] 63%|██████▎   | 47/75 [00:01<00:01, 27.41it/s] 67%|██████▋   | 50/75 [00:01<00:00, 27.40it/s] 71%|███████   | 53/75 [00:01<00:00, 27.41it/s] 75%|███████▍  | 56/75 [00:02<00:00, 27.40it/s] 79%|███████▊  | 59/75 [00:02<00:00, 27.37it/s] 83%|████████▎ | 62/75 [00:02<00:00, 27.39it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.40it/s] 91%|█████████ | 68/75 [00:02<00:00, 27.37it/s] 95%|█████████▍| 71/75 [00:02<00:00, 27.38it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 27.81it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.83it/s] 11%|█         | 8/75 [00:00<00:02, 30.79it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.20it/s] 20%|██        | 15/75 [00:00<00:02, 28.62it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.20it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.97it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.81it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.70it/s] 40%|████      | 30/75 [00:01<00:01, 27.65it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.60it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.55it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.53it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.52it/s] 60%|██████    | 45/75 [00:01<00:01, 27.50it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.47it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.46it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.47it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.47it/s] 80%|████████  | 60/75 [00:02<00:00, 27.45it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.44it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.45it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.43it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.42it/s]100%|██████████| 75/75 [00:02<00:00, 28.09it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.61it/s] 11%|█         | 8/75 [00:00<00:02, 30.67it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.07it/s] 20%|██        | 15/75 [00:00<00:02, 28.50it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.12it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.89it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.72it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.63it/s] 40%|████      | 30/75 [00:01<00:01, 27.57it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.52it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.47it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.45it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.40it/s] 60%|██████    | 45/75 [00:01<00:01, 27.40it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.40it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.39it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.39it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.41it/s] 80%|████████  | 60/75 [00:02<00:00, 27.41it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.39it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.38it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.39it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 28.02it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.75it/s] 11%|█         | 8/75 [00:00<00:02, 30.65it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.13it/s] 20%|██        | 15/75 [00:00<00:02, 28.55it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.19it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.96it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.80it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.68it/s] 40%|████      | 30/75 [00:01<00:01, 27.62it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.56it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.53it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.50it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.50it/s] 60%|██████    | 45/75 [00:01<00:01, 27.45it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.41it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.40it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.40it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.43it/s] 80%|████████  | 60/75 [00:02<00:00, 27.43it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.38it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.37it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.40it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.41it/s]100%|██████████| 75/75 [00:02<00:00, 28.10it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.55it/s] 11%|█         | 8/75 [00:00<00:02, 30.53it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.01it/s] 20%|██        | 15/75 [00:00<00:02, 28.46it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.12it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.91it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.75it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.63it/s] 40%|████      | 30/75 [00:01<00:01, 27.55it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.51it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.46it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.45it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.42it/s] 60%|██████    | 45/75 [00:01<00:01, 27.41it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.40it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.36it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.33it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.32it/s] 80%|████████  | 60/75 [00:02<00:00, 27.31it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.28it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.28it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.26it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.23it/s]100%|██████████| 75/75 [00:02<00:00, 28.01it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.70it/s] 11%|█         | 8/75 [00:00<00:02, 30.67it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.12it/s] 20%|██        | 15/75 [00:00<00:02, 28.55it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.17it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.95it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.77it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.65it/s] 40%|████      | 30/75 [00:01<00:01, 27.59it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.55it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.52it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.45it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.46it/s] 60%|██████    | 45/75 [00:01<00:01, 27.46it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.47it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.47it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.43it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.43it/s] 80%|████████  | 60/75 [00:02<00:00, 27.45it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.42it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.44it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.39it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.37it/s]100%|██████████| 75/75 [00:02<00:00, 28.10it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.52it/s] 11%|█         | 8/75 [00:00<00:02, 30.54it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.04it/s] 20%|██        | 15/75 [00:00<00:02, 28.46it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.11it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.88it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.68it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.59it/s] 40%|████      | 30/75 [00:01<00:01, 27.53it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.48it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.39it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.34it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.31it/s] 60%|██████    | 45/75 [00:01<00:01, 27.28it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.29it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.32it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.32it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.33it/s] 80%|████████  | 60/75 [00:02<00:00, 27.33it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.31it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.30it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.32it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.33it/s]100%|██████████| 75/75 [00:02<00:00, 28.00it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.66it/s] 11%|█         | 8/75 [00:00<00:02, 30.56it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.05it/s] 20%|██        | 15/75 [00:00<00:02, 28.51it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.15it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.91it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.77it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.65it/s] 40%|████      | 30/75 [00:01<00:01, 27.58it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.50it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.47it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.46it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.45it/s] 60%|██████    | 45/75 [00:01<00:01, 27.45it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.42it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.41it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.40it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.43it/s] 80%|████████  | 60/75 [00:02<00:00, 27.42it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.42it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.42it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.45it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.40it/s]100%|██████████| 75/75 [00:02<00:00, 28.08it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.22it/s] 11%|█         | 8/75 [00:00<00:02, 30.33it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.80it/s] 20%|██        | 15/75 [00:00<00:02, 28.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.46it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.35it/s] 40%|████      | 30/75 [00:01<00:01, 27.29it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.17it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.13it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.12it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.11it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.09it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.11it/s] 80%|████████  | 60/75 [00:02<00:00, 27.08it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.07it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.06it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.03it/s]100%|██████████| 75/75 [00:02<00:00, 27.75it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.3_0.7/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.3_0.7/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.3_0.7/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.31it/s] 11%|█         | 8/75 [00:00<00:02, 30.33it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.83it/s] 20%|██        | 15/75 [00:00<00:02, 28.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.90it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.68it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.54it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.28it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.22it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.21it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.19it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.20it/s] 60%|██████    | 45/75 [00:01<00:01, 27.19it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.20it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.19it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.20it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.15it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.17it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.17it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.16it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.17it/s]100%|██████████| 75/75 [00:02<00:00, 27.82it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below bt_0.3_0.7
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.670     0.623     0.646       414
           2      0.440     0.452     0.446       210
           3      0.514     0.487     0.500        76
           4      0.258     0.271     0.264       155
           5      0.673     0.652     0.662       957
           6      0.387     0.402     0.394       473
           7      0.731     0.785     0.757       803
           8      0.627     0.622     0.625       286
           9      0.637     0.603     0.619       239
          10      0.444     0.476     0.459       410
          11      0.781     0.674     0.724       556
          12      0.633     0.654     0.644       243
          13      0.761     0.788     0.774       969
          14      0.759     0.765     0.762       132

    accuracy                          0.639      5933
   macro avg      0.554     0.550     0.552      5933
weighted avg      0.641     0.639     0.639      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.11it/s] 11%|█         | 8/75 [00:00<00:02, 30.26it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.71it/s] 20%|██        | 15/75 [00:00<00:02, 28.17it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.82it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.47it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.34it/s] 40%|████      | 30/75 [00:01<00:01, 27.28it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.18it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.17it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.15it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.09it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.10it/s] 80%|████████  | 60/75 [00:02<00:00, 27.10it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.12it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.07it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.05it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.04it/s]100%|██████████| 75/75 [00:02<00:00, 27.71it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.26it/s] 11%|█         | 8/75 [00:00<00:02, 30.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.82it/s] 20%|██        | 15/75 [00:00<00:02, 28.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.88it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.27it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.24it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.22it/s] 60%|██████    | 45/75 [00:01<00:01, 27.19it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.16it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.18it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.19it/s] 80%|████████  | 60/75 [00:02<00:00, 27.12it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.06it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.07it/s]100%|██████████| 75/75 [00:02<00:00, 27.76it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.20it/s] 11%|█         | 8/75 [00:00<00:02, 30.24it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.30it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.25it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.21it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.17it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.15it/s] 60%|██████    | 45/75 [00:01<00:01, 27.13it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.15it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.10it/s] 80%|████████  | 60/75 [00:02<00:00, 27.09it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.08it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.11it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.11it/s]100%|██████████| 75/75 [00:02<00:00, 27.73it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.42it/s] 11%|█         | 8/75 [00:00<00:02, 30.34it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.81it/s] 20%|██        | 15/75 [00:00<00:02, 28.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.49it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.30it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.21it/s] 60%|██████    | 45/75 [00:01<00:01, 27.17it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.16it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.16it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.14it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.13it/s] 80%|████████  | 60/75 [00:02<00:00, 27.10it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.11it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.12it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.12it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.20it/s] 11%|█         | 8/75 [00:00<00:02, 30.26it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.74it/s] 20%|██        | 15/75 [00:00<00:02, 28.19it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.44it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.31it/s] 40%|████      | 30/75 [00:01<00:01, 27.23it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.17it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.10it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.07it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.08it/s] 60%|██████    | 45/75 [00:01<00:01, 27.09it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.06it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.05it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.02it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.01it/s] 80%|████████  | 60/75 [00:02<00:00, 27.04it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.02it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.05it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.07it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.71it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.33it/s] 11%|█         | 8/75 [00:00<00:02, 30.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.84it/s] 20%|██        | 15/75 [00:00<00:02, 28.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.93it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.71it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.49it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.34it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.26it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.16it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.17it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.17it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.14it/s] 80%|████████  | 60/75 [00:02<00:00, 27.11it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.04it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.04it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.10it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.12it/s]100%|██████████| 75/75 [00:02<00:00, 27.80it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.27it/s] 11%|█         | 8/75 [00:00<00:02, 30.26it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.75it/s] 20%|██        | 15/75 [00:00<00:02, 28.20it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.85it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.65it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.27it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.18it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.12it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.13it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.12it/s] 60%|██████    | 45/75 [00:01<00:01, 27.07it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.04it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.02it/s] 72%|███████▏  | 54/75 [00:02<00:01, 15.41it/s] 76%|███████▌  | 57/75 [00:02<00:01, 17.70it/s] 80%|████████  | 60/75 [00:02<00:00, 19.76it/s] 84%|████████▍ | 63/75 [00:02<00:00, 21.48it/s] 88%|████████▊ | 66/75 [00:02<00:00, 22.90it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.01it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.84it/s]100%|██████████| 75/75 [00:02<00:00, 25.13it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.34it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.83it/s] 20%|██        | 15/75 [00:00<00:02, 28.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.91it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.68it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.50it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.40it/s] 40%|████      | 30/75 [00:01<00:01, 27.35it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.30it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.23it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.18it/s] 60%|██████    | 45/75 [00:01<00:01, 27.18it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.16it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.15it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.13it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.12it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.13it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.13it/s]100%|██████████| 75/75 [00:02<00:00, 27.81it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.31it/s] 11%|█         | 8/75 [00:00<00:02, 30.32it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.78it/s] 20%|██        | 15/75 [00:00<00:02, 28.20it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.34it/s] 40%|████      | 30/75 [00:01<00:01, 27.27it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.23it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.19it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.18it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.14it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.13it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.11it/s] 80%|████████  | 60/75 [00:02<00:00, 27.08it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.00it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.03it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.02it/s]100%|██████████| 75/75 [00:02<00:00, 27.75it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.5_0.5/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.5_0.5/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.5_0.5/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.25it/s] 11%|█         | 8/75 [00:00<00:02, 30.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.78it/s] 20%|██        | 15/75 [00:00<00:02, 28.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.88it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.50it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.41it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.18it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.17it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.15it/s] 60%|██████    | 45/75 [00:01<00:01, 27.12it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.13it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.16it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.07it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.77it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below bt_0.5_0.5
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.676     0.681     0.679       414
           2      0.505     0.452     0.477       210
           3      0.507     0.461     0.483        76
           4      0.320     0.265     0.290       155
           5      0.685     0.642     0.663       957
           6      0.383     0.421     0.401       473
           7      0.716     0.783     0.748       803
           8      0.593     0.636     0.614       286
           9      0.647     0.628     0.637       239
          10      0.491     0.483     0.487       410
          11      0.774     0.685     0.727       556
          12      0.651     0.630     0.640       243
          13      0.756     0.805     0.780       969
          14      0.696     0.712     0.704       132

    accuracy                          0.646      5933
   macro avg      0.560     0.552     0.555      5933
weighted avg      0.645     0.646     0.644      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.14it/s] 11%|█         | 8/75 [00:00<00:02, 30.27it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.74it/s] 20%|██        | 15/75 [00:00<00:02, 28.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.40it/s] 40%|████      | 30/75 [00:01<00:01, 27.26it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.19it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.16it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.14it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.13it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.12it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.05it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.02it/s] 80%|████████  | 60/75 [00:02<00:00, 27.02it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.04it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.05it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.07it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.09it/s]100%|██████████| 75/75 [00:02<00:00, 27.70it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.45it/s] 11%|█         | 8/75 [00:00<00:02, 30.36it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.84it/s] 20%|██        | 15/75 [00:00<00:02, 28.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.65it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.41it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.23it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.22it/s] 60%|██████    | 45/75 [00:01<00:01, 27.20it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.19it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.16it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.16it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.13it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.11it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.13it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.15it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.18it/s] 11%|█         | 8/75 [00:00<00:02, 30.23it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.18it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.59it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.28it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.22it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.16it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.15it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.14it/s] 60%|██████    | 45/75 [00:01<00:01, 27.12it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.12it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.11it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.10it/s] 80%|████████  | 60/75 [00:02<00:00, 27.08it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.06it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.05it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.07it/s]100%|██████████| 75/75 [00:02<00:00, 27.71it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.27it/s] 11%|█         | 8/75 [00:00<00:02, 30.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.82it/s] 20%|██        | 15/75 [00:00<00:02, 28.27it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.92it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.68it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.40it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.29it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.23it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.16it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.14it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s] 80%|████████  | 60/75 [00:02<00:00, 27.15it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.12it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.10it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.05it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.19it/s] 11%|█         | 8/75 [00:00<00:02, 30.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.80it/s] 20%|██        | 15/75 [00:00<00:02, 28.22it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.64it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.47it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.37it/s] 40%|████      | 30/75 [00:01<00:01, 27.31it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.17it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.15it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.13it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.12it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.04it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.06it/s] 80%|████████  | 60/75 [00:02<00:00, 27.07it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.07it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.08it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.08it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.89it/s]100%|██████████| 75/75 [00:02<00:00, 27.58it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 30.35it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.81it/s] 20%|██        | 15/75 [00:00<00:02, 28.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.26it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.19it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.13it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.16it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.14it/s] 60%|██████    | 45/75 [00:01<00:01, 27.15it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.17it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.18it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.18it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.16it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.13it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.13it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.12it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.33it/s] 11%|█         | 8/75 [00:00<00:02, 30.29it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.75it/s] 20%|██        | 15/75 [00:00<00:02, 28.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.81it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.59it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.43it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.30it/s] 40%|████      | 30/75 [00:01<00:01, 27.25it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.18it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.14it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.12it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.12it/s] 60%|██████    | 45/75 [00:01<00:01, 27.09it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.05it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.06it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.03it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.06it/s] 80%|████████  | 60/75 [00:02<00:00, 27.03it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.02it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.99it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.02it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.02it/s]100%|██████████| 75/75 [00:02<00:00, 27.71it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.32it/s] 11%|█         | 8/75 [00:00<00:02, 30.26it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.63it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.34it/s] 40%|████      | 30/75 [00:01<00:01, 27.30it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.26it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.23it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.15it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.14it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.14it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.15it/s] 80%|████████  | 60/75 [00:02<00:00, 27.12it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.07it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.10it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.12it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.18it/s] 11%|█         | 8/75 [00:00<00:02, 30.26it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.78it/s] 20%|██        | 15/75 [00:00<00:02, 28.22it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.41it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.30it/s] 40%|████      | 30/75 [00:01<00:01, 27.25it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.17it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.12it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.09it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.10it/s] 60%|██████    | 45/75 [00:01<00:01, 27.10it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.11it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.11it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.10it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.11it/s] 80%|████████  | 60/75 [00:02<00:00, 27.09it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.07it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.07it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.04it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.02it/s]100%|██████████| 75/75 [00:02<00:00, 27.73it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.7_0.3/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.7_0.3/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.7_0.3/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 30.35it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.79it/s] 20%|██        | 15/75 [00:00<00:02, 28.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.34it/s] 40%|████      | 30/75 [00:01<00:01, 27.29it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.27it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.23it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.22it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.16it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.16it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.13it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.14it/s] 80%|████████  | 60/75 [00:02<00:00, 27.10it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.10it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.12it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.09it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below bt_0.7_0.3
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.677     0.655     0.666       414
           2      0.475     0.367     0.414       210
           3      0.487     0.500     0.494        76
           4      0.293     0.232     0.259       155
           5      0.666     0.641     0.653       957
           6      0.345     0.406     0.373       473
           7      0.729     0.763     0.746       803
           8      0.635     0.601     0.618       286
           9      0.671     0.623     0.646       239
          10      0.462     0.524     0.491       410
          11      0.704     0.694     0.699       556
          12      0.624     0.642     0.633       243
          13      0.766     0.767     0.766       969
          14      0.742     0.697     0.719       132

    accuracy                          0.633      5933
   macro avg      0.552     0.541     0.545      5933
weighted avg      0.634     0.633     0.633      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.22it/s] 11%|█         | 8/75 [00:00<00:02, 30.27it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.74it/s] 20%|██        | 15/75 [00:00<00:02, 28.19it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.56it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.37it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.28it/s] 40%|████      | 30/75 [00:01<00:01, 27.22it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.20it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.15it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.14it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.11it/s] 60%|██████    | 45/75 [00:01<00:01, 27.08it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.04it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.06it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.04it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.03it/s] 80%|████████  | 60/75 [00:02<00:00, 27.02it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.93it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.96it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.98it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.99it/s]100%|██████████| 75/75 [00:02<00:00, 27.66it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.29it/s] 11%|█         | 8/75 [00:00<00:02, 30.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.22it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.64it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.50it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.22it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.15it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.14it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.17it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.18it/s] 80%|████████  | 60/75 [00:02<00:00, 27.13it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.11it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.12it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.11it/s]100%|██████████| 75/75 [00:02<00:00, 27.76it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.18it/s] 11%|█         | 8/75 [00:00<00:02, 30.24it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.72it/s] 20%|██        | 15/75 [00:00<00:02, 28.18it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.81it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.60it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.43it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.31it/s] 40%|████      | 30/75 [00:01<00:01, 27.23it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.20it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.17it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.13it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.13it/s] 60%|██████    | 45/75 [00:01<00:01, 27.13it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.08it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.08it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.08it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.08it/s] 80%|████████  | 60/75 [00:02<00:00, 27.07it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.07it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.08it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.08it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.05it/s]100%|██████████| 75/75 [00:02<00:00, 27.69it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.27it/s] 11%|█         | 8/75 [00:00<00:02, 30.29it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.77it/s] 20%|██        | 15/75 [00:00<00:02, 28.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.37it/s] 40%|████      | 30/75 [00:01<00:01, 27.28it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.22it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.15it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.12it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.12it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.14it/s] 80%|████████  | 60/75 [00:02<00:00, 27.11it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.04it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.04it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.04it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.01it/s]100%|██████████| 75/75 [00:02<00:00, 27.75it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 30.29it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.74it/s] 20%|██        | 15/75 [00:00<00:02, 28.20it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.61it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.33it/s] 40%|████      | 30/75 [00:01<00:01, 27.25it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.21it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.18it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.15it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.11it/s] 60%|██████    | 45/75 [00:01<00:01, 27.06it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.05it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.07it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.09it/s] 80%|████████  | 60/75 [00:02<00:00, 27.09it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.07it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.05it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.06it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.04it/s]100%|██████████| 75/75 [00:02<00:00, 27.73it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.07it/s] 11%|█         | 8/75 [00:00<00:02, 30.21it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.77it/s] 20%|██        | 15/75 [00:00<00:02, 28.20it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.63it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.47it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.29it/s] 40%|████      | 30/75 [00:01<00:01, 27.23it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.19it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.16it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.14it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.14it/s] 60%|██████    | 45/75 [00:01<00:01, 27.12it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.08it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.06it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.07it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.08it/s] 80%|████████  | 60/75 [00:02<00:00, 27.09it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.12it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.10it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.09it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.74it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.25it/s] 11%|█         | 8/75 [00:00<00:02, 30.25it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.19it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.83it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.59it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.43it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.28it/s] 40%|████      | 30/75 [00:01<00:01, 27.20it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.12it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.10it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.11it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.13it/s] 60%|██████    | 45/75 [00:01<00:01, 27.08it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.06it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.03it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.04it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.05it/s] 80%|████████  | 60/75 [00:02<00:00, 27.05it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.08it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.05it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.04it/s]100%|██████████| 75/75 [00:02<00:00, 27.72it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.17it/s] 11%|█         | 8/75 [00:00<00:02, 30.25it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.86it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.44it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.33it/s] 40%|████      | 30/75 [00:01<00:01, 27.26it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.21it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.18it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.16it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.15it/s] 60%|██████    | 45/75 [00:01<00:01, 27.12it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.11it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.11it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.03it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.05it/s] 80%|████████  | 60/75 [00:02<00:00, 27.04it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.06it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.10it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.09it/s]100%|██████████| 75/75 [00:02<00:00, 27.74it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.16it/s] 11%|█         | 8/75 [00:00<00:02, 30.20it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.69it/s] 20%|██        | 15/75 [00:00<00:02, 28.11it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.75it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.49it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.37it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.24it/s] 40%|████      | 30/75 [00:01<00:01, 27.16it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.14it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.13it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.11it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.11it/s] 60%|██████    | 45/75 [00:01<00:01, 27.12it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.09it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.10it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.09it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.07it/s] 80%|████████  | 60/75 [00:02<00:00, 27.07it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.03it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.02it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.00it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.99it/s]100%|██████████| 75/75 [00:02<00:00, 27.69it/s]
loading configuration file ../../../models/data_aug/framing/bt_0.9_0.1/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/bt_0.9_0.1/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/bt_0.9_0.1/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 34.59it/s] 11%|█         | 8/75 [00:00<00:02, 29.75it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.53it/s] 20%|██        | 15/75 [00:00<00:02, 28.06it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.75it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.57it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.46it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.27it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.25it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.22it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.21it/s] 60%|██████    | 45/75 [00:01<00:01, 27.17it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.12it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.11it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.13it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.07it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 26.54it/s]
The result is below bt_0.9_0.1
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.641     0.655     0.648       414
           2      0.519     0.381     0.440       210
           3      0.604     0.421     0.496        76
           4      0.314     0.239     0.271       155
           5      0.636     0.671     0.653       957
           6      0.367     0.408     0.386       473
           7      0.722     0.758     0.740       803
           8      0.614     0.629     0.622       286
           9      0.648     0.615     0.631       239
          10      0.470     0.466     0.468       410
          11      0.712     0.721     0.717       556
          12      0.635     0.609     0.622       243
          13      0.762     0.761     0.761       969
          14      0.761     0.674     0.715       132

    accuracy                          0.633      5933
   macro avg      0.560     0.534     0.545      5933
weighted avg      0.631     0.633     0.631      5933


============================= JOB FEEDBACK =============================

NodeName=uc2n520
Job ID: 22003136
Cluster: uc2
User/Group: ma_ytong/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:03:31
CPU Efficiency: 5.17% of 01:08:00 core-walltime
Job Wall-clock time: 00:06:48
Memory Utilized: 3.01 GB
Memory Efficiency: 7.70% of 39.06 GB
