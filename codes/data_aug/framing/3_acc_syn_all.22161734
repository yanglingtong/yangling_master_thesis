2023-04-30 15:17:06.556941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-30 15:17:11.018722: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-30 15:17:11.683504: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-04-30 15:17:11.683560: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-04-30 15:17:26.567476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-04-30 15:17:26.582457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-04-30 15:17:26.582566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Using custom data configuration default-6a4048e0959d3390
Found cached dataset csv (/home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  4.40it/s]100%|██████████| 1/1 [00:00<00:00,  4.39it/s]
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-625c8a49e46d5dd8.arrow
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 32.65it/s] 11%|█         | 8/75 [00:00<00:02, 28.37it/s] 15%|█▍        | 11/75 [00:00<00:02, 27.83it/s] 19%|█▊        | 14/75 [00:00<00:02, 26.39it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.07it/s] 27%|██▋       | 20/75 [00:00<00:02, 25.04it/s] 31%|███       | 23/75 [00:00<00:02, 25.67it/s] 35%|███▍      | 26/75 [00:00<00:01, 26.13it/s] 39%|███▊      | 29/75 [00:01<00:01, 26.44it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.57it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.22it/s] 51%|█████     | 38/75 [00:01<00:01, 26.54it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.71it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.56it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.70it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.80it/s] 71%|███████   | 53/75 [00:01<00:00, 26.82it/s] 75%|███████▍  | 56/75 [00:02<00:00, 26.97it/s] 79%|███████▊  | 59/75 [00:02<00:00, 26.93it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.94it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.03it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.95it/s] 95%|█████████▍| 71/75 [00:02<00:00, 26.89it/s] 99%|█████████▊| 74/75 [00:02<00:00, 26.99it/s]100%|██████████| 75/75 [00:02<00:00, 26.54it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-16f1a20df289d911.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.11it/s] 11%|█         | 8/75 [00:00<00:02, 29.40it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.37it/s] 20%|██        | 15/75 [00:00<00:02, 26.91it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.27it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.57it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.06it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.66it/s] 40%|████      | 30/75 [00:01<00:01, 26.07it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.89it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.52it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.94it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.21it/s] 60%|██████    | 45/75 [00:01<00:01, 24.92it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.51it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.90it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.08it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.36it/s] 80%|████████  | 60/75 [00:02<00:00, 26.57it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.64it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.78it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.85it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.86it/s]100%|██████████| 75/75 [00:02<00:00, 26.72it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c96175d13133a58e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.02it/s] 11%|█         | 8/75 [00:00<00:02, 28.94it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.13it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.08it/s] 23%|██▎       | 17/75 [00:00<00:02, 23.41it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.52it/s] 31%|███       | 23/75 [00:00<00:02, 24.68it/s] 35%|███▍      | 26/75 [00:01<00:01, 24.78it/s] 39%|███▊      | 29/75 [00:01<00:01, 25.42it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.37it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.68it/s] 51%|█████     | 38/75 [00:01<00:01, 25.33it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.36it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.50it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.95it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.77it/s] 71%|███████   | 53/75 [00:02<00:00, 25.30it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.89it/s] 79%|███████▊  | 59/75 [00:02<00:00, 26.20it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.41it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.62it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.68it/s] 95%|█████████▍| 71/75 [00:02<00:00, 26.69it/s] 99%|█████████▊| 74/75 [00:02<00:00, 26.81it/s]100%|██████████| 75/75 [00:02<00:00, 26.24it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3734f8b77241d8b7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.15it/s] 11%|█         | 8/75 [00:00<00:02, 30.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.67it/s] 20%|██        | 15/75 [00:00<00:02, 28.13it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.09it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.08it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.00it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.02it/s] 40%|████      | 30/75 [00:01<00:01, 24.85it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.01it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.44it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.16it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.21it/s] 60%|██████    | 45/75 [00:01<00:01, 25.12it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.72it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.52it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.36it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.87it/s] 80%|████████  | 60/75 [00:02<00:00, 26.25it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.25it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.53it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.15it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.23it/s]100%|██████████| 75/75 [00:02<00:00, 26.44it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-18ad1da83475ff3d.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.46it/s] 11%|█         | 8/75 [00:00<00:02, 29.98it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.71it/s] 20%|██        | 15/75 [00:00<00:02, 27.41it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.55it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.84it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.95it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.00it/s] 40%|████      | 30/75 [00:01<00:01, 27.12it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.12it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.48it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.00it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.33it/s] 60%|██████    | 45/75 [00:01<00:01, 26.56it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.76it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.86it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.51it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.02it/s] 80%|████████  | 60/75 [00:02<00:00, 25.80it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.16it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.47it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.67it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.78it/s]100%|██████████| 75/75 [00:02<00:00, 27.06it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ca57cd2c2f3e6dc7.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.41it/s] 11%|█         | 8/75 [00:00<00:02, 30.39it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.94it/s] 20%|██        | 15/75 [00:00<00:02, 28.32it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.30it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.60it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.75it/s] 36%|███▌      | 27/75 [00:00<00:01, 26.83it/s] 40%|████      | 30/75 [00:01<00:01, 26.89it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.89it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.88it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.60it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.08it/s] 60%|██████    | 45/75 [00:01<00:01, 26.38it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.60it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.73it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.49it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.69it/s] 80%|████████  | 60/75 [00:02<00:00, 26.74it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.38it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.63it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.74it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.87it/s]100%|██████████| 75/75 [00:02<00:00, 27.11it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ca097e86947179db.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.48it/s] 11%|█         | 8/75 [00:00<00:02, 30.25it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.89it/s] 20%|██        | 15/75 [00:00<00:02, 28.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.69it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.49it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.33it/s] 40%|████      | 30/75 [00:01<00:01, 27.24it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.13it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.99it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.99it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.76it/s] 60%|██████    | 45/75 [00:01<00:01, 26.04it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.36it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.53it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.62it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.79it/s] 80%|████████  | 60/75 [00:02<00:00, 26.89it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.89it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.99it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.00it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.00it/s]100%|██████████| 75/75 [00:02<00:00, 27.51it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:01<00:00,  1.70s/ba]100%|██████████| 1/1 [00:01<00:00,  1.70s/ba]
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:02, 33.28it/s] 11%|█         | 8/75 [00:00<00:02, 28.89it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.23it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.87it/s] 23%|██▎       | 17/75 [00:00<00:02, 27.70it/s] 27%|██▋       | 20/75 [00:00<00:01, 27.60it/s] 31%|███       | 23/75 [00:00<00:01, 27.53it/s] 35%|███▍      | 26/75 [00:00<00:01, 27.51it/s] 39%|███▊      | 29/75 [00:01<00:01, 27.47it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.43it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.40it/s] 51%|█████     | 38/75 [00:01<00:01, 27.37it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.34it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.29it/s] 63%|██████▎   | 47/75 [00:01<00:01, 27.24it/s] 67%|██████▋   | 50/75 [00:01<00:00, 27.25it/s] 71%|███████   | 53/75 [00:01<00:00, 27.30it/s] 75%|███████▍  | 56/75 [00:02<00:00, 27.32it/s] 79%|███████▊  | 59/75 [00:02<00:00, 27.32it/s] 83%|████████▎ | 62/75 [00:02<00:00, 27.31it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.32it/s] 91%|█████████ | 68/75 [00:02<00:00, 27.34it/s] 95%|█████████▍| 71/75 [00:02<00:00, 27.31it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.29it/s]100%|██████████| 75/75 [00:02<00:00, 27.82it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.47it/s] 11%|█         | 8/75 [00:00<00:02, 30.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.94it/s] 20%|██        | 15/75 [00:00<00:02, 28.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.81it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.43it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.31it/s] 40%|████      | 30/75 [00:01<00:01, 27.28it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.20it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.07it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.13it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.38it/s] 60%|██████    | 45/75 [00:01<00:01, 26.56it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.75it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.83it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.82it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.94it/s] 80%|████████  | 60/75 [00:02<00:00, 26.96it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.98it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.06it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.02it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.02it/s]100%|██████████| 75/75 [00:02<00:00, 27.63it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.1_0.9/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.1_0.9/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.1_0.9/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.54it/s] 11%|█         | 8/75 [00:00<00:02, 30.41it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.93it/s] 20%|██        | 15/75 [00:00<00:02, 26.84it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.92it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.05it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.06it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.04it/s] 40%|████      | 30/75 [00:01<00:01, 27.07it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.09it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.05it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.12it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.10it/s] 60%|██████    | 45/75 [00:01<00:01, 26.73it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.87it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.95it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.79it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.91it/s] 80%|████████  | 60/75 [00:02<00:00, 26.95it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.03it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.09it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.10it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.11it/s]100%|██████████| 75/75 [00:02<00:00, 27.55it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below syn_0.1_0.9
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.687     0.688     0.688       414
           2      0.476     0.386     0.426       210
           3      0.554     0.539     0.547        76
           4      0.289     0.239     0.261       155
           5      0.678     0.639     0.658       957
           6      0.372     0.433     0.400       473
           7      0.736     0.780     0.757       803
           8      0.640     0.622     0.631       286
           9      0.627     0.619     0.623       239
          10      0.485     0.483     0.484       410
          11      0.738     0.716     0.727       556
          12      0.643     0.630     0.636       243
          13      0.753     0.798     0.775       969
          14      0.741     0.652     0.694       132

    accuracy                          0.644      5933
   macro avg      0.561     0.548     0.554      5933
weighted avg      0.643     0.644     0.643      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.36it/s] 11%|█         | 8/75 [00:00<00:02, 30.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.78it/s] 20%|██        | 15/75 [00:00<00:02, 28.19it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.32it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.58it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.13it/s] 36%|███▌      | 27/75 [00:00<00:01, 25.72it/s] 40%|████      | 30/75 [00:01<00:01, 26.16it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.86it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.00it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.66it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.49it/s] 60%|██████    | 45/75 [00:01<00:01, 25.96it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.33it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.58it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.96it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.35it/s] 80%|████████  | 60/75 [00:02<00:00, 26.54it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.65it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.84it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.91it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.91it/s]100%|██████████| 75/75 [00:02<00:00, 26.86it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.17it/s] 11%|█         | 8/75 [00:00<00:02, 27.59it/s] 15%|█▍        | 11/75 [00:00<00:02, 27.51it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.42it/s] 23%|██▎       | 17/75 [00:00<00:02, 26.28it/s] 27%|██▋       | 20/75 [00:00<00:02, 26.63it/s] 31%|███       | 23/75 [00:00<00:01, 26.79it/s] 35%|███▍      | 26/75 [00:00<00:01, 26.87it/s] 39%|███▊      | 29/75 [00:01<00:01, 27.02it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.05it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.71it/s] 51%|█████     | 38/75 [00:01<00:01, 26.87it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.37it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.43it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.68it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.18it/s] 71%|███████   | 53/75 [00:01<00:00, 24.69it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.43it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.91it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.25it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.57it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.75it/s] 95%|█████████▍| 71/75 [00:02<00:00, 26.87it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.00it/s]100%|██████████| 75/75 [00:02<00:00, 26.95it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.41it/s] 11%|█         | 8/75 [00:00<00:02, 29.49it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.50it/s] 20%|██        | 15/75 [00:00<00:02, 27.32it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.43it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.70it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.21it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.74it/s] 40%|████      | 30/75 [00:01<00:01, 24.00it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.55it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.51it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.36it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.91it/s] 60%|██████    | 45/75 [00:01<00:01, 26.18it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.55it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.71it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.71it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.92it/s] 80%|████████  | 60/75 [00:02<00:00, 26.97it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.30it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.64it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.17it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.42it/s]100%|██████████| 75/75 [00:02<00:00, 26.53it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.67it/s] 11%|█         | 8/75 [00:00<00:02, 29.32it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.47it/s] 20%|██        | 15/75 [00:00<00:02, 27.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.41it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.02it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.75it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.95it/s] 40%|████      | 30/75 [00:01<00:01, 26.40it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.99it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.15it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.54it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.10it/s] 60%|██████    | 45/75 [00:01<00:01, 26.12it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.48it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.46it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.88it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.33it/s] 80%|████████  | 60/75 [00:02<00:00, 26.55it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.72it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.94it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.04it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.04it/s]100%|██████████| 75/75 [00:02<00:00, 26.95it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.60it/s] 11%|█         | 8/75 [00:00<00:02, 30.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.96it/s] 20%|██        | 15/75 [00:00<00:02, 28.33it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.13it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.54it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.72it/s] 36%|███▌      | 27/75 [00:00<00:01, 26.83it/s] 40%|████      | 30/75 [00:01<00:01, 27.02it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.06it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.23it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.60it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.79it/s] 60%|██████    | 45/75 [00:01<00:01, 26.87it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.01it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.33it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.49it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.78it/s] 80%|████████  | 60/75 [00:02<00:00, 26.91it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.97it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.11it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.14it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.14it/s]100%|██████████| 75/75 [00:02<00:00, 27.45it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.72it/s] 11%|█         | 8/75 [00:00<00:02, 29.27it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.41it/s] 20%|██        | 15/75 [00:00<00:02, 23.69it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.74it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.78it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.48it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.94it/s] 40%|████      | 30/75 [00:01<00:01, 26.38it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.63it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.74it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.95it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.02it/s] 60%|██████    | 45/75 [00:01<00:01, 25.68it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.10it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.42it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.60it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.85it/s] 80%|████████  | 60/75 [00:02<00:00, 26.97it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.05it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.16it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.16it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.14it/s]100%|██████████| 75/75 [00:02<00:00, 26.88it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.61it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.95it/s] 20%|██        | 15/75 [00:00<00:02, 28.36it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.84it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.06it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.11it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.12it/s] 40%|████      | 30/75 [00:01<00:01, 27.20it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.75it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.27it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.59it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.74it/s] 60%|██████    | 45/75 [00:01<00:01, 26.87it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.02it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.08it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.61it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.81it/s] 80%|████████  | 60/75 [00:02<00:00, 26.90it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.43it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.71it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.84it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.90it/s]100%|██████████| 75/75 [00:02<00:00, 27.47it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.44it/s] 11%|█         | 8/75 [00:00<00:02, 29.01it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.32it/s] 19%|█▊        | 14/75 [00:00<00:03, 20.15it/s] 23%|██▎       | 17/75 [00:00<00:02, 21.53it/s] 27%|██▋       | 20/75 [00:00<00:02, 23.08it/s] 31%|███       | 23/75 [00:00<00:02, 24.20it/s] 35%|███▍      | 26/75 [00:01<00:01, 25.01it/s] 39%|███▊      | 29/75 [00:01<00:01, 25.65it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.00it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.54it/s] 51%|█████     | 38/75 [00:01<00:01, 26.03it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.05it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.31it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.58it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.67it/s] 71%|███████   | 53/75 [00:02<00:00, 26.68it/s] 75%|███████▍  | 56/75 [00:02<00:00, 26.83it/s] 79%|███████▊  | 59/75 [00:02<00:00, 26.88it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.87it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.97it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.92it/s] 95%|█████████▍| 71/75 [00:02<00:00, 26.93it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.03it/s]100%|██████████| 75/75 [00:02<00:00, 26.25it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.29it/s] 11%|█         | 8/75 [00:00<00:02, 30.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.79it/s] 20%|██        | 15/75 [00:00<00:02, 28.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.68it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.49it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.38it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.26it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.22it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.20it/s] 60%|██████    | 45/75 [00:01<00:01, 27.21it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.16it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.14it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.14it/s] 80%|████████  | 60/75 [00:02<00:00, 27.15it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.13it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.16it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.11it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.3_0.7/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.3_0.7/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.3_0.7/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.66it/s] 11%|█         | 8/75 [00:00<00:02, 30.58it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.11it/s] 20%|██        | 15/75 [00:00<00:02, 28.54it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.16it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.94it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.80it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.69it/s] 40%|████      | 30/75 [00:01<00:01, 27.63it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.54it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.48it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.47it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.48it/s] 60%|██████    | 45/75 [00:01<00:01, 27.42it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.43it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.42it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.39it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.40it/s] 80%|████████  | 60/75 [00:02<00:00, 27.40it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.38it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.40it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.41it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.39it/s]100%|██████████| 75/75 [00:02<00:00, 28.09it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below syn_0.3_0.7
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.655     0.674     0.664       414
           2      0.450     0.386     0.415       210
           3      0.642     0.447     0.527        76
           4      0.288     0.219     0.249       155
           5      0.638     0.683     0.660       957
           6      0.401     0.433     0.417       473
           7      0.726     0.746     0.736       803
           8      0.628     0.584     0.605       286
           9      0.695     0.590     0.638       239
          10      0.486     0.478     0.482       410
          11      0.708     0.698     0.703       556
          12      0.604     0.634     0.618       243
          13      0.754     0.787     0.770       969
          14      0.785     0.636     0.703       132

    accuracy                          0.637      5933
   macro avg      0.564     0.533     0.546      5933
weighted avg      0.634     0.637     0.634      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.43it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.76it/s] 20%|██        | 15/75 [00:00<00:02, 28.22it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.87it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.64it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.50it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.41it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.27it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.23it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.21it/s] 60%|██████    | 45/75 [00:01<00:01, 27.22it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.21it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.20it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.20it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.19it/s] 80%|████████  | 60/75 [00:02<00:00, 27.19it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.19it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.18it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.17it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.18it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.39it/s] 11%|█         | 8/75 [00:00<00:02, 30.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.86it/s] 20%|██        | 15/75 [00:00<00:02, 28.30it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.95it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.73it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.58it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.47it/s] 40%|████      | 30/75 [00:01<00:01, 27.39it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.35it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.22it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.22it/s] 60%|██████    | 45/75 [00:01<00:01, 27.21it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.23it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.24it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.24it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.22it/s] 80%|████████  | 60/75 [00:02<00:00, 27.23it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.23it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.23it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.21it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.22it/s]100%|██████████| 75/75 [00:02<00:00, 27.84it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.40it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.82it/s] 20%|██        | 15/75 [00:00<00:02, 28.27it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.90it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.41it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.19it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.18it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.18it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.17it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.17it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.17it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.17it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.15it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.40it/s] 11%|█         | 8/75 [00:00<00:02, 30.41it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.88it/s] 20%|██        | 15/75 [00:00<00:02, 28.31it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.95it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.75it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.59it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.47it/s] 40%|████      | 30/75 [00:01<00:01, 27.36it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.25it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.24it/s] 60%|██████    | 45/75 [00:01<00:01, 27.23it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.23it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.23it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.20it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.21it/s] 80%|████████  | 60/75 [00:02<00:00, 27.22it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.19it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.17it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.16it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.16it/s]100%|██████████| 75/75 [00:02<00:00, 27.86it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 30.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.82it/s] 20%|██        | 15/75 [00:00<00:02, 28.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.52it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.39it/s] 40%|████      | 30/75 [00:01<00:01, 27.34it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.24it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.12it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.11it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.09it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.11it/s] 80%|████████  | 60/75 [00:02<00:00, 27.13it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.13it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.15it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.11it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.09it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.34it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.88it/s] 20%|██        | 15/75 [00:00<00:02, 28.31it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.92it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.70it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.55it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.38it/s] 40%|████      | 30/75 [00:01<00:01, 27.34it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.29it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.25it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.24it/s] 60%|██████    | 45/75 [00:01<00:01, 27.21it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.19it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.21it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.20it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.21it/s] 80%|████████  | 60/75 [00:02<00:00, 27.20it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.21it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.21it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.22it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.21it/s]100%|██████████| 75/75 [00:02<00:00, 27.86it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.40it/s] 11%|█         | 8/75 [00:00<00:02, 30.34it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.81it/s] 20%|██        | 15/75 [00:00<00:02, 28.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.64it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.49it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.30it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.16it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.13it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.15it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.13it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.10it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.12it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.09it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.34it/s] 11%|█         | 8/75 [00:00<00:02, 30.39it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.84it/s] 20%|██        | 15/75 [00:00<00:02, 28.29it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.93it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.71it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.53it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.44it/s] 40%|████      | 30/75 [00:01<00:01, 27.36it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.29it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.24it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s] 60%|██████    | 45/75 [00:01<00:01, 27.18it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.18it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.20it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.18it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.15it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.15it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.13it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.16it/s]100%|██████████| 75/75 [00:02<00:00, 27.83it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.15it/s] 11%|█         | 8/75 [00:00<00:02, 30.23it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.75it/s] 20%|██        | 15/75 [00:00<00:02, 28.18it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.85it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.59it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.45it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.36it/s] 40%|████      | 30/75 [00:01<00:01, 27.31it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.23it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.19it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.16it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.16it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.17it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.17it/s] 80%|████████  | 60/75 [00:02<00:00, 27.17it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.14it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.15it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.12it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.09it/s]100%|██████████| 75/75 [00:02<00:00, 27.78it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.5_0.5/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.5_0.5/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.5_0.5/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.39it/s] 11%|█         | 8/75 [00:00<00:02, 30.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.87it/s] 20%|██        | 15/75 [00:00<00:02, 28.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.94it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.71it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.56it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.45it/s] 40%|████      | 30/75 [00:01<00:01, 27.37it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.33it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.30it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.27it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.26it/s] 60%|██████    | 45/75 [00:01<00:01, 27.27it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.26it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.24it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.25it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.21it/s] 80%|████████  | 60/75 [00:02<00:00, 27.22it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.17it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.15it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.12it/s]100%|██████████| 75/75 [00:02<00:00, 27.86it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below syn_0.5_0.5
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.643     0.691     0.666       414
           2      0.465     0.438     0.451       210
           3      0.556     0.461     0.504        76
           4      0.296     0.155     0.203       155
           5      0.698     0.624     0.659       957
           6      0.393     0.446     0.418       473
           7      0.702     0.765     0.732       803
           8      0.639     0.601     0.620       286
           9      0.659     0.598     0.627       239
          10      0.474     0.544     0.507       410
          11      0.757     0.696     0.725       556
          12      0.639     0.654     0.646       243
          13      0.746     0.805     0.775       969
          14      0.780     0.697     0.736       132

    accuracy                          0.643      5933
   macro avg      0.563     0.545     0.551      5933
weighted avg      0.642     0.643     0.640      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.15it/s] 11%|█         | 8/75 [00:00<00:02, 30.28it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.78it/s] 20%|██        | 15/75 [00:00<00:02, 28.25it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.89it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.40it/s] 40%|████      | 30/75 [00:01<00:01, 27.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.24it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.17it/s] 60%|██████    | 45/75 [00:01<00:01, 27.14it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.14it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.14it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.14it/s] 80%|████████  | 60/75 [00:02<00:00, 27.14it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.16it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.15it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.13it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.15it/s]100%|██████████| 75/75 [00:02<00:00, 27.76it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.52it/s] 11%|█         | 8/75 [00:00<00:02, 30.45it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.92it/s] 20%|██        | 15/75 [00:00<00:02, 28.33it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.97it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.70it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.55it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.44it/s] 40%|████      | 30/75 [00:01<00:01, 27.38it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.34it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.30it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.22it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.16it/s] 60%|██████    | 45/75 [00:01<00:01, 27.11it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.08it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.08it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.00it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.94it/s] 80%|████████  | 60/75 [00:02<00:00, 26.90it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.87it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.93it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.94it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.94it/s]100%|██████████| 75/75 [00:02<00:00, 27.70it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.17it/s] 11%|█         | 8/75 [00:00<00:02, 30.16it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.62it/s] 20%|██        | 15/75 [00:00<00:02, 28.06it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.69it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.47it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.31it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.19it/s] 40%|████      | 30/75 [00:01<00:01, 27.15it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.09it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.04it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.97it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.97it/s] 60%|██████    | 45/75 [00:01<00:01, 26.95it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.97it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.92it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.94it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.94it/s] 80%|████████  | 60/75 [00:02<00:00, 26.94it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.93it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.92it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.92it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.88it/s]100%|██████████| 75/75 [00:02<00:00, 27.55it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.18it/s] 11%|█         | 8/75 [00:00<00:02, 30.14it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.66it/s] 20%|██        | 15/75 [00:00<00:02, 28.09it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.68it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.45it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.35it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.22it/s] 40%|████      | 30/75 [00:01<00:01, 27.20it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.17it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.16it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.11it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.12it/s] 60%|██████    | 45/75 [00:01<00:01, 27.11it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.10it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.10it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.07it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.02it/s] 80%|████████  | 60/75 [00:02<00:00, 27.02it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.87it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.83it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.86it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.87it/s]100%|██████████| 75/75 [00:02<00:00, 27.62it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 35.69it/s] 11%|█         | 8/75 [00:00<00:02, 29.92it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.48it/s] 20%|██        | 15/75 [00:00<00:02, 27.90it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.58it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.39it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.20it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.13it/s] 40%|████      | 30/75 [00:01<00:01, 27.06it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.03it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.00it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.99it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.97it/s] 60%|██████    | 45/75 [00:01<00:01, 26.95it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.96it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.90it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.93it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.89it/s] 80%|████████  | 60/75 [00:02<00:00, 26.90it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.90it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.92it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.94it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.94it/s]100%|██████████| 75/75 [00:02<00:00, 27.54it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.23it/s] 11%|█         | 8/75 [00:00<00:02, 30.43it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.98it/s] 20%|██        | 15/75 [00:00<00:02, 28.43it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.06it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.80it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.59it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.50it/s] 40%|████      | 30/75 [00:01<00:01, 27.45it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.41it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.27it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.24it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.24it/s] 60%|██████    | 45/75 [00:01<00:01, 27.23it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.24it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.23it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.19it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.21it/s] 80%|████████  | 60/75 [00:02<00:00, 27.21it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.22it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.24it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.25it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.25it/s]100%|██████████| 75/75 [00:02<00:00, 27.90it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.03it/s] 11%|█         | 8/75 [00:00<00:02, 30.21it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.77it/s] 20%|██        | 15/75 [00:00<00:02, 28.26it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.91it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.66it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.48it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.37it/s] 40%|████      | 30/75 [00:01<00:01, 27.33it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.23it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.17it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.15it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.15it/s] 60%|██████    | 45/75 [00:01<00:01, 27.16it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.17it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.13it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s] 80%|████████  | 60/75 [00:02<00:00, 27.10it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.11it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.13it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.15it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.16it/s]100%|██████████| 75/75 [00:02<00:00, 27.79it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.33it/s] 11%|█         | 8/75 [00:00<00:02, 30.41it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.87it/s] 20%|██        | 15/75 [00:00<00:02, 28.32it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.88it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.68it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.56it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.47it/s] 40%|████      | 30/75 [00:01<00:01, 27.39it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.25it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.20it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.16it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.05it/s] 60%|██████    | 45/75 [00:01<00:01, 27.03it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.04it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.14it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.13it/s] 80%|████████  | 60/75 [00:02<00:00, 27.11it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.11it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.19it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.19it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.21it/s]100%|██████████| 75/75 [00:02<00:00, 27.80it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.34it/s] 11%|█         | 8/75 [00:00<00:02, 30.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.88it/s] 20%|██        | 15/75 [00:00<00:02, 28.33it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.99it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.77it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.59it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.45it/s] 40%|████      | 30/75 [00:01<00:01, 27.40it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.35it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.33it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.29it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.28it/s] 60%|██████    | 45/75 [00:01<00:01, 27.24it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.24it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.21it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.19it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.18it/s] 80%|████████  | 60/75 [00:02<00:00, 27.19it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.19it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.22it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.21it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.17it/s]100%|██████████| 75/75 [00:02<00:00, 27.86it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.7_0.3/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.7_0.3/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.7_0.3/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.62it/s] 11%|█         | 8/75 [00:00<00:02, 30.51it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.95it/s] 20%|██        | 15/75 [00:00<00:02, 28.39it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.00it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.76it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.63it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.47it/s] 40%|████      | 30/75 [00:01<00:01, 27.31it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.27it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.29it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.30it/s] 60%|██████    | 45/75 [00:01<00:01, 27.26it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.27it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.30it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.34it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.35it/s] 80%|████████  | 60/75 [00:02<00:00, 27.36it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.35it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.36it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.30it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.26it/s]100%|██████████| 75/75 [00:02<00:00, 27.92it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_1/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_1/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2765a3cb094c36cc.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
The result is below syn_0.7_0.3
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.668     0.667     0.667       414
           2      0.532     0.438     0.480       210
           3      0.521     0.487     0.503        76
           4      0.363     0.213     0.268       155
           5      0.690     0.638     0.663       957
           6      0.369     0.412     0.389       473
           7      0.714     0.809     0.758       803
           8      0.640     0.591     0.615       286
           9      0.634     0.594     0.613       239
          10      0.469     0.488     0.478       410
          11      0.741     0.700     0.720       556
          12      0.644     0.626     0.635       243
          13      0.743     0.822     0.781       969
          14      0.795     0.674     0.730       132

    accuracy                          0.646      5933
   macro avg      0.568     0.544     0.553      5933
weighted avg      0.643     0.646     0.642      5933

  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.43it/s] 11%|█         | 8/75 [00:00<00:02, 30.43it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.89it/s] 20%|██        | 15/75 [00:00<00:02, 28.34it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.98it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.73it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.55it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.44it/s] 40%|████      | 30/75 [00:01<00:01, 27.29it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.23it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.22it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.23it/s] 60%|██████    | 45/75 [00:01<00:01, 27.22it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.23it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.22it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.22it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.20it/s] 80%|████████  | 60/75 [00:02<00:00, 27.13it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.16it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.15it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.17it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.17it/s]100%|██████████| 75/75 [00:02<00:00, 27.81it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_2/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_2/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-906be79fc78f9632.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.41it/s] 11%|█         | 8/75 [00:00<00:02, 30.45it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.98it/s] 20%|██        | 15/75 [00:00<00:02, 28.43it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.06it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.81it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.64it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.48it/s] 40%|████      | 30/75 [00:01<00:01, 27.38it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.30it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.25it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.26it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.25it/s] 60%|██████    | 45/75 [00:01<00:01, 27.24it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.25it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.28it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.21it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.19it/s] 80%|████████  | 60/75 [00:02<00:00, 27.23it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.22it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.20it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.21it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.20it/s]100%|██████████| 75/75 [00:02<00:00, 27.85it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_3/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_3/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47bad7898f9a3f26.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 594
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.28it/s] 11%|█         | 8/75 [00:00<00:02, 25.78it/s] 15%|█▍        | 11/75 [00:00<00:02, 26.33it/s] 19%|█▊        | 14/75 [00:00<00:02, 26.60it/s] 23%|██▎       | 17/75 [00:00<00:02, 25.52it/s] 27%|██▋       | 20/75 [00:00<00:02, 26.06it/s] 31%|███       | 23/75 [00:00<00:01, 26.37it/s] 35%|███▍      | 26/75 [00:00<00:01, 26.57it/s] 39%|███▊      | 29/75 [00:01<00:01, 26.74it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.67it/s] 47%|████▋     | 35/75 [00:01<00:01, 23.39it/s] 51%|█████     | 38/75 [00:01<00:01, 24.41it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.12it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.68it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.10it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.38it/s] 71%|███████   | 53/75 [00:02<00:00, 26.49it/s] 75%|███████▍  | 56/75 [00:02<00:00, 26.73it/s] 79%|███████▊  | 59/75 [00:02<00:00, 26.85it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.41it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.64it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.73it/s] 95%|█████████▍| 71/75 [00:02<00:00, 26.80it/s] 99%|█████████▊| 74/75 [00:02<00:00, 26.94it/s]100%|██████████| 75/75 [00:02<00:00, 26.51it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_4/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_4/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eab6766e691e782a.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 35.67it/s] 11%|█         | 8/75 [00:00<00:02, 30.15it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.75it/s] 20%|██        | 15/75 [00:00<00:02, 28.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.85it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.63it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.51it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.42it/s] 40%|████      | 30/75 [00:01<00:01, 27.34it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.29it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.27it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.21it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.21it/s] 60%|██████    | 45/75 [00:01<00:01, 27.20it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.22it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.21it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.19it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.16it/s] 80%|████████  | 60/75 [00:02<00:00, 27.15it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.11it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.15it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.16it/s]100%|██████████| 75/75 [00:02<00:00, 27.73it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_5/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_5/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9b8dafa3c2a7846e.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.77it/s] 11%|█         | 8/75 [00:00<00:02, 30.48it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.04it/s] 20%|██        | 15/75 [00:00<00:02, 28.37it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.59it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.13it/s] 32%|███▏      | 24/75 [00:00<00:01, 26.44it/s] 36%|███▌      | 27/75 [00:00<00:01, 26.68it/s] 40%|████      | 30/75 [00:01<00:01, 26.85it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.90it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.23it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.58it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.72it/s] 60%|██████    | 45/75 [00:01<00:01, 26.79it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.96it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.25it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.80it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.22it/s] 80%|████████  | 60/75 [00:02<00:00, 26.49it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.61it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.82it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.89it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.93it/s]100%|██████████| 75/75 [00:02<00:00, 27.24it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_6/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_6/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_6.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01236dc88e6bf9bb.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.60it/s] 11%|█         | 8/75 [00:00<00:02, 29.36it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.45it/s] 20%|██        | 15/75 [00:00<00:02, 27.11it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.30it/s] 28%|██▊       | 21/75 [00:00<00:02, 26.63it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.57it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.85it/s] 40%|████      | 30/75 [00:01<00:01, 26.32it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.91it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.56it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.06it/s] 56%|█████▌    | 42/75 [00:01<00:01, 26.41it/s] 60%|██████    | 45/75 [00:01<00:01, 26.02it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.44it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.97it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.62it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.13it/s] 80%|████████  | 60/75 [00:02<00:00, 25.80it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.03it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.41it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.02it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.35it/s]100%|██████████| 75/75 [00:02<00:00, 26.74it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_7/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_7/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_7.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1da4be4bbdb5200f.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.13it/s] 11%|█         | 8/75 [00:00<00:02, 30.43it/s] 16%|█▌        | 12/75 [00:00<00:02, 28.97it/s] 20%|██        | 15/75 [00:00<00:02, 28.41it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.08it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.86it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.66it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.57it/s] 40%|████      | 30/75 [00:01<00:01, 27.51it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.44it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.41it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.38it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.36it/s] 60%|██████    | 45/75 [00:01<00:01, 27.37it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.36it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.34it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.34it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.33it/s] 80%|████████  | 60/75 [00:02<00:00, 27.30it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.21it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.22it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.24it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.23it/s]100%|██████████| 75/75 [00:02<00:00, 27.97it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_8/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_8/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f47dc15f59bf9bf0.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.37it/s] 11%|█         | 8/75 [00:00<00:02, 28.94it/s] 15%|█▍        | 11/75 [00:00<00:02, 28.37it/s] 19%|█▊        | 14/75 [00:00<00:02, 27.96it/s] 23%|██▎       | 17/75 [00:00<00:02, 27.57it/s] 27%|██▋       | 20/75 [00:00<00:01, 27.53it/s] 31%|███       | 23/75 [00:00<00:01, 27.40it/s] 35%|███▍      | 26/75 [00:00<00:01, 27.36it/s] 39%|███▊      | 29/75 [00:01<00:01, 27.40it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.28it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.17it/s] 51%|█████     | 38/75 [00:01<00:01, 27.25it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.89it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.99it/s] 63%|██████▎   | 47/75 [00:01<00:01, 27.09it/s] 67%|██████▋   | 50/75 [00:01<00:00, 27.11it/s] 71%|███████   | 53/75 [00:01<00:00, 27.01it/s] 75%|███████▍  | 56/75 [00:02<00:00, 27.14it/s] 79%|███████▊  | 59/75 [00:02<00:00, 27.11it/s] 83%|████████▎ | 62/75 [00:02<00:00, 27.07it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.17it/s] 91%|█████████ | 68/75 [00:02<00:00, 27.15it/s] 95%|█████████▍| 71/75 [00:02<00:00, 27.12it/s] 99%|█████████▊| 74/75 [00:02<00:00, 27.18it/s]100%|██████████| 75/75 [00:02<00:00, 27.70it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_9/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_9/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_9.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e8620dae530c390.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.65it/s] 11%|█         | 8/75 [00:00<00:02, 30.61it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.03it/s] 20%|██        | 15/75 [00:00<00:02, 28.48it/s] 24%|██▍       | 18/75 [00:00<00:02, 28.12it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.90it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.75it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.63it/s] 40%|████      | 30/75 [00:01<00:01, 27.56it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.49it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.42it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.43it/s] 56%|█████▌    | 42/75 [00:01<00:01, 27.39it/s] 60%|██████    | 45/75 [00:01<00:01, 27.38it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.37it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.39it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.37it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.37it/s] 80%|████████  | 60/75 [00:02<00:00, 27.38it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.37it/s] 88%|████████▊ | 66/75 [00:02<00:00, 27.34it/s] 92%|█████████▏| 69/75 [00:02<00:00, 27.32it/s] 96%|█████████▌| 72/75 [00:02<00:00, 27.32it/s]100%|██████████| 75/75 [00:02<00:00, 28.04it/s]
loading configuration file ../../../models/data_aug/framing/syn_0.9_0.1/bm_10/config.json
Model config RobertaConfig {
  "_name_or_path": "roberta-base",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.25.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file ../../../models/data_aug/framing/syn_0.9_0.1/bm_10/pytorch_model.bin
All model checkpoint weights were used when initializing RobertaForSequenceClassification.

All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/data_aug/framing/syn_0.9_0.1/bm_10.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
Loading cached processed dataset at /home/ma/ma_ma/ma_ytong/.cache/huggingface/datasets/csv/default-6a4048e0959d3390/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b3412f7b37c808ce.arrow
No `TrainingArguments` passed, using `output_dir=tmp_trainer`.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
***** Running Prediction *****
  Num examples = 593
  Batch size = 8
  0%|          | 0/75 [00:00<?, ?it/s]  5%|▌         | 4/75 [00:00<00:01, 36.66it/s] 11%|█         | 8/75 [00:00<00:02, 30.52it/s] 16%|█▌        | 12/75 [00:00<00:02, 29.03it/s] 20%|██        | 15/75 [00:00<00:02, 27.49it/s] 24%|██▍       | 18/75 [00:00<00:02, 27.22it/s] 28%|██▊       | 21/75 [00:00<00:01, 27.28it/s] 32%|███▏      | 24/75 [00:00<00:01, 27.23it/s] 36%|███▌      | 27/75 [00:00<00:01, 27.23it/s] 40%|████      | 30/75 [00:01<00:01, 27.27it/s] 44%|████▍     | 33/75 [00:01<00:02, 14.35it/s] 48%|████▊     | 36/75 [00:01<00:02, 16.71it/s] 52%|█████▏    | 39/75 [00:01<00:01, 18.95it/s] 56%|█████▌    | 42/75 [00:01<00:01, 20.86it/s] 60%|██████    | 45/75 [00:01<00:01, 21.69it/s] 64%|██████▍   | 48/75 [00:02<00:01, 23.15it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.22it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.99it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.66it/s] 80%|████████  | 60/75 [00:02<00:00, 26.10it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.31it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.66it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.80it/s] 96%|█████████▌| 72/75 [00:02<00:00, 26.92it/s]100%|██████████| 75/75 [00:03<00:00, 24.37it/s]
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/pfs/data5/home/ma/ma_ma/ma_ytong/someNewEnvName/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
The result is below syn_0.9_0.1
              precision    recall  f1-score   support

           0      0.000     0.000     0.000        10
           1      0.664     0.621     0.642       414
           2      0.577     0.410     0.479       210
           3      0.623     0.434     0.512        76
           4      0.425     0.200     0.272       155
           5      0.593     0.724     0.652       957
           6      0.385     0.345     0.364       473
           7      0.691     0.753     0.721       803
           8      0.634     0.587     0.610       286
           9      0.657     0.586     0.619       239
          10      0.473     0.468     0.471       410
          11      0.723     0.703     0.713       556
          12      0.681     0.572     0.622       243
          13      0.736     0.801     0.767       969
          14      0.775     0.705     0.738       132

    accuracy                          0.635      5933
   macro avg      0.576     0.527     0.545      5933
weighted avg      0.628     0.635     0.628      5933


============================= JOB FEEDBACK =============================

NodeName=uc2n520
Job ID: 22161734
Cluster: uc2
User/Group: ma_ytong/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:03:37
CPU Efficiency: 5.36% of 01:07:30 core-walltime
Job Wall-clock time: 00:06:45
Memory Utilized: 3.08 GB
Memory Efficiency: 7.88% of 39.06 GB
